<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--Description-->
    

    <!--Author-->
    
        <meta name="author" content="Yunfan Li">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="Hexo"/>
    

    <!--Open Graph Description-->
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content="Hexo"/>

    <!--Type page-->
    
        <meta property="og:type" content="website" />
    

    <!--Page Cover-->
    

    
        <meta name="twitter:card" content="summary" />
    
    
    

    <!-- Title -->
    
    <title>page - Hexo</title>

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.2/css/bootstrap.min.css" integrity="sha384-y3tfxAZXuh4HwSYylfB+J125MxIs6mR5FOHamPBG064zB+AFeWH94NdvaCBm8qnd" crossorigin="anonymous">

    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="//oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="//oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Gallery -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.css" type="text/css" rel="stylesheet" />

    <!-- Custom CSS -->
    
<link rel="stylesheet" href="/css/style.css">


    <!-- Google Analytics -->
    


<meta name="generator" content="Hexo 4.2.0"></head>


<body>

<div class="bg-gradient"></div>
<div class="bg-pattern"></div>

<!-- Menu -->
<!--Menu Links and Overlay-->
<div class="menu-bg">
    <div class="menu-container">
        <ul>
            
            <li class="menu-item">
                <a href="/archives">
                    Home
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/archives">
                    Archives
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/about.html">
                    About
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/tags">
                    Tags
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/categories">
                    Categories
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/contact.html">
                    Contact
                </a>
            </li>
            
        </ul>
    </div>
</div>

<!--Hamburger Icon-->
<nav>
    <a href="#menu"></a>
</nav>

<div class="container">

    <!-- Main Content -->
    <div class="row">
    <div class="col-sm-12">

        <!--Title and Logo-->
        <header>
    <div class="logo">
        <a href="/"><i class="logo-icon fa fa-cube" aria-hidden="true"></i></a>
        
            <h1 id="main-title" class="title">Hexo</h1>
        
    </div>
</header>

        <section class="main">
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2020/02/29/4.sentiment_with_mask/">
                4.sentiment_with_mask
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2020-02-29</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            <h1 id="情感分析"><a href="#情感分析" class="headerlink" title="情感分析"></a>情感分析</h1><p>褚则伟 <a href="mailto:zeweichu@gmail.com">zeweichu@gmail.com</a></p>
<p>学习目标</p>
<ul>
<li>学习和训练文本分类模型</li>
<li>学习torchtext的基本使用方法<ul>
<li>BucketIterator</li>
</ul>
</li>
<li>学习torch.nn的一些基本模型<ul>
<li>Conv2d</li>
</ul>
</li>
</ul>
<p>本notebook参考了<a href="https://github.com/bentrevett/pytorch-sentiment-analysis" target="_blank" rel="noopener">https://github.com/bentrevett/pytorch-sentiment-analysis</a></p>
<p>在这份notebook中，我们会用PyTorch模型和TorchText再来做情感分析(检测一段文字的情感是正面的还是负面的)。我们会使用<a href="http://ai.stanford.edu/~amaas/data/sentiment/" target="_blank" rel="noopener">IMDb 数据集</a>，即电影评论。</p>
<p>模型从简单到复杂，我们会依次构建：</p>
<ul>
<li>Word Averaging模型</li>
<li>RNN/LSTM模型</li>
<li>CNN模型</li>
</ul>
<h2 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h2><ul>
<li>TorchText中的一个重要概念是<code>Field</code>。<code>Field</code>决定了你的数据会被怎样处理。在我们的情感分类任务中，我们所需要接触到的数据有文本字符串和两种情感，”pos”或者”neg”。</li>
<li><code>Field</code>的参数制定了数据会被怎样处理。</li>
<li>我们使用<code>TEXT</code> field来定义如何处理电影评论，使用<code>LABEL</code> field来处理两个情感类别。</li>
<li>我们的<code>TEXT</code> field带有<code>tokenize=&#39;spacy&#39;</code>，这表示我们会用<a href="https://spacy.io" target="_blank" rel="noopener">spaCy</a> tokenizer来tokenize英文句子。如果我们不特别声明<code>tokenize</code>这个参数，那么默认的分词方法是使用空格。</li>
<li>安装spaCy<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install -U spacy</span><br><span class="line">python -m spacy download en</span><br></pre></td></tr></table></figure></li>
<li><code>LABEL</code>由<code>LabelField</code>定义。这是一种特别的用来处理label的<code>Field</code>。我们后面会解释dtype。</li>
<li>更多关于<code>Fields</code>，参见<a href="https://github.com/pytorch/text/blob/master/torchtext/data/field.py" target="_blank" rel="noopener">https://github.com/pytorch/text/blob/master/torchtext/data/field.py</a></li>
<li>和之前一样，我们会设定random seeds使实验可以复现。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torchtext</span><br><span class="line"><span class="keyword">from</span> torchtext <span class="keyword">import</span> data</span><br><span class="line"></span><br><span class="line">SEED = <span class="number">1234</span></span><br><span class="line"></span><br><span class="line">torch.manual_seed(SEED)</span><br><span class="line">torch.cuda.manual_seed(SEED)</span><br><span class="line">torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">TEXT = data.Field(tokenize=<span class="string">'spacy'</span>)</span><br><span class="line">LABEL = data.LabelField()<span class="comment"># dtype=torch.float)</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(torch.__version__)</span><br><span class="line">print(torchtext.__version__)</span><br></pre></td></tr></table></figure>

<pre><code>1.1.0
0.3.1</code></pre><ul>
<li>TorchText支持很多常见的自然语言处理数据集。</li>
<li>下面的代码会自动下载IMDb数据集，然后分成train/test两个<code>torchtext.datasets</code>类别。数据被前面的<code>Fields</code>处理。IMDb数据集一共有50000电影评论，每个评论都被标注为正面的或负面的。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchtext <span class="keyword">import</span> datasets</span><br><span class="line">train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)</span><br></pre></td></tr></table></figure>

<p>查看每个数据split有多少条数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">f'Number of training examples: <span class="subst">&#123;len(train_data)&#125;</span>'</span>)</span><br><span class="line">print(<span class="string">f'Number of testing examples: <span class="subst">&#123;len(test_data)&#125;</span>'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Number of training examples: 25000
Number of testing examples: 25000</code></pre><p>查看一个example。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(vars(train_data.examples[<span class="number">0</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>{&apos;text&apos;: [&apos;Brilliant&apos;, &apos;adaptation&apos;, &apos;of&apos;, &apos;the&apos;, &apos;novel&apos;, &apos;that&apos;, &apos;made&apos;, &apos;famous&apos;, &apos;the&apos;, &apos;relatives&apos;, &apos;of&apos;, &apos;Chilean&apos;, &apos;President&apos;, &apos;Salvador&apos;, &apos;Allende&apos;, &apos;killed&apos;, &apos;.&apos;, &apos;In&apos;, &apos;the&apos;, &apos;environment&apos;, &apos;of&apos;, &apos;a&apos;, &apos;large&apos;, &apos;estate&apos;, &apos;that&apos;, &apos;arises&apos;, &apos;from&apos;, &apos;the&apos;, &apos;ruins&apos;, &apos;,&apos;, &apos;becoming&apos;, &apos;a&apos;, &apos;force&apos;, &apos;to&apos;, &apos;abuse&apos;, &apos;and&apos;, &apos;exploitation&apos;, &apos;of&apos;, &apos;outrage&apos;, &apos;,&apos;, &apos;a&apos;, &apos;luxury&apos;, &apos;estate&apos;, &apos;for&apos;, &apos;the&apos;, &apos;benefit&apos;, &apos;of&apos;, &apos;the&apos;, &apos;upstart&apos;, &apos;Esteban&apos;, &apos;Trueba&apos;, &apos;and&apos;, &apos;his&apos;, &apos;undeserved&apos;, &apos;family&apos;, &apos;,&apos;, &apos;the&apos;, &apos;brilliant&apos;, &apos;Danish&apos;, &apos;director&apos;, &apos;Bille&apos;, &apos;August&apos;, &apos;recreates&apos;, &apos;,&apos;, &apos;in&apos;, &apos;micro&apos;, &apos;,&apos;, &apos;which&apos;, &apos;at&apos;, &apos;the&apos;, &apos;time&apos;, &apos;would&apos;, &apos;be&apos;, &apos;the&apos;, &apos;process&apos;, &apos;leading&apos;, &apos;to&apos;, &apos;the&apos;, &apos;greatest&apos;, &apos;infamy&apos;, &apos;of&apos;, &apos;his&apos;, &apos;story&apos;, &apos;to&apos;, &apos;the&apos;, &apos;hardened&apos;, &apos;Chilean&apos;, &apos;nation&apos;, &apos;,&apos;, &apos;and&apos;, &apos;whose&apos;, &apos;main&apos;, &apos;character&apos;, &apos;would&apos;, &apos;Augusto&apos;, &apos;Pinochet&apos;, &apos;(&apos;, &apos;Stephen&apos;, &apos;similarities&apos;, &apos;with&apos;, &apos;it&apos;, &apos;are&apos;, &apos;inevitable&apos;, &apos;:&apos;, &apos;recall&apos;, &apos;,&apos;, &apos;as&apos;, &apos;an&apos;, &apos;example&apos;, &apos;,&apos;, &apos;that&apos;, &apos;image&apos;, &apos;of&apos;, &apos;the&apos;, &apos;senator&apos;, &apos;with&apos;, &apos;dark&apos;, &apos;glasses&apos;, &apos;that&apos;, &apos;makes&apos;, &apos;him&apos;, &apos;the&apos;, &apos;wink&apos;, &apos;to&apos;, &apos;the&apos;, &apos;general&apos;, &apos;to&apos;, &apos;begin&apos;, &apos;making&apos;, &apos;the&apos;, &apos;palace).&lt;br&apos;, &apos;/&gt;&lt;br&apos;, &apos;/&gt;Bille&apos;, &apos;August&apos;, &apos;attends&apos;, &apos;an&apos;, &apos;exceptional&apos;, &apos;cast&apos;, &apos;in&apos;, &apos;the&apos;, &apos;Jeremy&apos;, &apos;protruding&apos;, &apos;Irons&apos;, &apos;,&apos;, &apos;whose&apos;, &apos;character&apos;, &apos;changes&apos;, &apos;from&apos;, &apos;arrogance&apos;, &apos;and&apos;, &apos;extreme&apos;, &apos;cruelty&apos;, &apos;,&apos;, &apos;the&apos;, &apos;hard&apos;, &apos;lesson&apos;, &apos;that&apos;, &apos;life&apos;, &apos;always&apos;, &apos;brings&apos;, &apos;us&apos;, &apos;to&apos;, &apos;almost&apos;, &apos;force&apos;, &apos;us&apos;, &apos;to&apos;, &apos;change&apos;, &apos;.&apos;, &apos;In&apos;, &apos;Esteban&apos;, &apos;fully&apos;, &apos;applies&apos;, &apos;the&apos;, &apos;law&apos;, &apos;of&apos;, &apos;resonance&apos;, &apos;,&apos;, &apos;with&apos;, &apos;great&apos;, &apos;wisdom&apos;, &apos;,&apos;, &apos;Solomon&apos;, &apos;describes&apos;, &apos;in&apos;, &apos;these&apos;, &apos;words:&quot;The&apos;, &apos;things&apos;, &apos;that&apos;, &apos;freckles&apos;, &apos;are&apos;, &apos;the&apos;, &apos;same&apos;, &apos;punishment&apos;, &apos;that&apos;, &apos;will&apos;, &apos;serve&apos;, &apos;you&apos;, &apos;.&apos;, &apos;&quot;&apos;, &apos;&lt;&apos;, &apos;br&apos;, &apos;/&gt;&lt;br&apos;, &apos;/&gt;Unforgettable&apos;, &apos;Glenn&apos;, &apos;Close&apos;, &apos;playing&apos;, &apos;splint&apos;, &apos;,&apos;, &apos;the&apos;, &apos;tainted&apos;, &apos;sister&apos;, &apos;of&apos;, &apos;Stephen&apos;, &apos;,&apos;, &apos;whose&apos;, &apos;sin&apos;, &apos;,&apos;, &apos;driven&apos;, &apos;by&apos;, &apos;loneliness&apos;, &apos;,&apos;, &apos;spiritual&apos;, &apos;and&apos;, &apos;platonic&apos;, &apos;love&apos;, &apos;was&apos;, &apos;the&apos;, &apos;wife&apos;, &apos;of&apos;, &apos;his&apos;, &apos;cruel&apos;, &apos;snowy&apos;, &apos;brother&apos;, &apos;.&apos;, &apos;Meryl&apos;, &apos;Streep&apos;, &apos;also&apos;, &apos;brilliant&apos;, &apos;,&apos;, &apos;a&apos;, &apos;woman&apos;, &apos;whose&apos;, &apos;name&apos;, &apos;came&apos;, &apos;to&apos;, &apos;him&apos;, &apos;like&apos;, &apos;a&apos;, &apos;glove&apos;, &apos;Clara&apos;, &apos;.&apos;, &apos;With&apos;, &apos;telekinetic&apos;, &apos;powers&apos;, &apos;,&apos;, &apos;cognitive&apos;, &apos;and&apos;, &apos;mediumistic&apos;, &apos;,&apos;, &apos;this&apos;, &apos;hardened&apos;, &apos;woman&apos;, &apos;,&apos;, &apos;loyal&apos;, &apos;to&apos;, &apos;his&apos;, &apos;blunt&apos;, &apos;,&apos;, &apos;conservative&apos;, &apos;husband&apos;, &apos;,&apos;, &apos;is&apos;, &apos;an&apos;, &apos;indicator&apos;, &apos;of&apos;, &apos;character&apos;, &apos;and&apos;, &apos;self&apos;, &apos;-&apos;, &apos;control&apos;, &apos;that&apos;, &apos;we&apos;, &apos;wish&apos;, &apos;for&apos;, &apos;ourselves&apos;, &apos;and&apos;, &apos;for&apos;, &apos;all&apos;, &apos;human&apos;, &apos;beings&apos;, &apos;.&apos;, &apos;&lt;&apos;, &apos;br&apos;, &apos;/&gt;&lt;br&apos;, &apos;/&gt;Every&apos;, &apos;character&apos;, &apos;is&apos;, &apos;a&apos;, &apos;portrait&apos;, &apos;of&apos;, &apos;virtuosity&apos;, &apos;(&apos;, &apos;as&apos;, &apos;Blanca&apos;, &apos;worthy&apos;, &apos;rebel&apos;, &apos;leader&apos;, &apos;Pedro&apos;, &apos;Segundo&apos;, &apos;unhappy&apos;, &apos;...&apos;, &apos;)&apos;, &apos;or&apos;, &apos;a&apos;, &apos;portrait&apos;, &apos;of&apos;, &apos;humiliation&apos;, &apos;,&apos;, &apos;like&apos;, &apos;Stephen&apos;, &apos;Jr.&apos;, &apos;,&apos;, &apos;the&apos;, &apos;bastard&apos;, &apos;child&apos;, &apos;of&apos;, &apos;Senator&apos;, &apos;,&apos;, &apos;who&apos;, &apos;serves&apos;, &apos;as&apos;, &apos;an&apos;, &apos;instrument&apos;, &apos;for&apos;, &apos;the&apos;, &apos;return&apos;, &apos;of&apos;, &apos;the&apos;, &apos;boomerang&apos;, &apos;.&apos;, &apos;&lt;&apos;, &apos;br&apos;, &apos;/&gt;&lt;br&apos;, &apos;/&gt;The&apos;, &apos;film&apos;, &apos;moves&apos;, &apos;the&apos;, &apos;bowels&apos;, &apos;,&apos;, &apos;we&apos;, &apos;recreated&apos;, &apos;some&apos;, &apos;facts&apos;, &apos;that&apos;, &apos;should&apos;, &apos;not&apos;, &apos;ever&apos;, &apos;be&apos;, &apos;repeated&apos;, &apos;,&apos;, &apos;but&apos;, &apos;that&apos;, &apos;absurdly&apos;, &apos;still&apos;, &apos;happen&apos;, &apos;(&apos;, &apos;Colombia&apos;, &apos;is&apos;, &apos;a&apos;, &apos;sad&apos;, &apos;example&apos;, &apos;)&apos;, &apos;and&apos;, &apos;another&apos;, &apos;reminder&apos;, &apos;that&apos;, &apos;,&apos;, &apos;against&apos;, &apos;all&apos;, &apos;,&apos;, &apos;life&apos;, &apos;is&apos;, &apos;wonderful&apos;, &apos;because&apos;, &apos;there&apos;, &apos;are&apos;, &apos;always&apos;, &apos;people&apos;, &apos;like&apos;, &apos;Isabel&apos;, &apos;Allende&apos;, &apos;and&apos;, &apos;immortalize&apos;, &apos;just&apos;, &apos;Bille&apos;, &apos;August&apos;, &apos;.&apos;], &apos;label&apos;: &apos;pos&apos;}</code></pre><ul>
<li>由于我们现在只有train/test这两个分类，所以我们需要创建一个新的validation set。我们可以使用<code>.split()</code>创建新的分类。</li>
<li>默认的数据分割是 70、30，如果我们声明<code>split_ratio</code>，可以改变split之间的比例，<code>split_ratio=0.8</code>表示80%的数据是训练集，20%是验证集。</li>
<li>我们还声明<code>random_state</code>这个参数，确保我们每次分割的数据集都是一样的。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line">train_data, valid_data = train_data.split(random_state=random.seed(SEED))</span><br></pre></td></tr></table></figure>

<p>检查一下现在每个部分有多少条数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">f'Number of training examples: <span class="subst">&#123;len(train_data)&#125;</span>'</span>)</span><br><span class="line">print(<span class="string">f'Number of validation examples: <span class="subst">&#123;len(valid_data)&#125;</span>'</span>)</span><br><span class="line">print(<span class="string">f'Number of testing examples: <span class="subst">&#123;len(test_data)&#125;</span>'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Number of training examples: 17500
Number of validation examples: 7500
Number of testing examples: 25000</code></pre><ul>
<li>下一步我们需要创建 <em>vocabulary</em> 。<em>vocabulary</em> 就是把每个单词一一映射到一个数字。<br><img src="assets/sentiment5.png" alt=""></li>
<li>我们使用最常见的25k个单词来构建我们的单词表，用<code>max_size</code>这个参数可以做到这一点。</li>
<li>所有其他的单词都用<code>&lt;unk&gt;</code>来表示。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TEXT.build_vocab(train_data, max_size=25000)</span></span><br><span class="line"><span class="comment"># LABEL.build_vocab(train_data)</span></span><br><span class="line">TEXT.build_vocab(train_data, max_size=<span class="number">25000</span>, vectors=<span class="string">"glove.6B.100d"</span>, unk_init=torch.Tensor.normal_)</span><br><span class="line">LABEL.build_vocab(train_data)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">f"Unique tokens in TEXT vocabulary: <span class="subst">&#123;len(TEXT.vocab)&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Unique tokens in LABEL vocabulary: <span class="subst">&#123;len(LABEL.vocab)&#125;</span>"</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Unique tokens in TEXT vocabulary: 25002
Unique tokens in LABEL vocabulary: 2</code></pre><ul>
<li>当我们把句子传进模型的时候，我们是按照一个个 <em>batch</em> 穿进去的，也就是说，我们一次传入了好几个句子，而且每个batch中的句子必须是相同的长度。为了确保句子的长度相同，TorchText会把短的句子pad到和最长的句子等长。<br><img src="assets/sentiment6.png" alt=""></li>
<li>下面我们来看看训练数据集中最常见的单词。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(TEXT.vocab.freqs.most_common(<span class="number">20</span>))</span><br></pre></td></tr></table></figure>

<pre><code>[(&apos;the&apos;, 201455), (&apos;,&apos;, 192552), (&apos;.&apos;, 164402), (&apos;a&apos;, 108963), (&apos;and&apos;, 108649), (&apos;of&apos;, 100010), (&apos;to&apos;, 92873), (&apos;is&apos;, 76046), (&apos;in&apos;, 60904), (&apos;I&apos;, 54486), (&apos;it&apos;, 53405), (&apos;that&apos;, 49155), (&apos;&quot;&apos;, 43890), (&quot;&apos;s&quot;, 43151), (&apos;this&apos;, 42454), (&apos;-&apos;, 36769), (&apos;/&gt;&lt;br&apos;, 35511), (&apos;was&apos;, 34990), (&apos;as&apos;, 30324), (&apos;with&apos;, 29691)]</code></pre><p>我们可以直接用 <code>stoi</code>(<strong>s</strong>tring <strong>to</strong> <strong>i</strong>nt) 或者 <code>itos</code> (<strong>i</strong>nt <strong>to</strong>  <strong>s</strong>tring) 来查看我们的单词表。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(TEXT.vocab.itos[:<span class="number">10</span>])</span><br></pre></td></tr></table></figure>

<pre><code>[&apos;&lt;unk&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;the&apos;, &apos;,&apos;, &apos;.&apos;, &apos;a&apos;, &apos;and&apos;, &apos;of&apos;, &apos;to&apos;, &apos;is&apos;]</code></pre><p>查看labels。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(LABEL.vocab.stoi)</span><br></pre></td></tr></table></figure>

<pre><code>defaultdict(&lt;function _default_unk_index at 0x7f6944d3f730&gt;, {&apos;neg&apos;: 0, &apos;pos&apos;: 1})</code></pre><ul>
<li>最后一步数据的准备是创建iterators。每个itartion都会返回一个batch的examples。</li>
<li>我们会使用<code>BucketIterator</code>。<code>BucketIterator</code>会把长度差不多的句子放到同一个batch中，确保每个batch中不出现太多的padding。</li>
<li>严格来说，我们这份notebook中的模型代码都有一个问题，也就是我们把<code>&lt;pad&gt;</code>也当做了模型的输入进行训练。更好的做法是在模型中把由<code>&lt;pad&gt;</code>产生的输出给消除掉。在这节课中我们简单处理，直接把<code>&lt;pad&gt;</code>也用作模型输入了。由于<code>&lt;pad&gt;</code>数量不多，模型的效果也不差。</li>
<li>如果我们有GPU，还可以指定每个iteration返回的tensor都在GPU上。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">BATCH_SIZE = <span class="number">64</span></span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line">train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(</span><br><span class="line">    (train_data, valid_data, test_data), </span><br><span class="line">    batch_size=BATCH_SIZE,</span><br><span class="line">    device=device,</span><br><span class="line">    repeat=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># for i, _ in enumerate(train_iterator):</span></span><br><span class="line"><span class="comment">#     print(i)</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">batch = next(iter(train_iterator))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">batch.text</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[   65,  6706,    23,  ...,  3101,    54,    87],
        [   52, 11017,    83,  ..., 24113,    15,  1078],
        [    8,     3,   671,  ...,    52,    73,     3],
        ...,
        [    1,     1,     1,  ...,     1,     1,     1],
        [    1,     1,     1,  ...,     1,     1,     1],
        [    1,     1,     1,  ...,     1,     1,     1]], device=&apos;cuda:0&apos;)</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]</span><br><span class="line">mask = batch.text == PAD_IDX</span><br><span class="line">mask</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        [0, 0, 0,  ..., 0, 0, 0],
        ...,
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1],
        [1, 1, 1,  ..., 1, 1, 1]], device=&apos;cuda:0&apos;, dtype=torch.uint8)</code></pre><h2 id="Word-Averaging模型"><a href="#Word-Averaging模型" class="headerlink" title="Word Averaging模型"></a>Word Averaging模型</h2><ul>
<li>我们首先介绍一个简单的Word Averaging模型。这个模型非常简单，我们把每个单词都通过<code>Embedding</code>层投射成word embedding vector，然后把一句话中的所有word vector做个平均，就是整个句子的vector表示了。接下来把这个sentence vector传入一个<code>Linear</code>层，做分类即可。</li>
</ul>
<p><img src="assets/sentiment8.png" alt=""></p>
<ul>
<li>我们使用<a href="https://pytorch.org/docs/stable/nn.html?highlight=avg_pool2d#torch.nn.functional.avg_pool2d" target="_blank" rel="noopener"><code>avg_pool2d</code></a>来做average pooling。我们的目标是把sentence length那个维度平均成1，然后保留embedding这个维度。</li>
</ul>
<p><img src="assets/sentiment9.png" alt=""></p>
<ul>
<li><code>avg_pool2d</code>的kernel size是 (<code>embedded.shape[1]</code>, 1)，所以句子长度的那个维度会被压扁。</li>
</ul>
<p><img src="assets/sentiment10.png" alt=""></p>
<p><img src="assets/sentiment11.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordAVGModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_dim, output_dim, pad_idx)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)</span><br><span class="line">        self.fc = nn.Linear(embedding_dim, output_dim)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text, mask)</span>:</span></span><br><span class="line">        embedded = self.embedding(text) <span class="comment"># [batch_size, seq_len, emb_dim]</span></span><br><span class="line">        sent_embed = torch.sum(embedded * mask.unsqueeze(<span class="number">2</span>), <span class="number">1</span>) / mask.sum(<span class="number">1</span>).unsqueeze(<span class="number">1</span>) <span class="comment"># [batch size, embedding_dim]</span></span><br><span class="line">        <span class="keyword">return</span> self.fc(sent_embed)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">INPUT_DIM = len(TEXT.vocab)</span><br><span class="line">EMBEDDING_DIM = <span class="number">100</span></span><br><span class="line">OUTPUT_DIM = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">model = WordAVGModel(INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM, PAD_IDX)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_parameters</span><span class="params">(model)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> sum(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f'The model has <span class="subst">&#123;count_parameters(model):,&#125;</span> trainable parameters'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>The model has 2,500,301 trainable parameters</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pretrained_embeddings = TEXT.vocab.vectors</span><br><span class="line">model.embedding.weight.data.copy_(pretrained_embeddings)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],
        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],
        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],
        ...,
        [-0.7244, -0.0186,  0.0996,  ...,  0.0045, -1.0037,  0.6646],
        [-1.1243,  1.2040, -0.6489,  ..., -0.7526,  0.5711,  1.0081],
        [ 0.0860,  0.1367,  0.0321,  ..., -0.5542, -0.4557, -0.0382]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]</span><br><span class="line"></span><br><span class="line">model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)</span><br><span class="line">model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)</span><br></pre></td></tr></table></figure>

<h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">optimizer = optim.Adam(model.parameters())</span><br><span class="line">criterion = nn.BCEWithLogitsLoss()</span><br><span class="line">model = model.to(device)</span><br><span class="line">criterion = criterion.to(device)</span><br></pre></td></tr></table></figure>

<p>计算预测的准确率</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_accuracy</span><span class="params">(preds, y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#round predictions to the closest integer</span></span><br><span class="line">    rounded_preds = torch.round(torch.sigmoid(preds))</span><br><span class="line">    correct = (rounded_preds == y).float() <span class="comment">#convert into float for division </span></span><br><span class="line">    acc = correct.sum()/len(correct)</span><br><span class="line">    <span class="keyword">return</span> acc</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(model, iterator, optimizer, criterion)</span>:</span></span><br><span class="line">    </span><br><span class="line">    epoch_loss = <span class="number">0</span></span><br><span class="line">    epoch_acc = <span class="number">0</span></span><br><span class="line">    model.train()</span><br><span class="line">    </span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> iterator:</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        <span class="comment"># CHANGED</span></span><br><span class="line">        text = batch.text.permute(<span class="number">1</span>, <span class="number">0</span>) <span class="comment"># [batch_size, seq_length]</span></span><br><span class="line">        mask = <span class="number">1.</span> - (text == PAD_IDX).float() <span class="comment"># [batch_size, seq_len]</span></span><br><span class="line">        predictions = model(text, mask).squeeze(<span class="number">1</span>)</span><br><span class="line">        loss = criterion(predictions, batch.label.float())</span><br><span class="line">        acc = binary_accuracy(predictions, batch.label.float())</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"batch &#123;&#125;, loss &#123;&#125;"</span>.format(i, loss.item()))</span><br><span class="line">        i += <span class="number">1</span></span><br><span class="line">        </span><br><span class="line">        epoch_loss += loss.item()</span><br><span class="line">        epoch_acc += acc.item()</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> epoch_loss / len(iterator), epoch_acc / len(iterator)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(model, iterator, criterion)</span>:</span></span><br><span class="line">    </span><br><span class="line">    epoch_loss = <span class="number">0</span></span><br><span class="line">    epoch_acc = <span class="number">0</span></span><br><span class="line">    model.eval()</span><br><span class="line">    </span><br><span class="line">    i = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> iterator:</span><br><span class="line">            text = batch.text.permute(<span class="number">1</span>, <span class="number">0</span>) <span class="comment"># [batch_size, seq_length]</span></span><br><span class="line">            mask = <span class="number">1.</span> - (text == PAD_IDX).float() <span class="comment"># [batch_size, seq_len]</span></span><br><span class="line">            predictions = model(text, mask).squeeze(<span class="number">1</span>)</span><br><span class="line">            loss = criterion(predictions, batch.label.float())</span><br><span class="line">            acc = binary_accuracy(predictions, batch.label.float())</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                print(<span class="string">"batch &#123;&#125;, loss &#123;&#125;"</span>.format(i, loss.item()))</span><br><span class="line">            i += <span class="number">1</span></span><br><span class="line">            epoch_loss += loss.item()</span><br><span class="line">            epoch_acc += acc.item()</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> epoch_loss / len(iterator), epoch_acc / len(iterator)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">epoch_time</span><span class="params">(start_time, end_time)</span>:</span></span><br><span class="line">    elapsed_time = end_time - start_time</span><br><span class="line">    elapsed_mins = int(elapsed_time / <span class="number">60</span>)</span><br><span class="line">    elapsed_secs = int(elapsed_time - (elapsed_mins * <span class="number">60</span>))</span><br><span class="line">    <span class="keyword">return</span> elapsed_mins, elapsed_secs</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">N_EPOCHS = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">best_valid_loss = float(<span class="string">'inf'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(N_EPOCHS):</span><br><span class="line"></span><br><span class="line">    start_time = time.time()</span><br><span class="line">    </span><br><span class="line">    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)</span><br><span class="line">    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)</span><br><span class="line">    </span><br><span class="line">    end_time = time.time()</span><br><span class="line"></span><br><span class="line">    epoch_mins, epoch_secs = epoch_time(start_time, end_time)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> valid_loss &lt; best_valid_loss:</span><br><span class="line">        best_valid_loss = valid_loss</span><br><span class="line">        torch.save(model.state_dict(), <span class="string">'wordavg-model.pt'</span>)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">f'Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>:<span class="number">02</span>&#125;</span> | Epoch Time: <span class="subst">&#123;epoch_mins&#125;</span>m <span class="subst">&#123;epoch_secs&#125;</span>s'</span>)</span><br><span class="line">    print(<span class="string">f'\tTrain Loss: <span class="subst">&#123;train_loss:<span class="number">.3</span>f&#125;</span> | Train Acc: <span class="subst">&#123;train_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br><span class="line">    print(<span class="string">f'\t Val. Loss: <span class="subst">&#123;valid_loss:<span class="number">.3</span>f&#125;</span> |  Val. Acc: <span class="subst">&#123;valid_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>batch 0, loss 0.6835149526596069
batch 100, loss 0.6759217977523804
batch 200, loss 0.6536192297935486
batch 0, loss 0.5802608132362366
batch 100, loss 0.6405552625656128
Epoch: 01 | Epoch Time: 0m 2s
    Train Loss: 0.661 | Train Acc: 66.62%
     Val. Loss: 0.615 |  Val. Acc: 74.25%
batch 0, loss 0.6175215244293213
batch 100, loss 0.5193076133728027
batch 200, loss 0.523094654083252
batch 0, loss 0.41260701417922974
batch 100, loss 0.546144425868988
Epoch: 02 | Epoch Time: 0m 2s
    Train Loss: 0.542 | Train Acc: 78.82%
     Val. Loss: 0.482 |  Val. Acc: 81.45%
batch 0, loss 0.48719578981399536
batch 100, loss 0.3965785503387451
batch 200, loss 0.4322021007537842
batch 0, loss 0.35118478536605835
batch 100, loss 0.46531984210014343
Epoch: 03 | Epoch Time: 0m 2s
    Train Loss: 0.414 | Train Acc: 85.14%
     Val. Loss: 0.391 |  Val. Acc: 85.33%
batch 0, loss 0.31555071473121643
batch 100, loss 0.3576723039150238
batch 200, loss 0.43358099460601807
batch 0, loss 0.3284790515899658
batch 100, loss 0.4068619906902313
Epoch: 04 | Epoch Time: 0m 2s
    Train Loss: 0.333 | Train Acc: 88.22%
     Val. Loss: 0.341 |  Val. Acc: 86.73%
batch 0, loss 0.21446196734905243
batch 100, loss 0.29952651262283325
batch 200, loss 0.33016496896743774
batch 0, loss 0.33019396662712097
batch 100, loss 0.372672975063324
Epoch: 05 | Epoch Time: 0m 2s
    Train Loss: 0.284 | Train Acc: 90.08%
     Val. Loss: 0.311 |  Val. Acc: 87.85%
batch 0, loss 0.21933476626873016
batch 100, loss 0.20656771957874298
batch 200, loss 0.2411007285118103
batch 0, loss 0.3338389992713928
batch 100, loss 0.35051852464675903
Epoch: 06 | Epoch Time: 0m 2s
    Train Loss: 0.248 | Train Acc: 91.57%
     Val. Loss: 0.292 |  Val. Acc: 88.37%
batch 0, loss 0.2381495237350464
batch 100, loss 0.3066502809524536
batch 200, loss 0.17593657970428467
batch 0, loss 0.33260178565979004
batch 100, loss 0.3287006616592407
Epoch: 07 | Epoch Time: 0m 2s
    Train Loss: 0.220 | Train Acc: 92.62%
     Val. Loss: 0.281 |  Val. Acc: 88.89%
batch 0, loss 0.18733319640159607
batch 100, loss 0.2353360801935196
batch 200, loss 0.19918608665466309
batch 0, loss 0.34648358821868896
batch 100, loss 0.3191569447517395
Epoch: 08 | Epoch Time: 0m 2s
    Train Loss: 0.197 | Train Acc: 93.63%
     Val. Loss: 0.269 |  Val. Acc: 89.23%
batch 0, loss 0.10634639114141464
batch 100, loss 0.11403544247150421
batch 200, loss 0.29342859983444214
batch 0, loss 0.35649430751800537
batch 100, loss 0.3183209300041199
Epoch: 09 | Epoch Time: 0m 2s
    Train Loss: 0.177 | Train Acc: 94.27%
     Val. Loss: 0.264 |  Val. Acc: 89.26%
batch 0, loss 0.16292411088943481
batch 100, loss 0.08687698841094971
batch 200, loss 0.21162091195583344
batch 0, loss 0.3467680811882019
batch 100, loss 0.2997514605522156
Epoch: 10 | Epoch Time: 0m 2s
    Train Loss: 0.160 | Train Acc: 94.98%
     Val. Loss: 0.258 |  Val. Acc: 89.72%</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line">nlp = spacy.load(<span class="string">'en'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_sentiment</span><span class="params">(sentence)</span>:</span></span><br><span class="line">    tokenized = [tok.text <span class="keyword">for</span> tok <span class="keyword">in</span> nlp.tokenizer(sentence)]</span><br><span class="line">    indexed = [TEXT.vocab.stoi[t] <span class="keyword">for</span> t <span class="keyword">in</span> tokenized]</span><br><span class="line">    tensor = torch.LongTensor(indexed).to(device)</span><br><span class="line">    text = tensor.unsqueeze(<span class="number">0</span>)</span><br><span class="line">    mask = <span class="number">1.</span> - (text == PAD_IDX).float() <span class="comment"># [batch_size, seq_len]</span></span><br><span class="line">    prediction = torch.sigmoid(model(tensor, mask))</span><br><span class="line">    <span class="keyword">return</span> prediction.item()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict_sentiment(<span class="string">"This film is terrible"</span>)</span><br></pre></td></tr></table></figure>




<pre><code>2.4536811471520537e-10</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict_sentiment(<span class="string">"This film is great"</span>)</span><br></pre></td></tr></table></figure>




<pre><code>1.0</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.load_state_dict(torch.load(<span class="string">'wordavg-model.pt'</span>))</span><br><span class="line">test_loss, test_acc = evaluate(model, test_iterator, criterion)</span><br><span class="line">print(<span class="string">f'Test Loss: <span class="subst">&#123;test_loss:<span class="number">.3</span>f&#125;</span> | Test Acc: <span class="subst">&#123;test_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>batch 0, loss 0.32167336344718933
batch 100, loss 0.34431976079940796
batch 200, loss 0.18615691363811493
batch 300, loss 0.37860944867134094
Test Loss: 0.290 | Test Acc: 88.03%</code></pre><h2 id="RNN模型"><a href="#RNN模型" class="headerlink" title="RNN模型"></a>RNN模型</h2><ul>
<li>下面我们尝试把模型换成一个<strong>recurrent neural network</strong> (RNN)。RNN经常会被用来encode一个sequence<br>$$h_t = \text{RNN}(x_t, h_{t-1})$$</li>
<li>我们使用最后一个hidden state $h_T$来表示整个句子。</li>
<li>然后我们把$h_T$通过一个线性变换$f$，然后用来预测句子的情感。</li>
</ul>
<p><img src="assets/sentiment1.png" alt=""></p>
<p><img src="assets/sentiment7.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_dim, hidden_dim, output_dim, </span></span></span><br><span class="line"><span class="function"><span class="params">                 n_layers, bidirectional, dropout, pad_idx, avg_hidden=True)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.bidirectional = bidirectional</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)</span><br><span class="line">        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, </span><br><span class="line">                           bidirectional=bidirectional, dropout=dropout)</span><br><span class="line">        </span><br><span class="line">        self.fc = nn.Linear(hidden_dim*<span class="number">2</span> <span class="keyword">if</span> self.bidirectional <span class="keyword">else</span> hidden_dim, output_dim)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.avg_hidden = avg_hidden</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text, mask)</span>:</span></span><br><span class="line">        embedded = self.dropout(self.embedding(text)) <span class="comment">#[sent len, batch size, emb dim]</span></span><br><span class="line">        <span class="comment"># CHANGED</span></span><br><span class="line">        seq_length = mask.sum(<span class="number">1</span>)</span><br><span class="line">        embedded = torch.nn.utils.rnn.pack_padded_sequence(</span><br><span class="line">            input=embedded, </span><br><span class="line">            lengths=seq_length, </span><br><span class="line">            batch_first=<span class="literal">True</span>,</span><br><span class="line">            enforce_sorted=<span class="literal">False</span>)</span><br><span class="line">        output, (hidden, cell) = self.rnn(embedded)</span><br><span class="line">        output, seq_length = torch.nn.utils.rnn.pad_packed_sequence(</span><br><span class="line">            sequence=output,</span><br><span class="line">            batch_first=<span class="literal">True</span>,</span><br><span class="line">            padding_value=<span class="number">0</span>,</span><br><span class="line">            total_length=mask.shape[<span class="number">1</span>]</span><br><span class="line">        )</span><br><span class="line">        <span class="comment">#output = [sent len, batch size, hid dim * num directions]</span></span><br><span class="line">        <span class="comment">#hidden = [num layers * num directions, batch size, hid dim]</span></span><br><span class="line">        <span class="comment">#cell = [num layers * num directions, batch size, hid dim]</span></span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> self.avg_hidden:</span><br><span class="line"><span class="comment">#             print(output)</span></span><br><span class="line">            hidden = torch.sum(output * mask.unsqueeze(<span class="number">2</span>), <span class="number">1</span>) / torch.sum(mask, <span class="number">1</span>, keepdim=<span class="literal">True</span>) <span class="comment"># [batch size, embedding_dim]</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">if</span> self.bidirectional:</span><br><span class="line">                <span class="comment">#concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers</span></span><br><span class="line">                hidden = torch.cat((hidden[<span class="number">-2</span>,:,:], hidden[<span class="number">-1</span>,:,:]), dim=<span class="number">1</span>) <span class="comment"># [batch size, hid dim * num directions]</span></span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                hidden = self.dropout(hidden[<span class="number">-1</span>,:,:]) <span class="comment"># [batch size, hid dim * num directions]</span></span><br><span class="line">        <span class="comment"># apply dropout</span></span><br><span class="line">        hidden = self.dropout(hidden)</span><br><span class="line">        <span class="keyword">return</span> self.fc(hidden)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">INPUT_DIM = len(TEXT.vocab)</span><br><span class="line">EMBEDDING_DIM = <span class="number">100</span></span><br><span class="line">HIDDEN_DIM = <span class="number">256</span></span><br><span class="line">OUTPUT_DIM = <span class="number">1</span></span><br><span class="line">N_LAYERS = <span class="number">2</span></span><br><span class="line">BIDIRECTIONAL = <span class="literal">True</span></span><br><span class="line">DROPOUT = <span class="number">0.5</span></span><br><span class="line">PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]</span><br><span class="line"></span><br><span class="line">model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, </span><br><span class="line">            N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX, avg_hidden=<span class="literal">False</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">f'The model has <span class="subst">&#123;count_parameters(model):,&#125;</span> trainable parameters'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>The model has 4,810,857 trainable parameters</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model.embedding.weight.data.copy_(pretrained_embeddings)</span><br><span class="line">UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]</span><br><span class="line"></span><br><span class="line">model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)</span><br><span class="line">model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)</span><br><span class="line"></span><br><span class="line">print(model.embedding.weight.data)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],
        ...,
        [-0.7244, -0.0186,  0.0996,  ...,  0.0045, -1.0037,  0.6646],
        [-1.1243,  1.2040, -0.6489,  ..., -0.7526,  0.5711,  1.0081],
        [ 0.0860,  0.1367,  0.0321,  ..., -0.5542, -0.4557, -0.0382]])</code></pre><h2 id="训练RNN模型"><a href="#训练RNN模型" class="headerlink" title="训练RNN模型"></a>训练RNN模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.Adam(model.parameters())</span><br><span class="line">model = model.to(device)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">N_EPOCHS = <span class="number">5</span></span><br><span class="line">best_valid_loss = float(<span class="string">'inf'</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(N_EPOCHS):</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)</span><br><span class="line">    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)</span><br><span class="line">    </span><br><span class="line">    end_time = time.time()</span><br><span class="line"></span><br><span class="line">    epoch_mins, epoch_secs = epoch_time(start_time, end_time)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> valid_loss &lt; best_valid_loss:</span><br><span class="line">        best_valid_loss = valid_loss</span><br><span class="line">        torch.save(model.state_dict(), <span class="string">'lstm-model.pt'</span>)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">f'Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>:<span class="number">02</span>&#125;</span> | Epoch Time: <span class="subst">&#123;epoch_mins&#125;</span>m <span class="subst">&#123;epoch_secs&#125;</span>s'</span>)</span><br><span class="line">    print(<span class="string">f'\tTrain Loss: <span class="subst">&#123;train_loss:<span class="number">.3</span>f&#125;</span> | Train Acc: <span class="subst">&#123;train_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br><span class="line">    print(<span class="string">f'\t Val. Loss: <span class="subst">&#123;valid_loss:<span class="number">.3</span>f&#125;</span> |  Val. Acc: <span class="subst">&#123;valid_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>batch 0, loss 0.6940298080444336
batch 100, loss 0.6605077981948853
batch 200, loss 0.5677657723426819
batch 0, loss 0.6464325189590454
batch 100, loss 0.7902224659919739
Epoch: 01 | Epoch Time: 1m 1s
    Train Loss: 0.651 | Train Acc: 61.65%
     Val. Loss: 0.717 |  Val. Acc: 52.98%
batch 0, loss 0.7926035523414612
batch 100, loss 0.7492727637290955
batch 200, loss 0.7025203704833984
batch 0, loss 0.6599957942962646
batch 100, loss 0.6523773670196533
Epoch: 02 | Epoch Time: 1m 1s
    Train Loss: 0.673 | Train Acc: 57.12%
     Val. Loss: 0.659 |  Val. Acc: 61.20%
batch 0, loss 0.64130699634552
batch 100, loss 0.6027564406394958
batch 200, loss 0.6683254837989807
batch 0, loss 0.5396684408187866
batch 100, loss 0.5652653574943542
Epoch: 03 | Epoch Time: 1m 2s
    Train Loss: 0.610 | Train Acc: 66.25%
     Val. Loss: 0.597 |  Val. Acc: 68.90%
batch 0, loss 0.580141544342041
batch 100, loss 0.2638660669326782
batch 200, loss 0.4949319064617157
batch 0, loss 0.3330756723880768
batch 100, loss 0.39001500606536865
Epoch: 04 | Epoch Time: 1m 1s
    Train Loss: 0.479 | Train Acc: 77.27%
     Val. Loss: 0.378 |  Val. Acc: 84.53%
batch 0, loss 0.4124695062637329
batch 100, loss 0.5047512054443359
batch 200, loss 0.4246818423271179
batch 0, loss 0.3377535343170166
batch 100, loss 0.29955512285232544
Epoch: 05 | Epoch Time: 1m 1s
    Train Loss: 0.343 | Train Acc: 85.52%
     Val. Loss: 0.309 |  Val. Acc: 87.28%</code></pre><h3 id="我们来尝试一个AVG的版本"><a href="#我们来尝试一个AVG的版本" class="headerlink" title="我们来尝试一个AVG的版本"></a>我们来尝试一个AVG的版本</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">INPUT_DIM = len(TEXT.vocab)</span><br><span class="line">EMBEDDING_DIM = <span class="number">100</span></span><br><span class="line">HIDDEN_DIM = <span class="number">256</span></span><br><span class="line">OUTPUT_DIM = <span class="number">1</span></span><br><span class="line">N_LAYERS = <span class="number">2</span></span><br><span class="line">BIDIRECTIONAL = <span class="literal">True</span></span><br><span class="line">DROPOUT = <span class="number">0.5</span></span><br><span class="line">PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]</span><br><span class="line"></span><br><span class="line">rnn_model_avg = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, </span><br><span class="line">            N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">f'The model has <span class="subst">&#123;count_parameters(rnn_model_avg):,&#125;</span> trainable parameters'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>The model has 4,810,857 trainable parameters</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">rnn_model_avg.embedding.weight.data.copy_(pretrained_embeddings)</span><br><span class="line">UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]</span><br><span class="line"></span><br><span class="line">rnn_model_avg.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)</span><br><span class="line">rnn_model_avg.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)</span><br><span class="line"></span><br><span class="line">print(rnn_model_avg.embedding.weight.data)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],
        ...,
        [-0.7244, -0.0186,  0.0996,  ...,  0.0045, -1.0037,  0.6646],
        [-1.1243,  1.2040, -0.6489,  ..., -0.7526,  0.5711,  1.0081],
        [ 0.0860,  0.1367,  0.0321,  ..., -0.5542, -0.4557, -0.0382]])</code></pre><h2 id="训练RNN模型-1"><a href="#训练RNN模型-1" class="headerlink" title="训练RNN模型"></a>训练RNN模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.Adam(rnn_model_avg.parameters())</span><br><span class="line">rnn_model_avg = rnn_model_avg.to(device)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">N_EPOCHS = <span class="number">5</span></span><br><span class="line">best_valid_loss = float(<span class="string">'inf'</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(N_EPOCHS):</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    train_loss, train_acc = train(rnn_model_avg, train_iterator, optimizer, criterion)</span><br><span class="line">    valid_loss, valid_acc = evaluate(rnn_model_avg, valid_iterator, criterion)</span><br><span class="line">    </span><br><span class="line">    end_time = time.time()</span><br><span class="line"></span><br><span class="line">    epoch_mins, epoch_secs = epoch_time(start_time, end_time)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> valid_loss &lt; best_valid_loss:</span><br><span class="line">        best_valid_loss = valid_loss</span><br><span class="line">        torch.save(rnn_model_avg.state_dict(), <span class="string">'lstm-avg-model.pt'</span>)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">f'Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>:<span class="number">02</span>&#125;</span> | Epoch Time: <span class="subst">&#123;epoch_mins&#125;</span>m <span class="subst">&#123;epoch_secs&#125;</span>s'</span>)</span><br><span class="line">    print(<span class="string">f'\tTrain Loss: <span class="subst">&#123;train_loss:<span class="number">.3</span>f&#125;</span> | Train Acc: <span class="subst">&#123;train_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br><span class="line">    print(<span class="string">f'\t Val. Loss: <span class="subst">&#123;valid_loss:<span class="number">.3</span>f&#125;</span> |  Val. Acc: <span class="subst">&#123;valid_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>batch 0, loss 0.6885155439376831
batch 100, loss 0.5888913869857788
batch 200, loss 0.4656108617782593
batch 0, loss 0.4603933095932007
batch 100, loss 0.38754644989967346
Epoch: 01 | Epoch Time: 1m 20s
    Train Loss: 0.528 | Train Acc: 72.70%
     Val. Loss: 0.362 |  Val. Acc: 84.47%
batch 0, loss 0.29848513007164
batch 100, loss 0.27336984872817993
batch 200, loss 0.35852643847465515
batch 0, loss 0.4745270907878876
batch 100, loss 0.32764753699302673
Epoch: 02 | Epoch Time: 1m 20s
    Train Loss: 0.342 | Train Acc: 85.55%
     Val. Loss: 0.294 |  Val. Acc: 88.03%
batch 0, loss 0.31138738989830017
batch 100, loss 0.3301498591899872
batch 200, loss 0.5036394596099854
batch 0, loss 0.36463940143585205
batch 100, loss 0.3079427480697632
Epoch: 03 | Epoch Time: 1m 20s
    Train Loss: 0.276 | Train Acc: 88.91%
     Val. Loss: 0.257 |  Val. Acc: 89.85%
batch 0, loss 0.19154249131679535
batch 100, loss 0.24453845620155334
batch 200, loss 0.2616804540157318
batch 0, loss 0.4100673198699951
batch 100, loss 0.29790183901786804
Epoch: 04 | Epoch Time: 1m 20s
    Train Loss: 0.230 | Train Acc: 91.25%
     Val. Loss: 0.250 |  Val. Acc: 90.16%
batch 0, loss 0.21265330910682678
batch 100, loss 0.34193551540374756
batch 200, loss 0.19812607765197754
batch 0, loss 0.3696991205215454
batch 100, loss 0.30417782068252563
Epoch: 05 | Epoch Time: 1m 20s
    Train Loss: 0.202 | Train Acc: 92.49%
     Val. Loss: 0.251 |  Val. Acc: 90.26%</code></pre><p>You may have noticed the loss is not really decreasing and the accuracy is poor. This is due to several issues with the model which we’ll improve in the next notebook.</p>
<p>Finally, the metric we actually care about, the test loss and accuracy, which we get from our parameters that gave us the best validation loss.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">rnn_model_avg.load_state_dict(torch.load(<span class="string">'lstm-avg-model.pt'</span>))</span><br><span class="line">test_loss, test_acc = evaluate(model, test_iterator, criterion)</span><br><span class="line">print(<span class="string">f'Test Loss: <span class="subst">&#123;test_loss:<span class="number">.3</span>f&#125;</span> | Test Acc: <span class="subst">&#123;test_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>batch 0, loss 0.24243609607219696
batch 100, loss 0.28235137462615967
batch 200, loss 0.20145781338214874
batch 300, loss 0.3198160231113434
Test Loss: 0.335 | Test Acc: 85.88%</code></pre><h2 id="CNN模型"><a href="#CNN模型" class="headerlink" title="CNN模型"></a>CNN模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_dim, n_filters, </span></span></span><br><span class="line"><span class="function"><span class="params">                 filter_sizes, output_dim, dropout, pad_idx)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        </span><br><span class="line">        self.filter_sizes = filter_sizes</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)</span><br><span class="line">        self.convs = nn.ModuleList([</span><br><span class="line">                                    nn.Conv2d(in_channels = <span class="number">1</span>, out_channels = n_filters, </span><br><span class="line">                                              kernel_size = (fs, embedding_dim)) </span><br><span class="line">                                    <span class="keyword">for</span> fs <span class="keyword">in</span> filter_sizes</span><br><span class="line">                                    ])</span><br><span class="line">        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text, mask)</span>:</span></span><br><span class="line">        <span class="comment"># CHANGED</span></span><br><span class="line">        embedded = self.embedding(text) <span class="comment"># [batch size, sent len, emb dim]</span></span><br><span class="line">        embedded = embedded.unsqueeze(<span class="number">1</span>) <span class="comment"># [batch size, 1, sent len, emb dim]</span></span><br><span class="line">        conved = [F.relu(conv(embedded)).squeeze(<span class="number">3</span>) <span class="keyword">for</span> conv <span class="keyword">in</span> self.convs]</span><br><span class="line">            </span><br><span class="line">        <span class="comment">#conv_n = [batch size, n_filters, sent len - filter_sizes[n]]</span></span><br><span class="line">        </span><br><span class="line"><span class="comment">#         print((1.-mask[:, :-3+1]).unsqueeze(1).byte().shape)</span></span><br><span class="line">        conved = [conv.masked_fill((<span class="number">1.</span>-mask[:, :-filter_size+<span class="number">1</span>]).unsqueeze(<span class="number">1</span>).byte(), <span class="number">-999999</span>) \</span><br><span class="line">                  <span class="keyword">for</span> (conv, filter_size) <span class="keyword">in</span> zip(conved, self.filter_sizes)]</span><br><span class="line">        </span><br><span class="line">        pooled = [F.max_pool1d(conv, conv.shape[<span class="number">2</span>]).squeeze(<span class="number">2</span>) <span class="keyword">for</span> conv <span class="keyword">in</span> conved]</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#pooled_n = [batch size, n_filters]</span></span><br><span class="line">        </span><br><span class="line">        cat = self.dropout(torch.cat(pooled, dim=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment">#cat = [batch size, n_filters * len(filter_sizes)]</span></span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> self.fc(cat)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">INPUT_DIM = len(TEXT.vocab)</span><br><span class="line">EMBEDDING_DIM = <span class="number">100</span></span><br><span class="line">N_FILTERS = <span class="number">100</span></span><br><span class="line">FILTER_SIZES = [<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line">OUTPUT_DIM = <span class="number">1</span></span><br><span class="line">DROPOUT = <span class="number">0.5</span></span><br><span class="line">PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)</span><br><span class="line">model.embedding.weight.data.copy_(pretrained_embeddings)</span><br><span class="line">UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]</span><br><span class="line"></span><br><span class="line">model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)</span><br><span class="line">model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)</span><br><span class="line">model = model.to(device)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.Adam(model.parameters())</span><br><span class="line">criterion = nn.BCEWithLogitsLoss()</span><br><span class="line">criterion = criterion.to(device)</span><br><span class="line"></span><br><span class="line">N_EPOCHS = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">best_valid_loss = float(<span class="string">'inf'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(N_EPOCHS):</span><br><span class="line"></span><br><span class="line">    start_time = time.time()</span><br><span class="line">    </span><br><span class="line">    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)</span><br><span class="line">    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)</span><br><span class="line">    </span><br><span class="line">    end_time = time.time()</span><br><span class="line"></span><br><span class="line">    epoch_mins, epoch_secs = epoch_time(start_time, end_time)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> valid_loss &lt; best_valid_loss:</span><br><span class="line">        best_valid_loss = valid_loss</span><br><span class="line">        torch.save(model.state_dict(), <span class="string">'CNN-model.pt'</span>)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">f'Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>:<span class="number">02</span>&#125;</span> | Epoch Time: <span class="subst">&#123;epoch_mins&#125;</span>m <span class="subst">&#123;epoch_secs&#125;</span>s'</span>)</span><br><span class="line">    print(<span class="string">f'\tTrain Loss: <span class="subst">&#123;train_loss:<span class="number">.3</span>f&#125;</span> | Train Acc: <span class="subst">&#123;train_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br><span class="line">    print(<span class="string">f'\t Val. Loss: <span class="subst">&#123;valid_loss:<span class="number">.3</span>f&#125;</span> |  Val. Acc: <span class="subst">&#123;valid_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>batch 0, loss 0.7456250190734863
batch 100, loss 0.7356712818145752
batch 200, loss 0.608451247215271
batch 0, loss 0.5171981453895569
batch 100, loss 0.5627424716949463
Epoch: 01 | Epoch Time: 0m 11s
    Train Loss: 0.653 | Train Acc: 61.19%
     Val. Loss: 0.511 |  Val. Acc: 78.05%
batch 0, loss 0.5206002593040466
batch 100, loss 0.4522325098514557
batch 200, loss 0.39397668838500977
batch 0, loss 0.36625632643699646
batch 100, loss 0.34350645542144775
Epoch: 02 | Epoch Time: 0m 11s
    Train Loss: 0.430 | Train Acc: 80.41%
     Val. Loss: 0.356 |  Val. Acc: 85.21%
batch 0, loss 0.3453408479690552
batch 100, loss 0.3106832504272461
batch 200, loss 0.29214251041412354
batch 0, loss 0.34314772486686707
batch 100, loss 0.27926790714263916
Epoch: 03 | Epoch Time: 0m 11s
    Train Loss: 0.305 | Train Acc: 87.17%
     Val. Loss: 0.318 |  Val. Acc: 86.52%
batch 0, loss 0.2820616066455841
batch 100, loss 0.2185526192188263
batch 200, loss 0.2295588254928589
batch 0, loss 0.3212977647781372
batch 100, loss 0.2501620352268219
Epoch: 04 | Epoch Time: 0m 11s
    Train Loss: 0.222 | Train Acc: 91.30%
     Val. Loss: 0.311 |  Val. Acc: 87.25%
batch 0, loss 0.06584674119949341
batch 100, loss 0.1338910311460495
batch 200, loss 0.22213703393936157
batch 0, loss 0.32934656739234924
batch 100, loss 0.2596980333328247
Epoch: 05 | Epoch Time: 0m 11s
    Train Loss: 0.162 | Train Acc: 94.01%
     Val. Loss: 0.318 |  Val. Acc: 87.29%</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.load_state_dict(torch.load(<span class="string">'CNN-model.pt'</span>))</span><br><span class="line">test_loss, test_acc = evaluate(model, test_iterator, criterion)</span><br><span class="line">print(<span class="string">f'Test Loss: <span class="subst">&#123;test_loss:<span class="number">.3</span>f&#125;</span> | Test Acc: <span class="subst">&#123;test_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>batch 0, loss 0.1641087532043457
batch 100, loss 0.38564836978912354
batch 200, loss 0.26448047161102295
batch 300, loss 0.4913085401058197
Test Loss: 0.350 | Test Acc: 85.04%</code></pre>
        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2020/02/29/4.sentiment/">
                4.sentiment
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2020-02-29</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            <h1 id="第四课-情感分析"><a href="#第四课-情感分析" class="headerlink" title="第四课 情感分析"></a>第四课 情感分析</h1><p>褚则伟 <a href="mailto:zeweichu@gmail.com">zeweichu@gmail.com</a></p>
<p>学习目标</p>
<ul>
<li>学习和训练文本分类模型</li>
<li>学习torchtext的基本使用方法<ul>
<li>BucketIterator</li>
</ul>
</li>
<li>学习torch.nn的一些基本模型<ul>
<li>Conv2d</li>
</ul>
</li>
</ul>
<p>本notebook参考了<a href="https://github.com/bentrevett/pytorch-sentiment-analysis" target="_blank" rel="noopener">https://github.com/bentrevett/pytorch-sentiment-analysis</a></p>
<p>在这份notebook中，我们会用PyTorch模型和TorchText再来做情感分析(检测一段文字的情感是正面的还是负面的)。我们会使用<a href="http://ai.stanford.edu/~amaas/data/sentiment/" target="_blank" rel="noopener">IMDb 数据集</a>，即电影评论。</p>
<p>模型从简单到复杂，我们会依次构建：</p>
<ul>
<li>Word Averaging模型</li>
<li>RNN/LSTM模型</li>
<li>CNN模型</li>
</ul>
<h2 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h2><ul>
<li>TorchText中的一个重要概念是<code>Field</code>。<code>Field</code>决定了你的数据会被怎样处理。在我们的情感分类任务中，我们所需要接触到的数据有文本字符串和两种情感，”pos”或者”neg”。</li>
<li><code>Field</code>的参数制定了数据会被怎样处理。</li>
<li>我们使用<code>TEXT</code> field来定义如何处理电影评论，使用<code>LABEL</code> field来处理两个情感类别。</li>
<li>我们的<code>TEXT</code> field带有<code>tokenize=&#39;spacy&#39;</code>，这表示我们会用<a href="https://spacy.io" target="_blank" rel="noopener">spaCy</a> tokenizer来tokenize英文句子。如果我们不特别声明<code>tokenize</code>这个参数，那么默认的分词方法是使用空格。</li>
<li>安装spaCy<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install -U spacy</span><br><span class="line">python -m spacy download en</span><br></pre></td></tr></table></figure></li>
<li><code>LABEL</code>由<code>LabelField</code>定义。这是一种特别的用来处理label的<code>Field</code>。我们后面会解释dtype。</li>
<li>更多关于<code>Fields</code>，参见<a href="https://github.com/pytorch/text/blob/master/torchtext/data/field.py" target="_blank" rel="noopener">https://github.com/pytorch/text/blob/master/torchtext/data/field.py</a></li>
<li>和之前一样，我们会设定random seeds使实验可以复现。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchtext <span class="keyword">import</span> data</span><br><span class="line"></span><br><span class="line">SEED = <span class="number">1234</span></span><br><span class="line"></span><br><span class="line">torch.manual_seed(SEED)</span><br><span class="line">torch.cuda.manual_seed(SEED)</span><br><span class="line">torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">TEXT = data.Field(tokenize=<span class="string">'spacy'</span>)</span><br><span class="line">LABEL = data.LabelField(dtype=torch.float)</span><br></pre></td></tr></table></figure>

<ul>
<li>TorchText支持很多常见的自然语言处理数据集。</li>
<li>下面的代码会自动下载IMDb数据集，然后分成train/test两个<code>torchtext.datasets</code>类别。数据被前面的<code>Fields</code>处理。IMDb数据集一共有50000电影评论，每个评论都被标注为正面的或负面的。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchtext <span class="keyword">import</span> datasets</span><br><span class="line">train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)</span><br></pre></td></tr></table></figure>

<p>查看每个数据split有多少条数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">f'Number of training examples: <span class="subst">&#123;len(train_data)&#125;</span>'</span>)</span><br><span class="line">print(<span class="string">f'Number of testing examples: <span class="subst">&#123;len(test_data)&#125;</span>'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Number of training examples: 25000
Number of testing examples: 25000</code></pre><p>查看一个example。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(vars(train_data.examples[<span class="number">0</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>{&apos;text&apos;: [&apos;Brilliant&apos;, &apos;adaptation&apos;, &apos;of&apos;, &apos;the&apos;, &apos;novel&apos;, &apos;that&apos;, &apos;made&apos;, &apos;famous&apos;, &apos;the&apos;, &apos;relatives&apos;, &apos;of&apos;, &apos;Chilean&apos;, &apos;President&apos;, &apos;Salvador&apos;, &apos;Allende&apos;, &apos;killed&apos;, &apos;.&apos;, &apos;In&apos;, &apos;the&apos;, &apos;environment&apos;, &apos;of&apos;, &apos;a&apos;, &apos;large&apos;, &apos;estate&apos;, &apos;that&apos;, &apos;arises&apos;, &apos;from&apos;, &apos;the&apos;, &apos;ruins&apos;, &apos;,&apos;, &apos;becoming&apos;, &apos;a&apos;, &apos;force&apos;, &apos;to&apos;, &apos;abuse&apos;, &apos;and&apos;, &apos;exploitation&apos;, &apos;of&apos;, &apos;outrage&apos;, &apos;,&apos;, &apos;a&apos;, &apos;luxury&apos;, &apos;estate&apos;, &apos;for&apos;, &apos;the&apos;, &apos;benefit&apos;, &apos;of&apos;, &apos;the&apos;, &apos;upstart&apos;, &apos;Esteban&apos;, &apos;Trueba&apos;, &apos;and&apos;, &apos;his&apos;, &apos;undeserved&apos;, &apos;family&apos;, &apos;,&apos;, &apos;the&apos;, &apos;brilliant&apos;, &apos;Danish&apos;, &apos;director&apos;, &apos;Bille&apos;, &apos;August&apos;, &apos;recreates&apos;, &apos;,&apos;, &apos;in&apos;, &apos;micro&apos;, &apos;,&apos;, &apos;which&apos;, &apos;at&apos;, &apos;the&apos;, &apos;time&apos;, &apos;would&apos;, &apos;be&apos;, &apos;the&apos;, &apos;process&apos;, &apos;leading&apos;, &apos;to&apos;, &apos;the&apos;, &apos;greatest&apos;, &apos;infamy&apos;, &apos;of&apos;, &apos;his&apos;, &apos;story&apos;, &apos;to&apos;, &apos;the&apos;, &apos;hardened&apos;, &apos;Chilean&apos;, &apos;nation&apos;, &apos;,&apos;, &apos;and&apos;, &apos;whose&apos;, &apos;main&apos;, &apos;character&apos;, &apos;would&apos;, &apos;Augusto&apos;, &apos;Pinochet&apos;, &apos;(&apos;, &apos;Stephen&apos;, &apos;similarities&apos;, &apos;with&apos;, &apos;it&apos;, &apos;are&apos;, &apos;inevitable&apos;, &apos;:&apos;, &apos;recall&apos;, &apos;,&apos;, &apos;as&apos;, &apos;an&apos;, &apos;example&apos;, &apos;,&apos;, &apos;that&apos;, &apos;image&apos;, &apos;of&apos;, &apos;the&apos;, &apos;senator&apos;, &apos;with&apos;, &apos;dark&apos;, &apos;glasses&apos;, &apos;that&apos;, &apos;makes&apos;, &apos;him&apos;, &apos;the&apos;, &apos;wink&apos;, &apos;to&apos;, &apos;the&apos;, &apos;general&apos;, &apos;to&apos;, &apos;begin&apos;, &apos;making&apos;, &apos;the&apos;, &apos;palace).&lt;br&apos;, &apos;/&gt;&lt;br&apos;, &apos;/&gt;Bille&apos;, &apos;August&apos;, &apos;attends&apos;, &apos;an&apos;, &apos;exceptional&apos;, &apos;cast&apos;, &apos;in&apos;, &apos;the&apos;, &apos;Jeremy&apos;, &apos;protruding&apos;, &apos;Irons&apos;, &apos;,&apos;, &apos;whose&apos;, &apos;character&apos;, &apos;changes&apos;, &apos;from&apos;, &apos;arrogance&apos;, &apos;and&apos;, &apos;extreme&apos;, &apos;cruelty&apos;, &apos;,&apos;, &apos;the&apos;, &apos;hard&apos;, &apos;lesson&apos;, &apos;that&apos;, &apos;life&apos;, &apos;always&apos;, &apos;brings&apos;, &apos;us&apos;, &apos;to&apos;, &apos;almost&apos;, &apos;force&apos;, &apos;us&apos;, &apos;to&apos;, &apos;change&apos;, &apos;.&apos;, &apos;In&apos;, &apos;Esteban&apos;, &apos;fully&apos;, &apos;applies&apos;, &apos;the&apos;, &apos;law&apos;, &apos;of&apos;, &apos;resonance&apos;, &apos;,&apos;, &apos;with&apos;, &apos;great&apos;, &apos;wisdom&apos;, &apos;,&apos;, &apos;Solomon&apos;, &apos;describes&apos;, &apos;in&apos;, &apos;these&apos;, &apos;words:&quot;The&apos;, &apos;things&apos;, &apos;that&apos;, &apos;freckles&apos;, &apos;are&apos;, &apos;the&apos;, &apos;same&apos;, &apos;punishment&apos;, &apos;that&apos;, &apos;will&apos;, &apos;serve&apos;, &apos;you&apos;, &apos;.&apos;, &apos;&quot;&apos;, &apos;&lt;&apos;, &apos;br&apos;, &apos;/&gt;&lt;br&apos;, &apos;/&gt;Unforgettable&apos;, &apos;Glenn&apos;, &apos;Close&apos;, &apos;playing&apos;, &apos;splint&apos;, &apos;,&apos;, &apos;the&apos;, &apos;tainted&apos;, &apos;sister&apos;, &apos;of&apos;, &apos;Stephen&apos;, &apos;,&apos;, &apos;whose&apos;, &apos;sin&apos;, &apos;,&apos;, &apos;driven&apos;, &apos;by&apos;, &apos;loneliness&apos;, &apos;,&apos;, &apos;spiritual&apos;, &apos;and&apos;, &apos;platonic&apos;, &apos;love&apos;, &apos;was&apos;, &apos;the&apos;, &apos;wife&apos;, &apos;of&apos;, &apos;his&apos;, &apos;cruel&apos;, &apos;snowy&apos;, &apos;brother&apos;, &apos;.&apos;, &apos;Meryl&apos;, &apos;Streep&apos;, &apos;also&apos;, &apos;brilliant&apos;, &apos;,&apos;, &apos;a&apos;, &apos;woman&apos;, &apos;whose&apos;, &apos;name&apos;, &apos;came&apos;, &apos;to&apos;, &apos;him&apos;, &apos;like&apos;, &apos;a&apos;, &apos;glove&apos;, &apos;Clara&apos;, &apos;.&apos;, &apos;With&apos;, &apos;telekinetic&apos;, &apos;powers&apos;, &apos;,&apos;, &apos;cognitive&apos;, &apos;and&apos;, &apos;mediumistic&apos;, &apos;,&apos;, &apos;this&apos;, &apos;hardened&apos;, &apos;woman&apos;, &apos;,&apos;, &apos;loyal&apos;, &apos;to&apos;, &apos;his&apos;, &apos;blunt&apos;, &apos;,&apos;, &apos;conservative&apos;, &apos;husband&apos;, &apos;,&apos;, &apos;is&apos;, &apos;an&apos;, &apos;indicator&apos;, &apos;of&apos;, &apos;character&apos;, &apos;and&apos;, &apos;self&apos;, &apos;-&apos;, &apos;control&apos;, &apos;that&apos;, &apos;we&apos;, &apos;wish&apos;, &apos;for&apos;, &apos;ourselves&apos;, &apos;and&apos;, &apos;for&apos;, &apos;all&apos;, &apos;human&apos;, &apos;beings&apos;, &apos;.&apos;, &apos;&lt;&apos;, &apos;br&apos;, &apos;/&gt;&lt;br&apos;, &apos;/&gt;Every&apos;, &apos;character&apos;, &apos;is&apos;, &apos;a&apos;, &apos;portrait&apos;, &apos;of&apos;, &apos;virtuosity&apos;, &apos;(&apos;, &apos;as&apos;, &apos;Blanca&apos;, &apos;worthy&apos;, &apos;rebel&apos;, &apos;leader&apos;, &apos;Pedro&apos;, &apos;Segundo&apos;, &apos;unhappy&apos;, &apos;...&apos;, &apos;)&apos;, &apos;or&apos;, &apos;a&apos;, &apos;portrait&apos;, &apos;of&apos;, &apos;humiliation&apos;, &apos;,&apos;, &apos;like&apos;, &apos;Stephen&apos;, &apos;Jr.&apos;, &apos;,&apos;, &apos;the&apos;, &apos;bastard&apos;, &apos;child&apos;, &apos;of&apos;, &apos;Senator&apos;, &apos;,&apos;, &apos;who&apos;, &apos;serves&apos;, &apos;as&apos;, &apos;an&apos;, &apos;instrument&apos;, &apos;for&apos;, &apos;the&apos;, &apos;return&apos;, &apos;of&apos;, &apos;the&apos;, &apos;boomerang&apos;, &apos;.&apos;, &apos;&lt;&apos;, &apos;br&apos;, &apos;/&gt;&lt;br&apos;, &apos;/&gt;The&apos;, &apos;film&apos;, &apos;moves&apos;, &apos;the&apos;, &apos;bowels&apos;, &apos;,&apos;, &apos;we&apos;, &apos;recreated&apos;, &apos;some&apos;, &apos;facts&apos;, &apos;that&apos;, &apos;should&apos;, &apos;not&apos;, &apos;ever&apos;, &apos;be&apos;, &apos;repeated&apos;, &apos;,&apos;, &apos;but&apos;, &apos;that&apos;, &apos;absurdly&apos;, &apos;still&apos;, &apos;happen&apos;, &apos;(&apos;, &apos;Colombia&apos;, &apos;is&apos;, &apos;a&apos;, &apos;sad&apos;, &apos;example&apos;, &apos;)&apos;, &apos;and&apos;, &apos;another&apos;, &apos;reminder&apos;, &apos;that&apos;, &apos;,&apos;, &apos;against&apos;, &apos;all&apos;, &apos;,&apos;, &apos;life&apos;, &apos;is&apos;, &apos;wonderful&apos;, &apos;because&apos;, &apos;there&apos;, &apos;are&apos;, &apos;always&apos;, &apos;people&apos;, &apos;like&apos;, &apos;Isabel&apos;, &apos;Allende&apos;, &apos;and&apos;, &apos;immortalize&apos;, &apos;just&apos;, &apos;Bille&apos;, &apos;August&apos;, &apos;.&apos;], &apos;label&apos;: &apos;pos&apos;}</code></pre><ul>
<li>由于我们现在只有train/test这两个分类，所以我们需要创建一个新的validation set。我们可以使用<code>.split()</code>创建新的分类。</li>
<li>默认的数据分割是 70、30，如果我们声明<code>split_ratio</code>，可以改变split之间的比例，<code>split_ratio=0.8</code>表示80%的数据是训练集，20%是验证集。</li>
<li>我们还声明<code>random_state</code>这个参数，确保我们每次分割的数据集都是一样的。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line">train_data, valid_data = train_data.split(random_state=random.seed(SEED))</span><br></pre></td></tr></table></figure>

<p>检查一下现在每个部分有多少条数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">f'Number of training examples: <span class="subst">&#123;len(train_data)&#125;</span>'</span>)</span><br><span class="line">print(<span class="string">f'Number of validation examples: <span class="subst">&#123;len(valid_data)&#125;</span>'</span>)</span><br><span class="line">print(<span class="string">f'Number of testing examples: <span class="subst">&#123;len(test_data)&#125;</span>'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Number of training examples: 17500
Number of validation examples: 7500
Number of testing examples: 25000</code></pre><ul>
<li>下一步我们需要创建 <em>vocabulary</em> 。<em>vocabulary</em> 就是把每个单词一一映射到一个数字。<br><img src="assets/sentiment5.png" alt=""></li>
<li>我们使用最常见的25k个单词来构建我们的单词表，用<code>max_size</code>这个参数可以做到这一点。</li>
<li>所有其他的单词都用<code>&lt;unk&gt;</code>来表示。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TEXT.build_vocab(train_data, max_size=25000)</span></span><br><span class="line"><span class="comment"># LABEL.build_vocab(train_data)</span></span><br><span class="line">TEXT.build_vocab(train_data, max_size=<span class="number">25000</span>, vectors=<span class="string">"glove.6B.100d"</span>, unk_init=torch.Tensor.normal_)</span><br><span class="line">LABEL.build_vocab(train_data)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">f"Unique tokens in TEXT vocabulary: <span class="subst">&#123;len(TEXT.vocab)&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Unique tokens in LABEL vocabulary: <span class="subst">&#123;len(LABEL.vocab)&#125;</span>"</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Unique tokens in TEXT vocabulary: 25002
Unique tokens in LABEL vocabulary: 2</code></pre><ul>
<li>当我们把句子传进模型的时候，我们是按照一个个 <em>batch</em> 穿进去的，也就是说，我们一次传入了好几个句子，而且每个batch中的句子必须是相同的长度。为了确保句子的长度相同，TorchText会把短的句子pad到和最长的句子等长。<br><img src="assets/sentiment6.png" alt=""></li>
<li>下面我们来看看训练数据集中最常见的单词。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(TEXT.vocab.freqs.most_common(<span class="number">20</span>))</span><br></pre></td></tr></table></figure>

<pre><code>[(&apos;the&apos;, 201455), (&apos;,&apos;, 192552), (&apos;.&apos;, 164402), (&apos;a&apos;, 108963), (&apos;and&apos;, 108649), (&apos;of&apos;, 100010), (&apos;to&apos;, 92873), (&apos;is&apos;, 76046), (&apos;in&apos;, 60904), (&apos;I&apos;, 54486), (&apos;it&apos;, 53405), (&apos;that&apos;, 49155), (&apos;&quot;&apos;, 43890), (&quot;&apos;s&quot;, 43151), (&apos;this&apos;, 42454), (&apos;-&apos;, 36769), (&apos;/&gt;&lt;br&apos;, 35511), (&apos;was&apos;, 34990), (&apos;as&apos;, 30324), (&apos;with&apos;, 29691)]</code></pre><p>我们可以直接用 <code>stoi</code>(<strong>s</strong>tring <strong>to</strong> <strong>i</strong>nt) 或者 <code>itos</code> (<strong>i</strong>nt <strong>to</strong>  <strong>s</strong>tring) 来查看我们的单词表。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(TEXT.vocab.itos[:<span class="number">10</span>])</span><br></pre></td></tr></table></figure>

<pre><code>[&apos;&lt;unk&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;the&apos;, &apos;,&apos;, &apos;.&apos;, &apos;a&apos;, &apos;and&apos;, &apos;of&apos;, &apos;to&apos;, &apos;is&apos;]</code></pre><p>查看labels。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(LABEL.vocab.stoi)</span><br></pre></td></tr></table></figure>

<pre><code>defaultdict(&lt;function _default_unk_index at 0x7fbec39a79d8&gt;, {&apos;neg&apos;: 0, &apos;pos&apos;: 1})</code></pre><ul>
<li>最后一步数据的准备是创建iterators。每个itartion都会返回一个batch的examples。</li>
<li>我们会使用<code>BucketIterator</code>。<code>BucketIterator</code>会把长度差不多的句子放到同一个batch中，确保每个batch中不出现太多的padding。</li>
<li>严格来说，我们这份notebook中的模型代码都有一个问题，也就是我们把<code>&lt;pad&gt;</code>也当做了模型的输入进行训练。更好的做法是在模型中把由<code>&lt;pad&gt;</code>产生的输出给消除掉。在这节课中我们简单处理，直接把<code>&lt;pad&gt;</code>也用作模型输入了。由于<code>&lt;pad&gt;</code>数量不多，模型的效果也不差。</li>
<li>如果我们有GPU，还可以指定每个iteration返回的tensor都在GPU上。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">BATCH_SIZE = <span class="number">64</span></span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line">train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(</span><br><span class="line">    (train_data, valid_data, test_data), </span><br><span class="line">    batch_size=BATCH_SIZE,</span><br><span class="line">    device=device)</span><br></pre></td></tr></table></figure>

<h2 id="Word-Averaging模型"><a href="#Word-Averaging模型" class="headerlink" title="Word Averaging模型"></a>Word Averaging模型</h2><ul>
<li>我们首先介绍一个简单的Word Averaging模型。这个模型非常简单，我们把每个单词都通过<code>Embedding</code>层投射成word embedding vector，然后把一句话中的所有word vector做个平均，就是整个句子的vector表示了。接下来把这个sentence vector传入一个<code>Linear</code>层，做分类即可。</li>
</ul>
<p><img src="assets/sentiment8.png" alt=""></p>
<ul>
<li>我们使用<a href="https://pytorch.org/docs/stable/nn.html?highlight=avg_pool2d#torch.nn.functional.avg_pool2d" target="_blank" rel="noopener"><code>avg_pool2d</code></a>来做average pooling。我们的目标是把sentence length那个维度平均成1，然后保留embedding这个维度。</li>
</ul>
<p><img src="assets/sentiment9.png" alt=""></p>
<ul>
<li><code>avg_pool2d</code>的kernel size是 (<code>embedded.shape[1]</code>, 1)，所以句子长度的那个维度会被压扁。</li>
</ul>
<p><img src="assets/sentiment10.png" alt=""></p>
<p><img src="assets/sentiment11.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordAVGModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_dim, output_dim, pad_idx)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)</span><br><span class="line">        self.fc = nn.Linear(embedding_dim, output_dim)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text)</span>:</span></span><br><span class="line">        embedded = self.embedding(text) <span class="comment"># [sent len, batch size, emb dim]</span></span><br><span class="line">        embedded = embedded.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>) <span class="comment"># [batch size, sent len, emb dim]</span></span><br><span class="line">        pooled = F.avg_pool2d(embedded, (embedded.shape[<span class="number">1</span>], <span class="number">1</span>)).squeeze(<span class="number">1</span>) <span class="comment"># [batch size, embedding_dim]</span></span><br><span class="line">        <span class="keyword">return</span> self.fc(pooled)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">INPUT_DIM = len(TEXT.vocab)</span><br><span class="line">EMBEDDING_DIM = <span class="number">100</span></span><br><span class="line">OUTPUT_DIM = <span class="number">1</span></span><br><span class="line">PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]</span><br><span class="line"></span><br><span class="line">model = WordAVGModel(INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM, PAD_IDX)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_parameters</span><span class="params">(model)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> sum(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f'The model has <span class="subst">&#123;count_parameters(model):,&#125;</span> trainable parameters'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>The model has 2,500,301 trainable parameters</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pretrained_embeddings = TEXT.vocab.vectors</span><br><span class="line">model.embedding.weight.data.copy_(pretrained_embeddings)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],
        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],
        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],
        ...,
        [-0.7244, -0.0186,  0.0996,  ...,  0.0045, -1.0037,  0.6646],
        [-1.1243,  1.2040, -0.6489,  ..., -0.7526,  0.5711,  1.0081],
        [ 0.0860,  0.1367,  0.0321,  ..., -0.5542, -0.4557, -0.0382]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]</span><br><span class="line"></span><br><span class="line">model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)</span><br><span class="line">model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)</span><br></pre></td></tr></table></figure>

<h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">optimizer = optim.Adam(model.parameters())</span><br><span class="line">criterion = nn.BCEWithLogitsLoss()</span><br><span class="line">model = model.to(device)</span><br><span class="line">criterion = criterion.to(device)</span><br></pre></td></tr></table></figure>

<p>计算预测的准确率</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_accuracy</span><span class="params">(preds, y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#round predictions to the closest integer</span></span><br><span class="line">    rounded_preds = torch.round(torch.sigmoid(preds))</span><br><span class="line">    correct = (rounded_preds == y).float() <span class="comment">#convert into float for division </span></span><br><span class="line">    acc = correct.sum()/len(correct)</span><br><span class="line">    <span class="keyword">return</span> acc</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(model, iterator, optimizer, criterion)</span>:</span></span><br><span class="line">    </span><br><span class="line">    epoch_loss = <span class="number">0</span></span><br><span class="line">    epoch_acc = <span class="number">0</span></span><br><span class="line">    model.train()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> iterator:</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        predictions = model(batch.text).squeeze(<span class="number">1</span>)</span><br><span class="line">        loss = criterion(predictions, batch.label)</span><br><span class="line">        acc = binary_accuracy(predictions, batch.label)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        epoch_loss += loss.item()</span><br><span class="line">        epoch_acc += acc.item()</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> epoch_loss / len(iterator), epoch_acc / len(iterator)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(model, iterator, criterion)</span>:</span></span><br><span class="line">    </span><br><span class="line">    epoch_loss = <span class="number">0</span></span><br><span class="line">    epoch_acc = <span class="number">0</span></span><br><span class="line">    model.eval()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> iterator:</span><br><span class="line">            predictions = model(batch.text).squeeze(<span class="number">1</span>)</span><br><span class="line">            loss = criterion(predictions, batch.label)</span><br><span class="line">            acc = binary_accuracy(predictions, batch.label)</span><br><span class="line">            epoch_loss += loss.item()</span><br><span class="line">            epoch_acc += acc.item()</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> epoch_loss / len(iterator), epoch_acc / len(iterator)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">epoch_time</span><span class="params">(start_time, end_time)</span>:</span></span><br><span class="line">    elapsed_time = end_time - start_time</span><br><span class="line">    elapsed_mins = int(elapsed_time / <span class="number">60</span>)</span><br><span class="line">    elapsed_secs = int(elapsed_time - (elapsed_mins * <span class="number">60</span>))</span><br><span class="line">    <span class="keyword">return</span> elapsed_mins, elapsed_secs</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">N_EPOCHS = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">best_valid_loss = float(<span class="string">'inf'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(N_EPOCHS):</span><br><span class="line"></span><br><span class="line">    start_time = time.time()</span><br><span class="line">    </span><br><span class="line">    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)</span><br><span class="line">    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)</span><br><span class="line">    </span><br><span class="line">    end_time = time.time()</span><br><span class="line"></span><br><span class="line">    epoch_mins, epoch_secs = epoch_time(start_time, end_time)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> valid_loss &lt; best_valid_loss:</span><br><span class="line">        best_valid_loss = valid_loss</span><br><span class="line">        torch.save(model.state_dict(), <span class="string">'wordavg-model.pt'</span>)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">f'Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>:<span class="number">02</span>&#125;</span> | Epoch Time: <span class="subst">&#123;epoch_mins&#125;</span>m <span class="subst">&#123;epoch_secs&#125;</span>s'</span>)</span><br><span class="line">    print(<span class="string">f'\tTrain Loss: <span class="subst">&#123;train_loss:<span class="number">.3</span>f&#125;</span> | Train Acc: <span class="subst">&#123;train_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br><span class="line">    print(<span class="string">f'\t Val. Loss: <span class="subst">&#123;valid_loss:<span class="number">.3</span>f&#125;</span> |  Val. Acc: <span class="subst">&#123;valid_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Epoch: 01 | Epoch Time: 0m 2s
    Train Loss: 0.685 | Train Acc: 56.84%
     Val. Loss: 0.622 |  Val. Acc: 71.09%
Epoch: 02 | Epoch Time: 0m 2s
    Train Loss: 0.642 | Train Acc: 71.31%
     Val. Loss: 0.510 |  Val. Acc: 75.48%
Epoch: 03 | Epoch Time: 0m 2s
    Train Loss: 0.573 | Train Acc: 78.31%
     Val. Loss: 0.449 |  Val. Acc: 79.52%
Epoch: 04 | Epoch Time: 0m 2s
    Train Loss: 0.503 | Train Acc: 82.78%
     Val. Loss: 0.419 |  Val. Acc: 82.72%
Epoch: 05 | Epoch Time: 0m 2s
    Train Loss: 0.440 | Train Acc: 85.84%
     Val. Loss: 0.408 |  Val. Acc: 84.75%
Epoch: 06 | Epoch Time: 0m 2s
    Train Loss: 0.389 | Train Acc: 87.59%
     Val. Loss: 0.413 |  Val. Acc: 86.02%
Epoch: 07 | Epoch Time: 0m 2s
    Train Loss: 0.352 | Train Acc: 88.85%
     Val. Loss: 0.425 |  Val. Acc: 86.92%
Epoch: 08 | Epoch Time: 0m 2s
    Train Loss: 0.320 | Train Acc: 89.93%
     Val. Loss: 0.440 |  Val. Acc: 87.54%
Epoch: 09 | Epoch Time: 0m 2s
    Train Loss: 0.294 | Train Acc: 90.74%
     Val. Loss: 0.456 |  Val. Acc: 88.09%
Epoch: 10 | Epoch Time: 0m 2s
    Train Loss: 0.274 | Train Acc: 91.27%
     Val. Loss: 0.468 |  Val. Acc: 88.49%</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line">nlp = spacy.load(<span class="string">'en'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_sentiment</span><span class="params">(sentence)</span>:</span></span><br><span class="line">    tokenized = [tok.text <span class="keyword">for</span> tok <span class="keyword">in</span> nlp.tokenizer(sentence)]</span><br><span class="line">    indexed = [TEXT.vocab.stoi[t] <span class="keyword">for</span> t <span class="keyword">in</span> tokenized]</span><br><span class="line">    tensor = torch.LongTensor(indexed).to(device)</span><br><span class="line">    tensor = tensor.unsqueeze(<span class="number">1</span>)</span><br><span class="line">    prediction = torch.sigmoid(model(tensor))</span><br><span class="line">    <span class="keyword">return</span> prediction.item()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict_sentiment(<span class="string">"This film is terrible"</span>)</span><br></pre></td></tr></table></figure>




<pre><code>5.568591932965664e-26</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict_sentiment(<span class="string">"This film is great"</span>)</span><br></pre></td></tr></table></figure>




<pre><code>1.0</code></pre><h2 id="RNN模型"><a href="#RNN模型" class="headerlink" title="RNN模型"></a>RNN模型</h2><ul>
<li>下面我们尝试把模型换成一个<strong>recurrent neural network</strong> (RNN)。RNN经常会被用来encode一个sequence<br>$$h_t = \text{RNN}(x_t, h_{t-1})$$</li>
<li>我们使用最后一个hidden state $h_T$来表示整个句子。</li>
<li>然后我们把$h_T$通过一个线性变换$f$，然后用来预测句子的情感。</li>
</ul>
<p><img src="assets/sentiment1.png" alt=""></p>
<p><img src="assets/sentiment7.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_dim, hidden_dim, output_dim, </span></span></span><br><span class="line"><span class="function"><span class="params">                 n_layers, bidirectional, dropout, pad_idx)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)</span><br><span class="line">        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, </span><br><span class="line">                           bidirectional=bidirectional, dropout=dropout)</span><br><span class="line">        self.fc = nn.Linear(hidden_dim*<span class="number">2</span>, output_dim)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text)</span>:</span></span><br><span class="line">        embedded = self.dropout(self.embedding(text)) <span class="comment">#[sent len, batch size, emb dim]</span></span><br><span class="line">        output, (hidden, cell) = self.rnn(embedded)</span><br><span class="line">        <span class="comment">#output = [sent len, batch size, hid dim * num directions]</span></span><br><span class="line">        <span class="comment">#hidden = [num layers * num directions, batch size, hid dim]</span></span><br><span class="line">        <span class="comment">#cell = [num layers * num directions, batch size, hid dim]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers</span></span><br><span class="line">        <span class="comment">#and apply dropout</span></span><br><span class="line">        hidden = self.dropout(torch.cat((hidden[<span class="number">-2</span>,:,:], hidden[<span class="number">-1</span>,:,:]), dim=<span class="number">1</span>)) <span class="comment"># [batch size, hid dim * num directions]</span></span><br><span class="line">        <span class="keyword">return</span> self.fc(hidden.squeeze(<span class="number">0</span>))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">INPUT_DIM = len(TEXT.vocab)</span><br><span class="line">EMBEDDING_DIM = <span class="number">100</span></span><br><span class="line">HIDDEN_DIM = <span class="number">256</span></span><br><span class="line">OUTPUT_DIM = <span class="number">1</span></span><br><span class="line">N_LAYERS = <span class="number">2</span></span><br><span class="line">BIDIRECTIONAL = <span class="literal">True</span></span><br><span class="line">DROPOUT = <span class="number">0.5</span></span><br><span class="line">PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]</span><br><span class="line"></span><br><span class="line">model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, </span><br><span class="line">            N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">f'The model has <span class="subst">&#123;count_parameters(model):,&#125;</span> trainable parameters'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>The model has 4,810,857 trainable parameters</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model.embedding.weight.data.copy_(pretrained_embeddings)</span><br><span class="line">UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]</span><br><span class="line"></span><br><span class="line">model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)</span><br><span class="line">model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)</span><br><span class="line"></span><br><span class="line">print(model.embedding.weight.data)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],
        ...,
        [-0.7244, -0.0186,  0.0996,  ...,  0.0045, -1.0037,  0.6646],
        [-1.1243,  1.2040, -0.6489,  ..., -0.7526,  0.5711,  1.0081],
        [ 0.0860,  0.1367,  0.0321,  ..., -0.5542, -0.4557, -0.0382]],
       device=&apos;cuda:0&apos;)</code></pre><h2 id="训练RNN模型"><a href="#训练RNN模型" class="headerlink" title="训练RNN模型"></a>训练RNN模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.Adam(model.parameters())</span><br><span class="line">model = model.to(device)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">N_EPOCHS = <span class="number">5</span></span><br><span class="line">best_valid_loss = float(<span class="string">'inf'</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(N_EPOCHS):</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)</span><br><span class="line">    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)</span><br><span class="line">    </span><br><span class="line">    end_time = time.time()</span><br><span class="line"></span><br><span class="line">    epoch_mins, epoch_secs = epoch_time(start_time, end_time)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> valid_loss &lt; best_valid_loss:</span><br><span class="line">        best_valid_loss = valid_loss</span><br><span class="line">        torch.save(model.state_dict(), <span class="string">'lstm-model.pt'</span>)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">f'Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>:<span class="number">02</span>&#125;</span> | Epoch Time: <span class="subst">&#123;epoch_mins&#125;</span>m <span class="subst">&#123;epoch_secs&#125;</span>s'</span>)</span><br><span class="line">    print(<span class="string">f'\tTrain Loss: <span class="subst">&#123;train_loss:<span class="number">.3</span>f&#125;</span> | Train Acc: <span class="subst">&#123;train_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br><span class="line">    print(<span class="string">f'\t Val. Loss: <span class="subst">&#123;valid_loss:<span class="number">.3</span>f&#125;</span> |  Val. Acc: <span class="subst">&#123;valid_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Epoch: 01 | Epoch Time: 1m 29s
    Train Loss: 0.676 | Train Acc: 57.69%
     Val. Loss: 0.694 |  Val. Acc: 53.40%
Epoch: 02 | Epoch Time: 1m 29s
    Train Loss: 0.641 | Train Acc: 63.77%
     Val. Loss: 0.744 |  Val. Acc: 49.22%
Epoch: 03 | Epoch Time: 1m 29s
    Train Loss: 0.618 | Train Acc: 65.77%
     Val. Loss: 0.534 |  Val. Acc: 73.72%
Epoch: 04 | Epoch Time: 1m 30s
    Train Loss: 0.634 | Train Acc: 63.79%
     Val. Loss: 0.619 |  Val. Acc: 66.85%
Epoch: 05 | Epoch Time: 1m 29s
    Train Loss: 0.448 | Train Acc: 79.19%
     Val. Loss: 0.340 |  Val. Acc: 86.63%</code></pre><p>You may have noticed the loss is not really decreasing and the accuracy is poor. This is due to several issues with the model which we’ll improve in the next notebook.</p>
<p>Finally, the metric we actually care about, the test loss and accuracy, which we get from our parameters that gave us the best validation loss.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.load_state_dict(torch.load(<span class="string">'lstm-model.pt'</span>))</span><br><span class="line">test_loss, test_acc = evaluate(model, test_iterator, criterion)</span><br><span class="line">print(<span class="string">f'Test Loss: <span class="subst">&#123;test_loss:<span class="number">.3</span>f&#125;</span> | Test Acc: <span class="subst">&#123;test_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br></pre></td></tr></table></figure>

<h2 id="CNN模型"><a href="#CNN模型" class="headerlink" title="CNN模型"></a>CNN模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_dim, n_filters, </span></span></span><br><span class="line"><span class="function"><span class="params">                 filter_sizes, output_dim, dropout, pad_idx)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        </span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)</span><br><span class="line">        self.convs = nn.ModuleList([</span><br><span class="line">                                    nn.Conv2d(in_channels = <span class="number">1</span>, out_channels = n_filters, </span><br><span class="line">                                              kernel_size = (fs, embedding_dim)) </span><br><span class="line">                                    <span class="keyword">for</span> fs <span class="keyword">in</span> filter_sizes</span><br><span class="line">                                    ])</span><br><span class="line">        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text)</span>:</span></span><br><span class="line">        text = text.permute(<span class="number">1</span>, <span class="number">0</span>) <span class="comment"># [batch size, sent len]</span></span><br><span class="line">        embedded = self.embedding(text) <span class="comment"># [batch size, sent len, emb dim]</span></span><br><span class="line">        embedded = embedded.unsqueeze(<span class="number">1</span>) <span class="comment"># [batch size, 1, sent len, emb dim]</span></span><br><span class="line">        conved = [F.relu(conv(embedded)).squeeze(<span class="number">3</span>) <span class="keyword">for</span> conv <span class="keyword">in</span> self.convs]</span><br><span class="line">            </span><br><span class="line">        <span class="comment">#conv_n = [batch size, n_filters, sent len - filter_sizes[n]]</span></span><br><span class="line">        </span><br><span class="line">        pooled = [F.max_pool1d(conv, conv.shape[<span class="number">2</span>]).squeeze(<span class="number">2</span>) <span class="keyword">for</span> conv <span class="keyword">in</span> conved]</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#pooled_n = [batch size, n_filters]</span></span><br><span class="line">        </span><br><span class="line">        cat = self.dropout(torch.cat(pooled, dim=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment">#cat = [batch size, n_filters * len(filter_sizes)]</span></span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> self.fc(cat)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">INPUT_DIM = len(TEXT.vocab)</span><br><span class="line">EMBEDDING_DIM = <span class="number">100</span></span><br><span class="line">N_FILTERS = <span class="number">100</span></span><br><span class="line">FILTER_SIZES = [<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line">OUTPUT_DIM = <span class="number">1</span></span><br><span class="line">DROPOUT = <span class="number">0.5</span></span><br><span class="line">PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)</span><br><span class="line">model.embedding.weight.data.copy_(pretrained_embeddings)</span><br><span class="line">UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]</span><br><span class="line"></span><br><span class="line">model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)</span><br><span class="line">model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)</span><br><span class="line">model = model.to(device)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.Adam(model.parameters())</span><br><span class="line">criterion = nn.BCEWithLogitsLoss()</span><br><span class="line">criterion = criterion.to(device)</span><br><span class="line"></span><br><span class="line">N_EPOCHS = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">best_valid_loss = float(<span class="string">'inf'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(N_EPOCHS):</span><br><span class="line"></span><br><span class="line">    start_time = time.time()</span><br><span class="line">    </span><br><span class="line">    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)</span><br><span class="line">    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)</span><br><span class="line">    </span><br><span class="line">    end_time = time.time()</span><br><span class="line"></span><br><span class="line">    epoch_mins, epoch_secs = epoch_time(start_time, end_time)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> valid_loss &lt; best_valid_loss:</span><br><span class="line">        best_valid_loss = valid_loss</span><br><span class="line">        torch.save(model.state_dict(), <span class="string">'CNN-model.pt'</span>)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">f'Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>:<span class="number">02</span>&#125;</span> | Epoch Time: <span class="subst">&#123;epoch_mins&#125;</span>m <span class="subst">&#123;epoch_secs&#125;</span>s'</span>)</span><br><span class="line">    print(<span class="string">f'\tTrain Loss: <span class="subst">&#123;train_loss:<span class="number">.3</span>f&#125;</span> | Train Acc: <span class="subst">&#123;train_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br><span class="line">    print(<span class="string">f'\t Val. Loss: <span class="subst">&#123;valid_loss:<span class="number">.3</span>f&#125;</span> |  Val. Acc: <span class="subst">&#123;valid_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Epoch: 01 | Epoch Time: 0m 11s
    Train Loss: 0.645 | Train Acc: 62.12%
     Val. Loss: 0.485 |  Val. Acc: 79.61%
Epoch: 02 | Epoch Time: 0m 11s
    Train Loss: 0.423 | Train Acc: 80.59%
     Val. Loss: 0.360 |  Val. Acc: 84.63%
Epoch: 03 | Epoch Time: 0m 11s
    Train Loss: 0.302 | Train Acc: 87.33%
     Val. Loss: 0.320 |  Val. Acc: 86.59%
Epoch: 04 | Epoch Time: 0m 11s
    Train Loss: 0.222 | Train Acc: 91.20%
     Val. Loss: 0.306 |  Val. Acc: 87.17%
Epoch: 05 | Epoch Time: 0m 11s
    Train Loss: 0.161 | Train Acc: 93.99%
     Val. Loss: 0.325 |  Val. Acc: 86.82%</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.load_state_dict(torch.load(<span class="string">'CNN-model.pt'</span>))</span><br><span class="line">test_loss, test_acc = evaluate(model, test_iterator, criterion)</span><br><span class="line">print(<span class="string">f'Test Loss: <span class="subst">&#123;test_loss:<span class="number">.3</span>f&#125;</span> | Test Acc: <span class="subst">&#123;test_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Test Loss: 0.336 | Test Acc: 85.66%</code></pre>
        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2020/02/29/3.language-model/">
                3.language-model
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2020-02-29</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            <h1 id="第三课-语言模型"><a href="#第三课-语言模型" class="headerlink" title="第三课 语言模型"></a>第三课 语言模型</h1><p>褚则伟 <a href="mailto:zeweichu@gmail.com">zeweichu@gmail.com</a></p>
<p>学习目标</p>
<ul>
<li>学习语言模型，以及如何训练一个语言模型</li>
<li>学习torchtext的基本使用方法<ul>
<li>构建 vocabulary</li>
<li>word to inde 和 index to word</li>
</ul>
</li>
<li>学习torch.nn的一些基本模型<ul>
<li>Linear</li>
<li>RNN</li>
<li>LSTM</li>
<li>GRU</li>
</ul>
</li>
<li>RNN的训练技巧<ul>
<li>Gradient Clipping</li>
</ul>
</li>
<li>如何保存和读取模型</li>
</ul>
<p>我们会使用 <a href="https://github.com/pytorch/text" target="_blank" rel="noopener">torchtext</a> 来创建vocabulary, 然后把数据读成batch的格式。请大家自行阅读README来学习torchtext。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torchtext</span><br><span class="line"><span class="keyword">from</span> torchtext.vocab <span class="keyword">import</span> Vectors</span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line">USE_CUDA = torch.cuda.is_available()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了保证实验结果可以复现，我们经常会把各种random seed固定在某一个值</span></span><br><span class="line">random.seed(<span class="number">53113</span>)</span><br><span class="line">np.random.seed(<span class="number">53113</span>)</span><br><span class="line">torch.manual_seed(<span class="number">53113</span>)</span><br><span class="line"><span class="keyword">if</span> USE_CUDA:</span><br><span class="line">    torch.cuda.manual_seed(<span class="number">53113</span>)</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">32</span></span><br><span class="line">EMBEDDING_SIZE = <span class="number">650</span></span><br><span class="line">MAX_VOCAB_SIZE = <span class="number">50000</span></span><br></pre></td></tr></table></figure>

<ul>
<li>我们会继续使用上次的text8作为我们的训练，验证和测试数据</li>
<li>TorchText的一个重要概念是<code>Field</code>，它决定了你的数据会如何被处理。我们使用<code>TEXT</code>这个field来处理文本数据。我们的<code>TEXT</code> field有<code>lower=True</code>这个参数，所以所有的单词都会被lowercase。</li>
<li>torchtext提供了LanguageModelingDataset这个class来帮助我们处理语言模型数据集。</li>
<li><code>build_vocab</code>可以根据我们提供的训练数据集来创建最高频单词的单词表，<code>max_size</code>帮助我们限定单词总量。</li>
<li>BPTTIterator可以连续地得到连贯的句子，BPTT的全程是back propagation through time。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">TEXT = torchtext.data.Field(lower=<span class="literal">True</span>)</span><br><span class="line">train, val, test = torchtext.datasets.LanguageModelingDataset.splits(path=<span class="string">"."</span>, </span><br><span class="line">    train=<span class="string">"text8.train.txt"</span>, validation=<span class="string">"text8.dev.txt"</span>, test=<span class="string">"text8.test.txt"</span>, text_field=TEXT)</span><br><span class="line">TEXT.build_vocab(train, max_size=MAX_VOCAB_SIZE)</span><br><span class="line">print(<span class="string">"vocabulary size: &#123;&#125;"</span>.format(len(TEXT.vocab)))</span><br><span class="line"></span><br><span class="line">VOCAB_SIZE = len(TEXT.vocab)</span><br><span class="line">train_iter, val_iter, test_iter = torchtext.data.BPTTIterator.splits(</span><br><span class="line">    (train, val, test), batch_size=BATCH_SIZE, device=<span class="number">-1</span>, bptt_len=<span class="number">32</span>, repeat=<span class="literal">False</span>, shuffle=<span class="literal">True</span>)</span><br></pre></td></tr></table></figure>

<pre><code>The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.
The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.
The `device` argument should be set by using `torch.device` or passing a string as an argument. This behavior will be deprecated soon and currently defaults to cpu.


vocabulary size: 50002</code></pre><ul>
<li>为什么我们的单词表有50002个单词而不是50000呢？因为TorchText给我们增加了两个特殊的token，<code>&lt;unk&gt;</code>表示未知的单词，<code>&lt;pad&gt;</code>表示padding。</li>
<li>模型的输入是一串文字，模型的输出也是一串文字，他们之间相差一个位置，因为语言模型的目标是根据之前的单词预测下一个单词。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">it = iter(train_iter)</span><br><span class="line">batch = next(it)</span><br><span class="line">print(<span class="string">" "</span>.join([TEXT.vocab.itos[i] <span class="keyword">for</span> i <span class="keyword">in</span> batch.text[:,<span class="number">1</span>].data]))</span><br><span class="line">print(<span class="string">" "</span>.join([TEXT.vocab.itos[i] <span class="keyword">for</span> i <span class="keyword">in</span> batch.target[:,<span class="number">1</span>].data]))</span><br></pre></td></tr></table></figure>

<pre><code>had dropped to just three zero zero zero k it was then cool enough to allow the nuclei to capture electrons this process is called recombination during which the first neutral atoms
dropped to just three zero zero zero k it was then cool enough to allow the nuclei to capture electrons this process is called recombination during which the first neutral atoms took</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">5</span>):</span><br><span class="line">    batch = next(it)</span><br><span class="line">    print(<span class="string">" "</span>.join([TEXT.vocab.itos[i] <span class="keyword">for</span> i <span class="keyword">in</span> batch.text[:,<span class="number">2</span>].data]))</span><br><span class="line">    print(<span class="string">" "</span>.join([TEXT.vocab.itos[i] <span class="keyword">for</span> i <span class="keyword">in</span> batch.target[:,<span class="number">2</span>].data]))</span><br></pre></td></tr></table></figure>

<pre><code>under the aegis of the emperor thus reducing the local identity and autonomy of the different regions of japan as japanese citizens the ainu are now governed by japanese laws though one
the aegis of the emperor thus reducing the local identity and autonomy of the different regions of japan as japanese citizens the ainu are now governed by japanese laws though one ainu
ainu man was acquitted of murder because he asserted that he was not a japanese citizen and the judge agreed and judged by japanese tribunals but in the past their affairs were
man was acquitted of murder because he asserted that he was not a japanese citizen and the judge agreed and judged by japanese tribunals but in the past their affairs were administered
administered by hereditary chiefs three in each village and for administrative purposes the country was divided into three districts &lt;unk&gt; &lt;unk&gt; and &lt;unk&gt; which were under the ultimate control of &lt;unk&gt; though
by hereditary chiefs three in each village and for administrative purposes the country was divided into three districts &lt;unk&gt; &lt;unk&gt; and &lt;unk&gt; which were under the ultimate control of &lt;unk&gt; though the
the relations between their respective inhabitants were not close and intermarriages were avoided the functions of judge were not entrusted to these chiefs an indefinite number of a community s members sat
relations between their respective inhabitants were not close and intermarriages were avoided the functions of judge were not entrusted to these chiefs an indefinite number of a community s members sat in
in judgement upon its criminals capital punishment did not exist nor was imprisonment resorted to beating being considered a sufficient and final penalty except in the case of murder when the nose
judgement upon its criminals capital punishment did not exist nor was imprisonment resorted to beating being considered a sufficient and final penalty except in the case of murder when the nose and</code></pre><h3 id="定义模型"><a href="#定义模型" class="headerlink" title="定义模型"></a>定义模型</h3><ul>
<li>继承nn.Module</li>
<li>初始化函数</li>
<li>forward函数</li>
<li>其余可以根据模型需要定义相关的函数</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"></span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNNModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">""" 一个简单的循环神经网络"""</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, rnn_type, ntoken, ninp, nhid, nlayers, dropout=<span class="number">0.5</span>)</span>:</span></span><br><span class="line">        <span class="string">''' 该模型包含以下几层:</span></span><br><span class="line"><span class="string">            - 词嵌入层</span></span><br><span class="line"><span class="string">            - 一个循环神经网络层(RNN, LSTM, GRU)</span></span><br><span class="line"><span class="string">            - 一个线性层，从hidden state到输出单词表</span></span><br><span class="line"><span class="string">            - 一个dropout层，用来做regularization</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        super(RNNModel, self).__init__()</span><br><span class="line">        self.drop = nn.Dropout(dropout)</span><br><span class="line">        self.encoder = nn.Embedding(ntoken, ninp)</span><br><span class="line">        <span class="keyword">if</span> rnn_type <span class="keyword">in</span> [<span class="string">'LSTM'</span>, <span class="string">'GRU'</span>]:</span><br><span class="line">            self.rnn = getattr(nn, rnn_type)(ninp, nhid, nlayers, dropout=dropout)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">try</span>:</span><br><span class="line">                nonlinearity = &#123;<span class="string">'RNN_TANH'</span>: <span class="string">'tanh'</span>, <span class="string">'RNN_RELU'</span>: <span class="string">'relu'</span>&#125;[rnn_type]</span><br><span class="line">            <span class="keyword">except</span> KeyError:</span><br><span class="line">                <span class="keyword">raise</span> ValueError( <span class="string">"""An invalid option for `--model` was supplied,</span></span><br><span class="line"><span class="string">                                 options are ['LSTM', 'GRU', 'RNN_TANH' or 'RNN_RELU']"""</span>)</span><br><span class="line">            self.rnn = nn.RNN(ninp, nhid, nlayers, nonlinearity=nonlinearity, dropout=dropout)</span><br><span class="line">        self.decoder = nn.Linear(nhid, ntoken)</span><br><span class="line"></span><br><span class="line">        self.init_weights()</span><br><span class="line"></span><br><span class="line">        self.rnn_type = rnn_type</span><br><span class="line">        self.nhid = nhid</span><br><span class="line">        self.nlayers = nlayers</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_weights</span><span class="params">(self)</span>:</span></span><br><span class="line">        initrange = <span class="number">0.1</span></span><br><span class="line">        self.encoder.weight.data.uniform_(-initrange, initrange)</span><br><span class="line">        self.decoder.bias.data.zero_()</span><br><span class="line">        self.decoder.weight.data.uniform_(-initrange, initrange)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, hidden)</span>:</span></span><br><span class="line">        <span class="string">''' Forward pass:</span></span><br><span class="line"><span class="string">            - word embedding</span></span><br><span class="line"><span class="string">            - 输入循环神经网络</span></span><br><span class="line"><span class="string">            - 一个线性层从hidden state转化为输出单词表</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        emb = self.drop(self.encoder(input))</span><br><span class="line">        output, hidden = self.rnn(emb, hidden)</span><br><span class="line">        output = self.drop(output)</span><br><span class="line">        decoded = self.decoder(output.view(output.size(<span class="number">0</span>)*output.size(<span class="number">1</span>), output.size(<span class="number">2</span>)))</span><br><span class="line">        <span class="keyword">return</span> decoded.view(output.size(<span class="number">0</span>), output.size(<span class="number">1</span>), decoded.size(<span class="number">1</span>)), hidden</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">init_hidden</span><span class="params">(self, bsz, requires_grad=True)</span>:</span></span><br><span class="line">        weight = next(self.parameters())</span><br><span class="line">        <span class="keyword">if</span> self.rnn_type == <span class="string">'LSTM'</span>:</span><br><span class="line">            <span class="keyword">return</span> (weight.new_zeros((self.nlayers, bsz, self.nhid), requires_grad=requires_grad),</span><br><span class="line">                    weight.new_zeros((self.nlayers, bsz, self.nhid), requires_grad=requires_grad))</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">return</span> weight.new_zeros((self.nlayers, bsz, self.nhid), requires_grad=requires_grad)</span><br></pre></td></tr></table></figure>

<p>初始化一个模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = RNNModel(<span class="string">"LSTM"</span>, VOCAB_SIZE, EMBEDDING_SIZE, EMBEDDING_SIZE, <span class="number">2</span>, dropout=<span class="number">0.5</span>)</span><br><span class="line"><span class="keyword">if</span> USE_CUDA:</span><br><span class="line">    model = model.cuda()</span><br></pre></td></tr></table></figure>

<ul>
<li>我们首先定义评估模型的代码。</li>
<li>模型的评估和模型的训练逻辑基本相同，唯一的区别是我们只需要forward pass，不需要backward pass</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(model, data)</span>:</span></span><br><span class="line">    model.eval()</span><br><span class="line">    total_loss = <span class="number">0.</span></span><br><span class="line">    it = iter(data)</span><br><span class="line">    total_count = <span class="number">0.</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        hidden = model.init_hidden(BATCH_SIZE, requires_grad=<span class="literal">False</span>)</span><br><span class="line">        <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(it):</span><br><span class="line">            data, target = batch.text, batch.target</span><br><span class="line">            <span class="keyword">if</span> USE_CUDA:</span><br><span class="line">                data, target = data.cuda(), target.cuda()</span><br><span class="line">            hidden = repackage_hidden(hidden)</span><br><span class="line">            <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">                output, hidden = model(data, hidden)</span><br><span class="line">            loss = loss_fn(output.view(<span class="number">-1</span>, VOCAB_SIZE), target.view(<span class="number">-1</span>))</span><br><span class="line">            total_count += np.multiply(*data.size())</span><br><span class="line">            total_loss += loss.item()*np.multiply(*data.size())</span><br><span class="line">            </span><br><span class="line">    loss = total_loss / total_count</span><br><span class="line">    model.train()</span><br><span class="line">    <span class="keyword">return</span> loss</span><br></pre></td></tr></table></figure>

<p>我们需要定义下面的一个function，帮助我们把一个hidden state和计算图之前的历史分离。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Remove this part</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">repackage_hidden</span><span class="params">(h)</span>:</span></span><br><span class="line">    <span class="string">"""Wraps hidden states in new Tensors, to detach them from their history."""</span></span><br><span class="line">    <span class="keyword">if</span> isinstance(h, torch.Tensor):</span><br><span class="line">        <span class="keyword">return</span> h.detach()</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> tuple(repackage_hidden(v) <span class="keyword">for</span> v <span class="keyword">in</span> h)</span><br></pre></td></tr></table></figure>

<p>定义loss function和optimizer</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">loss_fn = nn.CrossEntropyLoss()</span><br><span class="line">learning_rate = <span class="number">0.001</span></span><br><span class="line">optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</span><br><span class="line">scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, <span class="number">0.5</span>)</span><br></pre></td></tr></table></figure>

<p>训练模型：</p>
<ul>
<li>模型一般需要训练若干个epoch</li>
<li>每个epoch我们都把所有的数据分成若干个batch</li>
<li>把每个batch的输入和输出都包装成cuda tensor</li>
<li>forward pass，通过输入的句子预测每个单词的下一个单词</li>
<li>用模型的预测和正确的下一个单词计算cross entropy loss</li>
<li>清空模型当前gradient</li>
<li>backward pass</li>
<li>gradient clipping，防止梯度爆炸</li>
<li>更新模型参数</li>
<li>每隔一定的iteration输出模型在当前iteration的loss，以及在验证集上做模型的评估</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> copy</span><br><span class="line">GRAD_CLIP = <span class="number">1.</span></span><br><span class="line">NUM_EPOCHS = <span class="number">2</span></span><br><span class="line"></span><br><span class="line">val_losses = []</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(NUM_EPOCHS):</span><br><span class="line">    model.train()</span><br><span class="line">    it = iter(train_iter)</span><br><span class="line">    hidden = model.init_hidden(BATCH_SIZE)</span><br><span class="line">    <span class="keyword">for</span> i, batch <span class="keyword">in</span> enumerate(it):</span><br><span class="line">        data, target = batch.text, batch.target</span><br><span class="line">        <span class="keyword">if</span> USE_CUDA:</span><br><span class="line">            data, target = data.cuda(), target.cuda()</span><br><span class="line">        hidden = repackage_hidden(hidden)</span><br><span class="line">        model.zero_grad()</span><br><span class="line">        output, hidden = model(data, hidden)</span><br><span class="line">        loss = loss_fn(output.view(<span class="number">-1</span>, VOCAB_SIZE), target.view(<span class="number">-1</span>))</span><br><span class="line">        loss.backward()</span><br><span class="line">        torch.nn.utils.clip_grad_norm_(model.parameters(), GRAD_CLIP)</span><br><span class="line">        optimizer.step()</span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">1000</span> == <span class="number">0</span>:</span><br><span class="line">            print(<span class="string">"epoch"</span>, epoch, <span class="string">"iter"</span>, i, <span class="string">"loss"</span>, loss.item())</span><br><span class="line">    </span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">10000</span> == <span class="number">0</span>:</span><br><span class="line">            val_loss = evaluate(model, val_iter)</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> len(val_losses) == <span class="number">0</span> <span class="keyword">or</span> val_loss &lt; min(val_losses):</span><br><span class="line">                print(<span class="string">"best model, val loss: "</span>, val_loss)</span><br><span class="line">                torch.save(model.state_dict(), <span class="string">"lm-best.th"</span>)</span><br><span class="line">            <span class="keyword">else</span>:</span><br><span class="line">                scheduler.step()</span><br><span class="line">                optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)</span><br><span class="line">            val_losses.append(val_loss)</span><br></pre></td></tr></table></figure>

<pre><code>epoch 0 iter 0 loss 10.821578979492188
best model, val loss:  10.782116411285918
epoch 0 iter 1000 loss 6.5122528076171875
epoch 0 iter 2000 loss 6.3599748611450195
epoch 0 iter 3000 loss 6.13856315612793
epoch 0 iter 4000 loss 5.473214626312256
epoch 0 iter 5000 loss 5.901871204376221
epoch 0 iter 6000 loss 5.85321569442749
epoch 0 iter 7000 loss 5.636535167694092
epoch 0 iter 8000 loss 5.7489800453186035
epoch 0 iter 9000 loss 5.464158058166504
epoch 0 iter 10000 loss 5.554863452911377
best model, val loss:  5.264891533569864
epoch 0 iter 11000 loss 5.703625202178955
epoch 0 iter 12000 loss 5.6448974609375
epoch 0 iter 13000 loss 5.372857570648193
epoch 0 iter 14000 loss 5.2639479637146
epoch 1 iter 0 loss 5.696778297424316
best model, val loss:  5.124550380139679
epoch 1 iter 1000 loss 5.534722805023193
epoch 1 iter 2000 loss 5.599489212036133
epoch 1 iter 3000 loss 5.459986686706543
epoch 1 iter 4000 loss 4.927192211151123
epoch 1 iter 5000 loss 5.435710906982422
epoch 1 iter 6000 loss 5.4059576988220215
epoch 1 iter 7000 loss 5.308575630187988
epoch 1 iter 8000 loss 5.405811786651611
epoch 1 iter 9000 loss 5.1389055252075195
epoch 1 iter 10000 loss 5.226413726806641
best model, val loss:  4.946829228873176
epoch 1 iter 11000 loss 5.379891395568848
epoch 1 iter 12000 loss 5.360724925994873
epoch 1 iter 13000 loss 5.176026344299316
epoch 1 iter 14000 loss 5.110936641693115</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">best_model = RNNModel(<span class="string">"LSTM"</span>, VOCAB_SIZE, EMBEDDING_SIZE, EMBEDDING_SIZE, <span class="number">2</span>, dropout=<span class="number">0.5</span>)</span><br><span class="line"><span class="keyword">if</span> USE_CUDA:</span><br><span class="line">    best_model = best_model.cuda()</span><br><span class="line">best_model.load_state_dict(torch.load(<span class="string">"lm-best.th"</span>))</span><br></pre></td></tr></table></figure>

<h3 id="使用最好的模型在valid数据上计算perplexity"><a href="#使用最好的模型在valid数据上计算perplexity" class="headerlink" title="使用最好的模型在valid数据上计算perplexity"></a>使用最好的模型在valid数据上计算perplexity</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">val_loss = evaluate(best_model, val_iter)</span><br><span class="line">print(<span class="string">"perplexity: "</span>, np.exp(val_loss))</span><br></pre></td></tr></table></figure>

<pre><code>perplexity:  140.72803934425724</code></pre><h3 id="使用最好的模型在测试数据上计算perplexity"><a href="#使用最好的模型在测试数据上计算perplexity" class="headerlink" title="使用最好的模型在测试数据上计算perplexity"></a>使用最好的模型在测试数据上计算perplexity</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">test_loss = evaluate(best_model, test_iter)</span><br><span class="line">print(<span class="string">"perplexity: "</span>, np.exp(test_loss))</span><br></pre></td></tr></table></figure>

<pre><code>perplexity:  178.54742013696125</code></pre><p>使用训练好的模型生成一些句子。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">hidden = best_model.init_hidden(<span class="number">1</span>)</span><br><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">input = torch.randint(VOCAB_SIZE, (<span class="number">1</span>, <span class="number">1</span>), dtype=torch.long).to(device)</span><br><span class="line">words = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>):</span><br><span class="line">    output, hidden = best_model(input, hidden)</span><br><span class="line">    word_weights = output.squeeze().exp().cpu()</span><br><span class="line">    word_idx = torch.multinomial(word_weights, <span class="number">1</span>)[<span class="number">0</span>]</span><br><span class="line">    input.fill_(word_idx)</span><br><span class="line">    word = TEXT.vocab.itos[word_idx]</span><br><span class="line">    words.append(word)</span><br><span class="line">print(<span class="string">" "</span>.join(words))</span><br></pre></td></tr></table></figure>

<pre><code>s influence clinton decision de gaulle is himself sappho s iv one family banquet was made published by paul &lt;unk&gt; and by a persuaded to prevent arcane of animate poverty based at copernicus bachelor in search services and in a cruise corps references eds the robin series july four one nine zero eight summer gutenberg one nine six four births one nine two eight deaths timeline of this method by the fourth amendment the german ioc known for his &lt;unk&gt; from &lt;unk&gt; one eight nine eight one seven eight nine management was established in one nine seven zero they had</code></pre>
        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2020/02/29/2.word-embedding/">
                2.word-embedding
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2020-02-29</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            <h1 id="第二课-词向量"><a href="#第二课-词向量" class="headerlink" title="第二课 词向量"></a>第二课 词向量</h1><p>褚则伟 <a href="mailto:zeweichu@gmail.com">zeweichu@gmail.com</a></p>
<p>第二课学习目标</p>
<ul>
<li>学习词向量的概念</li>
<li>用Skip-thought模型训练词向量</li>
<li>学习使用PyTorch dataset和dataloader</li>
<li>学习定义PyTorch模型</li>
<li>学习torch.nn中常见的Module<ul>
<li>Embedding</li>
</ul>
</li>
<li>学习常见的PyTorch operations<ul>
<li>bmm</li>
<li>logsigmoid</li>
</ul>
</li>
<li>保存和读取PyTorch模型</li>
</ul>
<p>第二课使用的训练数据可以从以下链接下载到。</p>
<p>链接:<a href="https://pan.baidu.com/s/1tFeK3mXuVXEy3EMarfeWvg" target="_blank" rel="noopener">https://pan.baidu.com/s/1tFeK3mXuVXEy3EMarfeWvg</a>  密码:v2z5</p>
<p>在这一份notebook中，我们会（尽可能）尝试复现论文<a href="http://papers.nips.cc/paper/5021-distributed-representations-of-words-and-phrases-and-their-compositionality.pdf" target="_blank" rel="noopener">Distributed Representations of Words and Phrases and their Compositionality</a>中训练词向量的方法. 我们会实现Skip-gram模型，并且使用论文中noice contrastive sampling的目标函数。</p>
<p>这篇论文有很多模型实现的细节，这些细节对于词向量的好坏至关重要。我们虽然无法完全复现论文中的实验结果，主要是由于计算资源等各种细节原因，但是我们还是可以大致展示如何训练词向量。</p>
<p>以下是一些我们没有实现的细节</p>
<ul>
<li>subsampling：参考论文section 2.3</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"><span class="keyword">import</span> torch.utils.data <span class="keyword">as</span> tud</span><br><span class="line"><span class="keyword">from</span> torch.nn.parameter <span class="keyword">import</span> Parameter</span><br><span class="line"></span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> pandas <span class="keyword">as</span> pd</span><br><span class="line"><span class="keyword">import</span> scipy</span><br><span class="line"><span class="keyword">import</span> sklearn</span><br><span class="line"><span class="keyword">from</span> sklearn.metrics.pairwise <span class="keyword">import</span> cosine_similarity</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line">USE_CUDA = torch.cuda.is_available()</span><br><span class="line"></span><br><span class="line"><span class="comment"># 为了保证实验结果可以复现，我们经常会把各种random seed固定在某一个值</span></span><br><span class="line">random.seed(<span class="number">53113</span>)</span><br><span class="line">np.random.seed(<span class="number">53113</span>)</span><br><span class="line">torch.manual_seed(<span class="number">53113</span>)</span><br><span class="line"><span class="keyword">if</span> USE_CUDA:</span><br><span class="line">    torch.cuda.manual_seed(<span class="number">53113</span>)</span><br><span class="line">    </span><br><span class="line"><span class="comment"># 设定一些超参数</span></span><br><span class="line">    </span><br><span class="line">K = <span class="number">100</span> <span class="comment"># number of negative samples</span></span><br><span class="line">C = <span class="number">3</span> <span class="comment"># nearby words threshold</span></span><br><span class="line">NUM_EPOCHS = <span class="number">2</span> <span class="comment"># The number of epochs of training</span></span><br><span class="line">MAX_VOCAB_SIZE = <span class="number">30000</span> <span class="comment"># the vocabulary size</span></span><br><span class="line">BATCH_SIZE = <span class="number">128</span> <span class="comment"># the batch size</span></span><br><span class="line">LEARNING_RATE = <span class="number">0.2</span> <span class="comment"># the initial learning rate</span></span><br><span class="line">EMBEDDING_SIZE = <span class="number">100</span></span><br><span class="line">       </span><br><span class="line">    </span><br><span class="line">LOG_FILE = <span class="string">"word-embedding.log"</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># tokenize函数，把一篇文本转化成一个个单词</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">word_tokenize</span><span class="params">(text)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> text.split()</span><br></pre></td></tr></table></figure>

<ul>
<li>从文本文件中读取所有的文字，通过这些文本创建一个vocabulary</li>
<li>由于单词数量可能太大，我们只选取最常见的MAX_VOCAB_SIZE个单词</li>
<li>我们添加一个UNK单词表示所有不常见的单词</li>
<li>我们需要记录单词到index的mapping，以及index到单词的mapping，单词的count，单词的(normalized) frequency，以及单词总数。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">with</span> open(<span class="string">"./lesson2/text8.train.txt"</span>, <span class="string">"r"</span>) <span class="keyword">as</span> fin:</span><br><span class="line">    text = fin.read()</span><br><span class="line">    </span><br><span class="line">text = [w <span class="keyword">for</span> w <span class="keyword">in</span> word_tokenize(text.lower())]</span><br><span class="line">vocab = dict(Counter(text).most_common(MAX_VOCAB_SIZE<span class="number">-1</span>)) <span class="comment"># map to wordcount (MAX_VOCAB_SIZE-1 : except &lt;unk&gt;)</span></span><br><span class="line">vocab[<span class="string">"&lt;unk&gt;"</span>] = len(text) - np.sum(list(vocab.values()))</span><br><span class="line"><span class="comment"># two-way mapping</span></span><br><span class="line">idx_to_word = [word <span class="keyword">for</span> word <span class="keyword">in</span> vocab.keys()] </span><br><span class="line">word_to_idx = &#123;word:i <span class="keyword">for</span> i, word <span class="keyword">in</span> enumerate(idx_to_word)&#125;</span><br><span class="line"></span><br><span class="line">word_counts = np.array([count <span class="keyword">for</span> count <span class="keyword">in</span> vocab.values()], dtype=np.float32)</span><br><span class="line">word_freqs = word_counts / np.sum(word_counts)</span><br><span class="line">word_freqs = word_freqs ** (<span class="number">3.</span>/<span class="number">4.</span>)</span><br><span class="line">word_freqs = word_freqs / np.sum(word_freqs) <span class="comment"># 用来做 negative sampling</span></span><br><span class="line">VOCAB_SIZE = len(idx_to_word)</span><br><span class="line">VOCAB_SIZE</span><br></pre></td></tr></table></figure>




<pre><code>30000</code></pre><h3 id="实现Dataloader"><a href="#实现Dataloader" class="headerlink" title="实现Dataloader"></a>实现Dataloader</h3><p>一个dataloader需要以下内容：</p>
<ul>
<li>把所有text编码成数字，然后用subsampling预处理这些文字。</li>
<li>保存vocabulary，单词count，normalized word frequency</li>
<li>每个iteration sample一个中心词</li>
<li>根据当前的中心词返回context单词</li>
<li>根据中心词sample一些negative单词</li>
<li>返回单词的counts</li>
</ul>
<p>这里有一个好的tutorial介绍如何使用<a href="https://pytorch.org/tutorials/beginner/data_loading_tutorial.html" target="_blank" rel="noopener">PyTorch dataloader</a>.<br>为了使用dataloader，我们需要定义以下两个function:</p>
<ul>
<li><code>__len__</code> function需要返回整个数据集中有多少个item</li>
<li><code>__get__</code> 根据给定的index返回一个item</li>
</ul>
<p>有了dataloader之后，我们可以轻松随机打乱整个数据集，拿到一个batch的数据等等。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordEmbeddingDataset</span><span class="params">(tud.Dataset)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, text, word_to_idx, idx_to_word, word_freqs, word_counts)</span>:</span></span><br><span class="line">        <span class="string">''' text: a list of words, all text from the training dataset</span></span><br><span class="line"><span class="string">            word_to_idx: the dictionary from word to idx</span></span><br><span class="line"><span class="string">            idx_to_word: idx to word mapping</span></span><br><span class="line"><span class="string">            word_freq: the frequency of each word</span></span><br><span class="line"><span class="string">            word_counts: the word counts</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        super(WordEmbeddingDataset, self).__init__()</span><br><span class="line">        self.text_encoded = [word_to_idx.get(t, VOCAB_SIZE<span class="number">-1</span>) <span class="keyword">for</span> t <span class="keyword">in</span> text]</span><br><span class="line">        self.text_encoded = torch.Tensor(self.text_encoded).long()</span><br><span class="line">        self.word_to_idx = word_to_idx</span><br><span class="line">        self.idx_to_word = idx_to_word</span><br><span class="line">        self.word_freqs = torch.Tensor(word_freqs)</span><br><span class="line">        self.word_counts = torch.Tensor(word_counts)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="string">''' 返回整个数据集（所有单词）的长度</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        <span class="keyword">return</span> len(self.text_encoded)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__getitem__</span><span class="params">(self, idx)</span>:</span></span><br><span class="line">        <span class="string">''' 这个function返回以下数据用于训练</span></span><br><span class="line"><span class="string">            - 中心词</span></span><br><span class="line"><span class="string">            - 这个单词附近的(positive)单词</span></span><br><span class="line"><span class="string">            - 随机采样的K个单词作为negative sample</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        center_word = self.text_encoded[idx]</span><br><span class="line">        pos_indices = list(range(idx-C, idx)) + list(range(idx+<span class="number">1</span>, idx+C+<span class="number">1</span>))</span><br><span class="line">        pos_indices = [i%len(self.text_encoded) <span class="keyword">for</span> i <span class="keyword">in</span> pos_indices]</span><br><span class="line">        pos_words = self.text_encoded[pos_indices] </span><br><span class="line">        neg_words = torch.multinomial(self.word_freqs, K * pos_words.shape[<span class="number">0</span>], <span class="literal">True</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> center_word, pos_words, neg_words</span><br></pre></td></tr></table></figure>

<p>创建dataset和dataloader</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">dataset = WordEmbeddingDataset(text, word_to_idx, idx_to_word, word_freqs, word_counts)</span><br><span class="line">dataloader = tud.DataLoader(dataset, batch_size=BATCH_SIZE, shuffle=<span class="literal">True</span>, num_workers=<span class="number">0</span>)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">next(iter(dataloader))</span><br></pre></td></tr></table></figure>




<pre><code>[tensor([   48,  1495,  1799,  2312,  2329,  4869,     8,     0,  8746,    71,
           772,     7,  2837,     0, 11805,  1746,  7122,     6,    10,  1956,
          9429,    33,    17,   230, 29999,     5,   291,   640, 13454,   103,
          2505,   540,  1683,    25,   330,    52,  1890,   644,   219,     6,
         29999,    82, 27040,  2008,     1,  1832,  5685,     0,    53,  1214,
            58, 29999,  7677,     9,    14,     5,  7035, 12651,    73,   788,
          1159,     7,     0,    40,     3, 29999,    53,    57,   714,  5871,
          1056,  2756,     5,  5272,    26, 29999,    44,     8,    35,  6009,
            41,     5,   692,    17,   854,  2473,     4,    33,     1, 12482,
            48,     7,    30,   306,  7595,     6,     5,    56,  1461,     2,
           131,   167,   232,    11,  9131, 18990,     4,  4395,    32,    19,
             0,   847,    17,  6116,   608,  2573,  6378, 29999,    52, 17768,
            20,     0,  4113,  8883,  2380,   121,  1298,    87]),
 tensor([[  480,     0,   868,   363,    13,    93],
         [ 5889, 10403,  3751,  5168,   932, 16947],
         [  749,     3,     5,   345,     1,     0],
         [ 3798,  8075,     2,  1008,     2,    50],
         [ 5607, 19200,  3981,  1436,   582,     4],
         [ 2300,     4,     5,   213,    27,  1122],
         [   14,  1761,     3,    22,     8,  2573],
         [ 8123,  2813,    19,  9093,  2077,    96],
         [   13,    44,  2189,  5529,    26,    17],
         [ 2949,     0,  2266,  4809,  2876,   169],
         [ 2084,  4219,  1173,  1262, 29999,   772],
         [ 3284,     9,     7,     7,   599,   486],
         [29999,     5,  2252, 14789,     0,  7853],
         [   83,    70,    38,  4720,     1,  6251],
         [    0,  4719,     2,    39,  2117,     6],
         [   13,   254,   387,    23,     0,   121],
         [  266,    54,    11,  6962,  4473,     2],
         [  569,    10,  7479,  5115, 17946,    27],
         [    8,    32,   557,   657,     5,   297],
         [10982,   216,     6,  1051,  4695,   130],
         [ 6260,  1463,     6,     0, 11117,  1174],
         [ 8131, 16819,     1,     0, 29999,   695],
         [   11,     5,  9742,     4,     0,   490],
         [   14,   227,  6220,     5,     2,  2834],
         [ 8529,     1,     0,   212,     0,   314],
         [   20,    33,  6362,  5836,     4,    94],
         [   32,    10,  1305,   174,     0, 29999],
         [   38,   724,    19,     4,     0,   770],
         [ 9207,  5098,     2,     1,  1360,   105],
         [   14,     0, 29999,  7322,     6, 19459],
         [  974,     4,   864,     2,     0,   113],
         [   55,  5216,  2214,  1314,    80,     2],
         [    2, 26169,  1027,  2814,    24,   977],
         [  667,     1, 17338,  1043,    27,     0],
         [   18,  1574,  3048,   184,    66,   617],
         [   27,   280,   577,  1202,     0, 19816],
         [    5,  5331,     6,    26,   275,    23],
         [ 2638,     0,   219,    67,    37,   988],
         [   44,   890,    18,    32,   776, 27004],
         [    4,     0, 15733,  1790,     5,   953],
         [    6,  6495,     0,  1162,     2,  1316],
         [    3,    15,   346,     4,     0,  3131],
         [   11,    45,   136,    27,   135,    42],
         [ 5191,    24,     5, 14534,   481, 29282],
         [   10,     0,   181,   973,    19,    25],
         [  144,  1288,  3860,    30,  3778,     1],
         [  973, 10080,  1355,    25, 13110,    27],
         [    5,    62,    23,   358,  5077, 10844],
         [ 1778,    35,    48,   223,  1459,    13],
         [    0,   477,   158,  5005,    19,     0],
         [ 4364,  2567,    91,    25,   280,  2567],
         [29999, 24131,   300, 29999, 21067,   173],
         [   12, 29999, 29999, 29999, 29999,    14],
         [  145,     2,    29,  2020,   145,  2680],
         [ 4764,  2094, 12947,  3593,     2,     0],
         [   13,   168, 10474, 29999,    10,     5],
         [ 1011,  4210, 29999, 29999,     3,     3],
         [    9,     9,  2471,     3,    22,   227],
         [  227,     5,  5760,   227,     5,   227],
         [   26,    51,   154,     5,     3,   346],
         [  653,     0,   439,     2,   324,    44],
         [ 2179,    13,    20,     1,  1598,     2],
         [29999,   960,     1,  1098,   260, 11907],
         [  396, 29999,    26,    53,  1667,    19],
         [ 3245,  1027,     4,     8,    21,    22],
         [    2,  4557,     1,   222,   445,  8796],
         [ 3672,    19,    48,   292,   309,    70],
         [  918,    27,     0,  1384,     6,  4626],
         [   76,    59,     0,  1986,    34,    43],
         [    8,    12,  2702, 25944,   126,  2825],
         [   19,   132,   686,  1403,     1,     0],
         [ 1390,   348,  6753,     0, 14359, 12643],
         [    1,   356,    10,   951,    13,  5075],
         [12655,  8176, 29999,     5, 18003,   125],
         [ 3232,     1,   258,    10,   106,  1348],
         [   12, 29999,  1769,    74, 29999,   164],
         [  185,  1282,   624,  2038,  9707,    26],
         [ 3149,    73,     3,     7,     9,     3],
         [    6,  5528,   187,    48,   197,     0],
         [  597,     1,  5820,    14,     0,   898],
         [   32,   479,   259,  2685,   310,   385],
         [ 2542,    58,    10,   717,     4,  3663],
         [    1,    65,     9,  2206,     6,    38],
         [ 4089,  3286,  1475,     3,     1,     0],
         [    0,   838,   551,   136,   175,     0],
         [  267,    78,     0,    67, 29999,    20],
         [    7,     7, 29944,     0,     3,     8],
         [ 1883,   141,    24,     6,  6214,  1367],
         [  107,  5667,    89,     0,  1408,    35],
         [  156,    75,    32,     0,   774,     6],
         [   19,    35,   385,   643,     2,    19],
         [ 4823, 10775,    15,  1937,     2,  4839],
         [   24, 29999,    23,   362,  3762,   176],
         [ 2409,   268,     5,  3131,   209,    79],
         [ 9573,     1,   436, 18136,  7979,  5383],
         [  272,   242,   579,    11,   272,     3],
         [   11,   120,     1,   461,    25,    36],
         [   17,    74,     3, 28426,    13,   102],
         [    4,     0, 10960,     1, 25912,  1561],
         [  924,  3397,  1889,  1141,   843, 23654],
         [ 1950,     6,     0,  3384,  7379,    19],
         [   13,     0,  5386,    36,  1642,   275],
         [11913,   612,   768,     6,    44,  8895],
         [   51,    31,  3135,  1982,    19,     0],
         [    0,  2635,    27,   308,    48,  1335],
         [  343,  8573,    28,    32,  4394, 25742],
         [  125,   489,   767,    14,    98,     5],
         [  451,  1699, 21841, 29999,    56,   327],
         [    4,  3932,  1230, 11823,     3,     1],
         [  714,  5428,  2211,     0,   411,     1],
         [19968,     2,    18,  8396,  2094,  6346],
         [  222,  1333,  2301,   300,  2381,     2],
         [  305,    93,  1993,  5967,    63, 29999],
         [  916,    50,  1921,    25,  1277,  1539],
         [  312,    14,   413,    10,   147,  6320],
         [    7,     7,    15,   879,     9,     7],
         [    0,   200, 29999,     1,    50,     0],
         [    1, 22168,     4,  1523,  6396,   995],
         [22526,    47,    81,     1,  1632,    10],
         [ 2135,    10,  2059,   516,    89,     5],
         [ 2079,   909,    10,     7,     7,  2588],
         [ 6259,  7404,    10,   297,  2703,   628],
         [    5, 29999,   244,  2653,    13,    29],
         [ 1532,    23,     0,  7033,    76,  1174],
         [   26,   263,    36, 29999,    27,     0],
         [  674,   105,     0,     1,     0, 27222],
         [   51,    76,    31,   335,     2, 29999],
         [12187,     1,     0,   347,  8817, 29999]]),
 tensor([[ 6313, 23579,  1926,  ...,    64,   147,  1924],
         [12277,   523,   231,  ..., 19350, 17264,  8771],
         [  577, 27195, 11445,  ..., 27978, 19518,    59],
         ...,
         [  246,  7365,   280,  ...,  3584,    56,  3854],
         [    8, 24712,   125,  ..., 26156, 18621,   295],
         [25034,  5372,  5070,  ...,   965,  1358, 25900]])]</code></pre><h3 id="定义PyTorch模型"><a href="#定义PyTorch模型" class="headerlink" title="定义PyTorch模型"></a>定义PyTorch模型</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">EmbeddingModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embed_size)</span>:</span></span><br><span class="line">        <span class="string">''' 初始化输出和输出embedding</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        super(EmbeddingModel, self).__init__()</span><br><span class="line">        self.vocab_size = vocab_size</span><br><span class="line">        self.embed_size = embed_size</span><br><span class="line">        </span><br><span class="line">        initrange = <span class="number">0.5</span> / self.embed_size</span><br><span class="line">        self.out_embed = nn.Embedding(self.vocab_size, self.embed_size, sparse=<span class="literal">False</span>)</span><br><span class="line">        self.out_embed.weight.data.uniform_(-initrange, initrange)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        self.in_embed = nn.Embedding(self.vocab_size, self.embed_size, sparse=<span class="literal">False</span>)</span><br><span class="line">        self.in_embed.weight.data.uniform_(-initrange, initrange)</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input_labels, pos_labels, neg_labels)</span>:</span></span><br><span class="line">        <span class="string">'''</span></span><br><span class="line"><span class="string">        input_labels: 中心词, [batch_size]</span></span><br><span class="line"><span class="string">        pos_labels: 中心词周围 context window 出现过的单词 [batch_size * (window_size * 2)]</span></span><br><span class="line"><span class="string">        neg_labelss: 中心词周围没有出现过的单词，从 negative sampling 得到 [batch_size, (window_size * 2 * K)]</span></span><br><span class="line"><span class="string">        </span></span><br><span class="line"><span class="string">        return: loss, [batch_size]</span></span><br><span class="line"><span class="string">        '''</span></span><br><span class="line">        </span><br><span class="line">        batch_size = input_labels.size(<span class="number">0</span>)</span><br><span class="line">        </span><br><span class="line">        input_embedding = self.in_embed(input_labels) <span class="comment"># B * embed_size</span></span><br><span class="line">        pos_embedding = self.out_embed(pos_labels) <span class="comment"># B * (2*C) * embed_size</span></span><br><span class="line">        neg_embedding = self.out_embed(neg_labels) <span class="comment"># B * (2*C * K) * embed_size</span></span><br><span class="line">      </span><br><span class="line">        log_pos = torch.bmm(pos_embedding, input_embedding.unsqueeze(<span class="number">2</span>)).squeeze() <span class="comment"># B * (2*C)</span></span><br><span class="line">        log_neg = torch.bmm(neg_embedding, -input_embedding.unsqueeze(<span class="number">2</span>)).squeeze() <span class="comment"># B * (2*C*K)</span></span><br><span class="line"></span><br><span class="line">        log_pos = F.logsigmoid(log_pos).sum(<span class="number">1</span>)</span><br><span class="line">        log_neg = F.logsigmoid(log_neg).sum(<span class="number">1</span>) <span class="comment"># batch_size</span></span><br><span class="line">       </span><br><span class="line">        loss = log_pos + log_neg</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> -loss</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">input_embeddings</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> self.in_embed.weight.data.cpu().numpy()</span><br></pre></td></tr></table></figure>

<p>定义一个模型以及把模型移动到GPU</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model = EmbeddingModel(VOCAB_SIZE, EMBEDDING_SIZE)</span><br><span class="line"><span class="keyword">if</span> USE_CUDA:</span><br><span class="line">    model = model.cuda()</span><br></pre></td></tr></table></figure>

<p>下面是评估模型的代码，以及训练模型的代码</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(filename, embedding_weights)</span>:</span> </span><br><span class="line">    <span class="keyword">if</span> filename.endswith(<span class="string">".csv"</span>):</span><br><span class="line">        data = pd.read_csv(filename, sep=<span class="string">","</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        data = pd.read_csv(filename, sep=<span class="string">"\t"</span>)</span><br><span class="line">    human_similarity = []</span><br><span class="line">    model_similarity = []</span><br><span class="line">    <span class="keyword">for</span> i <span class="keyword">in</span> data.iloc[:, <span class="number">0</span>:<span class="number">2</span>].index:</span><br><span class="line">        word1, word2 = data.iloc[i, <span class="number">0</span>], data.iloc[i, <span class="number">1</span>]</span><br><span class="line">        <span class="keyword">if</span> word1 <span class="keyword">not</span> <span class="keyword">in</span> word_to_idx <span class="keyword">or</span> word2 <span class="keyword">not</span> <span class="keyword">in</span> word_to_idx:</span><br><span class="line">            <span class="keyword">continue</span></span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            word1_idx, word2_idx = word_to_idx[word1], word_to_idx[word2]</span><br><span class="line">            word1_embed, word2_embed = embedding_weights[[word1_idx]], embedding_weights[[word2_idx]]</span><br><span class="line">            model_similarity.append(float(sklearn.metrics.pairwise.cosine_similarity(word1_embed, word2_embed)))</span><br><span class="line">            human_similarity.append(float(data.iloc[i, <span class="number">2</span>]))</span><br><span class="line"></span><br><span class="line">    <span class="keyword">return</span> scipy.stats.spearmanr(human_similarity, model_similarity)<span class="comment"># , model_similarity</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">find_nearest</span><span class="params">(word)</span>:</span></span><br><span class="line">    index = word_to_idx[word]</span><br><span class="line">    embedding = embedding_weights[index]</span><br><span class="line">    cos_dis = np.array([scipy.spatial.distance.cosine(e, embedding) <span class="keyword">for</span> e <span class="keyword">in</span> embedding_weights])</span><br><span class="line">    <span class="keyword">return</span> [idx_to_word[i] <span class="keyword">for</span> i <span class="keyword">in</span> cos_dis.argsort()[:<span class="number">10</span>]]</span><br></pre></td></tr></table></figure>

<p>训练模型：</p>
<ul>
<li>模型一般需要训练若干个epoch</li>
<li>每个epoch我们都把所有的数据分成若干个batch</li>
<li>把每个batch的输入和输出都包装成cuda tensor</li>
<li>forward pass，通过输入的句子预测每个单词的下一个单词</li>
<li>用模型的预测和正确的下一个单词计算cross entropy loss</li>
<li>清空模型当前gradient</li>
<li>backward pass</li>
<li>更新模型参数</li>
<li>每隔一定的iteration输出模型在当前iteration的loss，以及在验证数据集上做模型的评估</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line">optimizer = torch.optim.SGD(model.parameters(), lr=LEARNING_RATE)</span><br><span class="line"><span class="keyword">for</span> e <span class="keyword">in</span> range(NUM_EPOCHS):</span><br><span class="line">    <span class="keyword">for</span> i, (input_labels, pos_labels, neg_labels) <span class="keyword">in</span> enumerate(dataloader):</span><br><span class="line">        </span><br><span class="line">        </span><br><span class="line">        <span class="comment"># TODO</span></span><br><span class="line">        input_labels = input_labels.long()</span><br><span class="line">        pos_labels = pos_labels.long()</span><br><span class="line">        neg_labels = neg_labels.long()</span><br><span class="line">        <span class="keyword">if</span> USE_CUDA:</span><br><span class="line">            input_labels = input_labels.cuda()</span><br><span class="line">            pos_labels = pos_labels.cuda()</span><br><span class="line">            neg_labels = neg_labels.cuda()</span><br><span class="line">            </span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        loss = model(input_labels, pos_labels, neg_labels).mean()</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line"></span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">            <span class="keyword">with</span> open(LOG_FILE, <span class="string">"a"</span>) <span class="keyword">as</span> fout:</span><br><span class="line">                fout.write(<span class="string">"epoch: &#123;&#125;, iter: &#123;&#125;, loss: &#123;&#125;\n"</span>.format(e, i, loss.item()))</span><br><span class="line">                print(<span class="string">"epoch: &#123;&#125;, iter: &#123;&#125;, loss: &#123;&#125;"</span>.format(e, i, loss.item()))</span><br><span class="line">            </span><br><span class="line">        </span><br><span class="line">        <span class="keyword">if</span> i % <span class="number">2000</span> == <span class="number">0</span>:</span><br><span class="line">            embedding_weights = model.input_embeddings()</span><br><span class="line">            sim_simlex = evaluate(<span class="string">"simlex-999.txt"</span>, embedding_weights)</span><br><span class="line">            sim_men = evaluate(<span class="string">"men.txt"</span>, embedding_weights)</span><br><span class="line">            sim_353 = evaluate(<span class="string">"wordsim353.csv"</span>, embedding_weights)</span><br><span class="line">            <span class="keyword">with</span> open(LOG_FILE, <span class="string">"a"</span>) <span class="keyword">as</span> fout:</span><br><span class="line">                print(<span class="string">"epoch: &#123;&#125;, iteration: &#123;&#125;, simlex-999: &#123;&#125;, men: &#123;&#125;, sim353: &#123;&#125;, nearest to monster: &#123;&#125;\n"</span>.format(</span><br><span class="line">                    e, i, sim_simlex, sim_men, sim_353, find_nearest(<span class="string">"monster"</span>)))</span><br><span class="line">                fout.write(<span class="string">"epoch: &#123;&#125;, iteration: &#123;&#125;, simlex-999: &#123;&#125;, men: &#123;&#125;, sim353: &#123;&#125;, nearest to monster: &#123;&#125;\n"</span>.format(</span><br><span class="line">                    e, i, sim_simlex, sim_men, sim_353, find_nearest(<span class="string">"monster"</span>)))</span><br><span class="line">                </span><br><span class="line">    embedding_weights = model.input_embeddings()</span><br><span class="line">    np.save(<span class="string">"embedding-&#123;&#125;"</span>.format(EMBEDDING_SIZE), embedding_weights)</span><br><span class="line">    torch.save(model.state_dict(), <span class="string">"embedding-&#123;&#125;.th"</span>.format(EMBEDDING_SIZE))</span><br></pre></td></tr></table></figure>

<pre><code>epoch: 0, iter: 0, loss: 420.04736328125
epoch: 0, iteration: 0, simlex-999: SpearmanrResult(correlation=0.002806243285464091, pvalue=0.9309107582703205), men: SpearmanrResult(correlation=-0.03578915454199749, pvalue=0.06854012381329619), sim353: SpearmanrResult(correlation=0.02468906830123471, pvalue=0.6609497549092586), nearest to monster: [&apos;monster&apos;, &apos;communism&apos;, &apos;bosses&apos;, &apos;microprocessors&apos;, &apos;infectious&apos;, &apos;debussy&apos;, &apos;unesco&apos;, &apos;tantamount&apos;, &apos;offices&apos;, &apos;tischendorf&apos;]

epoch: 0, iter: 100, loss: 278.9967041015625
epoch: 0, iter: 200, loss: 248.71990966796875
epoch: 0, iter: 300, loss: 202.95816040039062
epoch: 0, iter: 400, loss: 157.04776000976562
epoch: 0, iter: 500, loss: 137.83531188964844
epoch: 0, iter: 600, loss: 121.03585815429688
epoch: 0, iter: 700, loss: 105.300537109375
epoch: 0, iter: 800, loss: 114.10055541992188
epoch: 0, iter: 900, loss: 104.72723388671875
epoch: 0, iter: 1000, loss: 99.03569030761719
epoch: 0, iter: 1100, loss: 95.2179946899414
epoch: 0, iter: 1200, loss: 84.12557983398438
epoch: 0, iter: 1300, loss: 88.07209777832031
epoch: 0, iter: 1400, loss: 70.44454193115234
epoch: 0, iter: 1500, loss: 79.83641052246094
epoch: 0, iter: 1600, loss: 81.7451171875
epoch: 0, iter: 1700, loss: 75.91305541992188
epoch: 0, iter: 1800, loss: 65.86140441894531
epoch: 0, iter: 1900, loss: 69.81714630126953
epoch: 0, iter: 2000, loss: 71.05166625976562
epoch: 0, iteration: 2000, simlex-999: SpearmanrResult(correlation=-0.011490367338787073, pvalue=0.7225847577400916), men: SpearmanrResult(correlation=0.05671509287050605, pvalue=0.0038790264864563434), sim353: SpearmanrResult(correlation=-0.07381419228558825, pvalue=0.18921537418718104), nearest to monster: [&apos;monster&apos;, &apos;harm&apos;, &apos;steel&apos;, &apos;dean&apos;, &apos;kansas&apos;, &apos;surgery&apos;, &apos;regardless&apos;, &apos;capitalism&apos;, &apos;offers&apos;, &apos;hockey&apos;]

epoch: 0, iter: 2100, loss: 59.19840621948242
epoch: 0, iter: 2200, loss: 60.21418762207031
epoch: 0, iter: 2300, loss: 63.848148345947266
epoch: 0, iter: 2400, loss: 65.58479309082031
epoch: 0, iter: 2500, loss: 66.90382385253906
epoch: 0, iter: 2600, loss: 54.61847686767578
epoch: 0, iter: 2700, loss: 56.45966339111328
epoch: 0, iter: 2800, loss: 58.255210876464844
epoch: 0, iter: 2900, loss: 59.65287399291992
epoch: 0, iter: 3000, loss: 48.22801971435547
epoch: 0, iter: 3100, loss: 42.94969177246094
epoch: 0, iter: 3200, loss: 49.372528076171875
epoch: 0, iter: 3300, loss: 46.12495422363281
epoch: 0, iter: 3400, loss: 58.97121047973633
epoch: 0, iter: 3500, loss: 48.31055450439453
epoch: 0, iter: 3600, loss: 47.07227325439453
epoch: 0, iter: 3700, loss: 46.4068603515625
epoch: 0, iter: 3800, loss: 49.55707931518555
epoch: 0, iter: 3900, loss: 44.38733673095703
epoch: 0, iter: 4000, loss: 48.730342864990234
epoch: 0, iteration: 4000, simlex-999: SpearmanrResult(correlation=0.0190424235850696, pvalue=0.5562848091306694), men: SpearmanrResult(correlation=0.05404895260610133, pvalue=0.00592548586032086), sim353: SpearmanrResult(correlation=-0.039572591538143916, pvalue=0.4819454801463242), nearest to monster: [&apos;monster&apos;, &apos;electrical&apos;, &apos;northeast&apos;, &apos;surgery&apos;, &apos;entity&apos;, &apos;certainly&apos;, &apos;tea&apos;, &apos;establishing&apos;, &apos;archbishop&apos;, &apos;aging&apos;]

epoch: 0, iter: 4100, loss: 57.70344161987305
epoch: 0, iter: 4200, loss: 47.464820861816406
epoch: 0, iter: 4300, loss: 47.08036804199219
epoch: 0, iter: 4400, loss: 46.652706146240234
epoch: 0, iter: 4500, loss: 40.824310302734375
epoch: 0, iter: 4600, loss: 40.62211227416992
epoch: 0, iter: 4700, loss: 50.84752655029297
epoch: 0, iter: 4800, loss: 41.230072021484375
epoch: 0, iter: 4900, loss: 53.74473571777344
epoch: 0, iter: 5000, loss: 42.35053253173828
epoch: 0, iter: 5100, loss: 38.363189697265625
epoch: 0, iter: 5200, loss: 42.772552490234375
epoch: 0, iter: 5300, loss: 44.914913177490234
epoch: 0, iter: 5400, loss: 38.4688720703125
epoch: 0, iter: 5500, loss: 41.0843391418457
epoch: 0, iter: 5600, loss: 35.04629898071289
epoch: 0, iter: 5700, loss: 35.49506759643555
epoch: 0, iter: 5800, loss: 36.009666442871094
epoch: 0, iter: 5900, loss: 40.56498718261719
epoch: 0, iter: 6000, loss: 45.853214263916016
epoch: 0, iteration: 6000, simlex-999: SpearmanrResult(correlation=0.04213372810279324, pvalue=0.19281410892481102), men: SpearmanrResult(correlation=0.06483263975087832, pvalue=0.0009600352172924885), sim353: SpearmanrResult(correlation=-0.015385630136134733, pvalue=0.7846219761829791), nearest to monster: [&apos;monster&apos;, &apos;raw&apos;, &apos;romantic&apos;, &apos;oregon&apos;, &apos;protest&apos;, &apos;brunei&apos;, &apos;cartoon&apos;, &apos;offers&apos;, &apos;certainly&apos;, &apos;ill&apos;]

epoch: 0, iter: 6100, loss: 39.977508544921875
epoch: 0, iter: 6200, loss: 35.47979736328125
epoch: 0, iter: 6300, loss: 38.61311340332031
epoch: 0, iter: 6400, loss: 38.735679626464844
epoch: 0, iter: 6500, loss: 41.1725959777832
epoch: 0, iter: 6600, loss: 37.390037536621094
epoch: 0, iter: 6700, loss: 39.51911926269531
epoch: 0, iter: 6800, loss: 47.12213897705078
epoch: 0, iter: 6900, loss: 41.91630172729492
epoch: 0, iter: 7000, loss: 38.11504364013672
epoch: 0, iter: 7100, loss: 38.12763214111328
epoch: 0, iter: 7200, loss: 36.93813705444336
epoch: 0, iter: 7300, loss: 40.82877731323242
epoch: 0, iter: 7400, loss: 36.211429595947266
epoch: 0, iter: 7500, loss: 36.141693115234375
epoch: 0, iter: 7600, loss: 38.152610778808594
epoch: 0, iter: 7700, loss: 38.90789031982422
epoch: 0, iter: 7800, loss: 36.30712127685547
epoch: 0, iter: 7900, loss: 34.192440032958984
epoch: 0, iter: 8000, loss: 39.182212829589844
epoch: 0, iteration: 8000, simlex-999: SpearmanrResult(correlation=0.05506138271487322, pvalue=0.0886781241789579), men: SpearmanrResult(correlation=0.06796632118931804, pvalue=0.0005362832465382729), sim353: SpearmanrResult(correlation=-0.00727317983344893, pvalue=0.897207043425527), nearest to monster: [&apos;monster&apos;, &apos;raw&apos;, &apos;romantic&apos;, &apos;strategic&apos;, &apos;offers&apos;, &apos;invited&apos;, &apos;signature&apos;, &apos;piano&apos;, &apos;protest&apos;, &apos;bills&apos;]

epoch: 0, iter: 8100, loss: 35.08313751220703
epoch: 0, iter: 8200, loss: 33.23561096191406
epoch: 0, iter: 8300, loss: 36.047096252441406
epoch: 0, iter: 8400, loss: 37.01750946044922
epoch: 0, iter: 8500, loss: 33.679561614990234
epoch: 0, iter: 8600, loss: 36.492515563964844
epoch: 0, iter: 8700, loss: 34.439537048339844
epoch: 0, iter: 8800, loss: 38.89817428588867
epoch: 0, iter: 8900, loss: 34.17725372314453
epoch: 0, iter: 9000, loss: 33.869651794433594
epoch: 0, iter: 9100, loss: 33.63176727294922
epoch: 0, iter: 9200, loss: 35.203460693359375
epoch: 0, iter: 9300, loss: 36.060142517089844
epoch: 0, iter: 9400, loss: 35.6544303894043
epoch: 0, iter: 9500, loss: 35.01182556152344
epoch: 0, iter: 9600, loss: 35.48432540893555
epoch: 0, iter: 9700, loss: 34.940696716308594
epoch: 0, iter: 9800, loss: 33.99235534667969
epoch: 0, iter: 9900, loss: 35.14078903198242
epoch: 0, iter: 10000, loss: 34.10219192504883
epoch: 0, iteration: 10000, simlex-999: SpearmanrResult(correlation=0.0714732189475033, pvalue=0.02703637716635098), men: SpearmanrResult(correlation=0.07013186360584196, pvalue=0.00035356323424747736), sim353: SpearmanrResult(correlation=-0.0013966072615024432, pvalue=0.9802088977698729), nearest to monster: [&apos;monster&apos;, &apos;adoption&apos;, &apos;logo&apos;, &apos;particle&apos;, &apos;isle&apos;, &apos;remainder&apos;, &apos;profit&apos;, &apos;rank&apos;, &apos;execution&apos;, &apos;outer&apos;]

epoch: 0, iter: 10100, loss: 33.885284423828125
epoch: 0, iter: 10200, loss: 39.90406036376953
epoch: 0, iter: 10300, loss: 34.071014404296875
epoch: 0, iter: 10400, loss: 35.23554229736328
epoch: 0, iter: 10500, loss: 35.033878326416016
epoch: 0, iter: 10600, loss: 36.56634521484375
epoch: 0, iter: 10700, loss: 34.755027770996094
epoch: 0, iter: 10800, loss: 37.447967529296875
epoch: 0, iter: 10900, loss: 37.32883834838867
epoch: 0, iter: 11000, loss: 34.621700286865234
epoch: 0, iter: 11100, loss: 34.79033660888672
epoch: 0, iter: 11200, loss: 33.45790100097656
epoch: 0, iter: 11300, loss: 34.915672302246094
epoch: 0, iter: 11400, loss: 33.67906188964844
epoch: 0, iter: 11500, loss: 33.42378616333008
epoch: 0, iter: 11600, loss: 33.216270446777344
epoch: 0, iter: 11700, loss: 35.964393615722656
epoch: 0, iter: 11800, loss: 32.547569274902344
epoch: 0, iter: 11900, loss: 32.87192153930664
epoch: 0, iter: 12000, loss: 37.79120635986328
epoch: 0, iteration: 12000, simlex-999: SpearmanrResult(correlation=0.07427469122590927, pvalue=0.021568044209408773), men: SpearmanrResult(correlation=0.07554039135518772, pvalue=0.00011870202106880258), sim353: SpearmanrResult(correlation=0.003874488949244921, pvalue=0.9451327287240687), nearest to monster: [&apos;monster&apos;, &apos;adoption&apos;, &apos;immediate&apos;, &apos;patent&apos;, &apos;sphere&apos;, &apos;execution&apos;, &apos;shell&apos;, &apos;nucleus&apos;, &apos;ghost&apos;, &apos;label&apos;]

epoch: 0, iter: 12100, loss: 33.59938430786133
epoch: 0, iter: 12200, loss: 32.594879150390625
epoch: 0, iter: 12300, loss: 32.42393493652344
epoch: 0, iter: 12400, loss: 32.8863410949707
epoch: 0, iter: 12500, loss: 39.303016662597656
epoch: 0, iter: 12600, loss: 33.103118896484375
epoch: 0, iter: 12700, loss: 36.31195068359375
epoch: 0, iter: 12800, loss: 33.8329963684082
epoch: 0, iter: 12900, loss: 32.499595642089844
epoch: 0, iter: 13000, loss: 33.224632263183594
epoch: 0, iter: 13100, loss: 33.931884765625
epoch: 0, iter: 13200, loss: 33.35892105102539
epoch: 0, iter: 13300, loss: 33.33966064453125
epoch: 0, iter: 13400, loss: 34.09075164794922
epoch: 0, iter: 13500, loss: 33.52397918701172
epoch: 0, iter: 13600, loss: 34.18444061279297
epoch: 0, iter: 13700, loss: 33.96720886230469
epoch: 0, iter: 13800, loss: 34.23271942138672
epoch: 0, iter: 13900, loss: 33.36094665527344
epoch: 0, iter: 14000, loss: 35.998287200927734
epoch: 0, iteration: 14000, simlex-999: SpearmanrResult(correlation=0.07498900438956249, pvalue=0.0203380930498303), men: SpearmanrResult(correlation=0.07885185599812983, pvalue=5.8687463983198815e-05), sim353: SpearmanrResult(correlation=0.019838726849964704, pvalue=0.7245257659604268), nearest to monster: [&apos;monster&apos;, &apos;tale&apos;, &apos;patent&apos;, &apos;garden&apos;, &apos;outer&apos;, &apos;nucleus&apos;, &apos;logo&apos;, &apos;indians&apos;, &apos;fate&apos;, &apos;ghost&apos;]

epoch: 0, iter: 14100, loss: 32.86090087890625
epoch: 0, iter: 14200, loss: 32.27300262451172
epoch: 0, iter: 14300, loss: 32.97502136230469
epoch: 0, iter: 14400, loss: 33.18888473510742
epoch: 0, iter: 14500, loss: 33.709564208984375
epoch: 0, iter: 14600, loss: 33.725990295410156
epoch: 0, iter: 14700, loss: 34.124961853027344
epoch: 0, iter: 14800, loss: 34.69652557373047
epoch: 0, iter: 14900, loss: 36.399696350097656
epoch: 0, iter: 15000, loss: 32.656532287597656
epoch: 0, iter: 15100, loss: 33.403133392333984
epoch: 0, iter: 15200, loss: 32.11627960205078
epoch: 0, iter: 15300, loss: 32.489803314208984
epoch: 0, iter: 15400, loss: 32.96385192871094
epoch: 0, iter: 15500, loss: 33.85535430908203
epoch: 0, iter: 15600, loss: 33.443634033203125
epoch: 0, iter: 15700, loss: 32.89921569824219
epoch: 0, iter: 15800, loss: 31.661029815673828
epoch: 0, iter: 15900, loss: 32.627262115478516
epoch: 0, iter: 16000, loss: 32.10541534423828
epoch: 0, iteration: 16000, simlex-999: SpearmanrResult(correlation=0.0788889724045409, pvalue=0.014643454855412137), men: SpearmanrResult(correlation=0.08118046638145521, pvalue=3.517646407074078e-05), sim353: SpearmanrResult(correlation=0.03869824262332756, pvalue=0.49168668781560065), nearest to monster: [&apos;monster&apos;, &apos;tale&apos;, &apos;patent&apos;, &apos;garden&apos;, &apos;logo&apos;, &apos;headquarters&apos;, &apos;floor&apos;, &apos;nucleus&apos;, &apos;hotel&apos;, &apos;outer&apos;]

epoch: 0, iter: 16100, loss: 32.40728759765625
epoch: 0, iter: 16200, loss: 32.153541564941406
epoch: 0, iter: 16300, loss: 32.54335021972656
epoch: 0, iter: 16400, loss: 33.81620788574219
epoch: 0, iter: 16500, loss: 33.6131591796875
epoch: 0, iter: 16600, loss: 33.012855529785156
epoch: 0, iter: 16700, loss: 32.9858512878418
epoch: 0, iter: 16800, loss: 32.339019775390625
epoch: 0, iter: 16900, loss: 33.2204475402832
epoch: 0, iter: 17000, loss: 32.71576690673828
epoch: 0, iter: 17100, loss: 33.55080795288086
epoch: 0, iter: 17200, loss: 32.817447662353516
epoch: 0, iter: 17300, loss: 34.78331756591797
epoch: 0, iter: 17400, loss: 32.013267517089844
epoch: 0, iter: 17500, loss: 32.31776428222656
epoch: 0, iter: 17600, loss: 32.81449508666992
epoch: 0, iter: 17700, loss: 32.663665771484375
epoch: 0, iter: 17800, loss: 32.64860534667969
epoch: 0, iter: 17900, loss: 32.25948715209961
epoch: 0, iter: 18000, loss: 33.899532318115234
epoch: 0, iteration: 18000, simlex-999: SpearmanrResult(correlation=0.08197570796707307, pvalue=0.01118359931439746), men: SpearmanrResult(correlation=0.08119437744439352, pvalue=3.5067625057299385e-05), sim353: SpearmanrResult(correlation=0.048031348197188906, pvalue=0.39330365782911914), nearest to monster: [&apos;monster&apos;, &apos;tale&apos;, &apos;patent&apos;, &apos;household&apos;, &apos;dialogue&apos;, &apos;floor&apos;, &apos;sphere&apos;, &apos;mouse&apos;, &apos;fate&apos;, &apos;skin&apos;]

epoch: 0, iter: 18100, loss: 31.952678680419922
epoch: 0, iter: 18200, loss: 32.561737060546875
epoch: 0, iter: 18300, loss: 31.917354583740234
epoch: 0, iter: 18400, loss: 32.31993103027344
epoch: 0, iter: 18500, loss: 32.442169189453125
epoch: 0, iter: 18600, loss: 32.37964630126953
epoch: 0, iter: 18700, loss: 32.223846435546875
epoch: 0, iter: 18800, loss: 32.205589294433594
epoch: 0, iter: 18900, loss: 32.872222900390625
epoch: 0, iter: 19000, loss: 32.515403747558594
epoch: 0, iter: 19100, loss: 33.08296203613281
epoch: 0, iter: 19200, loss: 32.536170959472656
epoch: 0, iter: 19300, loss: 32.39844512939453
epoch: 0, iter: 19400, loss: 33.58967971801758
epoch: 0, iter: 19500, loss: 32.6730842590332
epoch: 0, iter: 19600, loss: 33.223388671875
epoch: 0, iter: 19700, loss: 32.08860397338867
epoch: 0, iter: 19800, loss: 31.78927993774414
epoch: 0, iter: 19900, loss: 31.92531967163086
epoch: 0, iter: 20000, loss: 32.14461898803711
epoch: 0, iteration: 20000, simlex-999: SpearmanrResult(correlation=0.08376406816249372, pvalue=0.00952959087521674), men: SpearmanrResult(correlation=0.08428805462978844, pvalue=1.7391127961421946e-05), sim353: SpearmanrResult(correlation=0.049551103193172526, pvalue=0.3784887673298559), nearest to monster: [&apos;monster&apos;, &apos;patent&apos;, &apos;sword&apos;, &apos;household&apos;, &apos;dialogue&apos;, &apos;comprehensive&apos;, &apos;mouse&apos;, &apos;label&apos;, &apos;plain&apos;, &apos;tale&apos;]

epoch: 0, iter: 20100, loss: 32.788509368896484
epoch: 0, iter: 20200, loss: 31.743305206298828
epoch: 0, iter: 20300, loss: 32.98844909667969
epoch: 0, iter: 20400, loss: 32.939300537109375
epoch: 0, iter: 20500, loss: 32.22157669067383
epoch: 0, iter: 20600, loss: 32.10664367675781
epoch: 0, iter: 20700, loss: 32.317832946777344
epoch: 0, iter: 20800, loss: 32.22321701049805
epoch: 0, iter: 20900, loss: 32.078826904296875
epoch: 0, iter: 21000, loss: 32.00135803222656
epoch: 0, iter: 21100, loss: 32.2218017578125
epoch: 0, iter: 21200, loss: 32.36552047729492
epoch: 0, iter: 21300, loss: 32.28803253173828
epoch: 0, iter: 21400, loss: 32.49916076660156
epoch: 0, iter: 21500, loss: 31.330402374267578
epoch: 0, iter: 21600, loss: 32.153507232666016
epoch: 0, iter: 21700, loss: 32.27666473388672
epoch: 0, iter: 21800, loss: 31.28035545349121
epoch: 0, iter: 21900, loss: 31.78491973876953
epoch: 0, iter: 22000, loss: 32.09901428222656
epoch: 0, iteration: 22000, simlex-999: SpearmanrResult(correlation=0.0841933673566154, pvalue=0.009166514672039517), men: SpearmanrResult(correlation=0.08568243547516359, pvalue=1.2577781665179613e-05), sim353: SpearmanrResult(correlation=0.05233237611768227, pvalue=0.3522765894341572), nearest to monster: [&apos;monster&apos;, &apos;sword&apos;, &apos;hero&apos;, &apos;ghost&apos;, &apos;patent&apos;, &apos;tale&apos;, &apos;comprehensive&apos;, &apos;plain&apos;, &apos;household&apos;, &apos;goddess&apos;]

epoch: 0, iter: 22100, loss: 32.55064392089844
epoch: 0, iter: 22200, loss: 32.269989013671875
epoch: 0, iter: 22300, loss: 31.861957550048828
epoch: 0, iter: 22400, loss: 35.57160949707031
epoch: 0, iter: 22500, loss: 31.28049087524414
epoch: 0, iter: 22600, loss: 32.447288513183594
epoch: 0, iter: 22700, loss: 31.807647705078125
epoch: 0, iter: 22800, loss: 31.540283203125
epoch: 0, iter: 22900, loss: 31.646018981933594
epoch: 0, iter: 23000, loss: 32.140228271484375
epoch: 0, iter: 23100, loss: 31.19212532043457
epoch: 0, iter: 23200, loss: 32.096595764160156
epoch: 0, iter: 23300, loss: 32.60624313354492
epoch: 0, iter: 23400, loss: 31.942745208740234
epoch: 0, iter: 23500, loss: 32.21788787841797
epoch: 0, iter: 23600, loss: 32.34299087524414
epoch: 0, iter: 23700, loss: 31.90642547607422
epoch: 0, iter: 23800, loss: 31.761348724365234
epoch: 0, iter: 23900, loss: 32.32670211791992
epoch: 0, iter: 24000, loss: 32.27470397949219
epoch: 0, iteration: 24000, simlex-999: SpearmanrResult(correlation=0.08547535475215376, pvalue=0.008154549896277891), men: SpearmanrResult(correlation=0.08635481027650124, pvalue=1.073940217602237e-05), sim353: SpearmanrResult(correlation=0.05715118428542604, pvalue=0.309644216967956), nearest to monster: [&apos;monster&apos;, &apos;sword&apos;, &apos;hero&apos;, &apos;ghost&apos;, &apos;plain&apos;, &apos;household&apos;, &apos;situated&apos;, &apos;brand&apos;, &apos;torah&apos;, &apos;mouse&apos;]

epoch: 0, iter: 24100, loss: 31.711109161376953
epoch: 0, iter: 24200, loss: 31.729236602783203
epoch: 0, iter: 24300, loss: 31.751216888427734
epoch: 0, iter: 24400, loss: 31.54802131652832
epoch: 0, iter: 24500, loss: 31.819448471069336
epoch: 0, iter: 24600, loss: 31.87582778930664
epoch: 0, iter: 24700, loss: 32.44230651855469
epoch: 0, iter: 24800, loss: 32.13909149169922
epoch: 0, iter: 24900, loss: 31.6838321685791
epoch: 0, iter: 25000, loss: 32.01523208618164
epoch: 0, iter: 25100, loss: 31.727489471435547
epoch: 0, iter: 25200, loss: 32.378543853759766
epoch: 0, iter: 25300, loss: 32.155052185058594
epoch: 0, iter: 25400, loss: 32.30049514770508
epoch: 0, iter: 25500, loss: 32.10628128051758
epoch: 0, iter: 25600, loss: 32.01287841796875
epoch: 0, iter: 25700, loss: 32.22496032714844
epoch: 0, iter: 25800, loss: 32.15202331542969
epoch: 0, iter: 25900, loss: 32.43567657470703
epoch: 0, iter: 26000, loss: 31.745975494384766
epoch: 0, iteration: 26000, simlex-999: SpearmanrResult(correlation=0.08715629365703002, pvalue=0.0069793574982666565), men: SpearmanrResult(correlation=0.08749437789759629, pvalue=8.194697761171436e-06), sim353: SpearmanrResult(correlation=0.05971657549964074, pvalue=0.28839311254438554), nearest to monster: [&apos;monster&apos;, &apos;sword&apos;, &apos;household&apos;, &apos;hero&apos;, &apos;tale&apos;, &apos;priest&apos;, &apos;label&apos;, &apos;plain&apos;, &apos;mouse&apos;, &apos;ghost&apos;]

epoch: 0, iter: 26100, loss: 32.09526824951172
epoch: 0, iter: 26200, loss: 31.927221298217773
epoch: 0, iter: 26300, loss: 31.239913940429688
epoch: 0, iter: 26400, loss: 31.676891326904297
epoch: 0, iter: 26500, loss: 31.83584213256836
epoch: 0, iter: 26600, loss: 32.34405517578125
epoch: 0, iter: 26700, loss: 31.836318969726562
epoch: 0, iter: 26800, loss: 31.805145263671875
epoch: 0, iter: 26900, loss: 31.517250061035156
epoch: 0, iter: 27000, loss: 32.060646057128906
epoch: 0, iter: 27100, loss: 31.427961349487305
epoch: 0, iter: 27200, loss: 32.71056365966797
epoch: 0, iter: 27300, loss: 32.101768493652344
epoch: 0, iter: 27400, loss: 31.706729888916016
epoch: 0, iter: 27500, loss: 31.794944763183594
epoch: 0, iter: 27600, loss: 31.043569564819336
epoch: 0, iter: 27700, loss: 31.815420150756836
epoch: 0, iter: 27800, loss: 31.480987548828125
epoch: 0, iter: 27900, loss: 32.0
epoch: 0, iter: 28000, loss: 31.647756576538086
epoch: 0, iteration: 28000, simlex-999: SpearmanrResult(correlation=0.0879225360679704, pvalue=0.006495932231970623), men: SpearmanrResult(correlation=0.08939464976521133, pvalue=5.181905435780726e-06), sim353: SpearmanrResult(correlation=0.06028361484068362, pvalue=0.28383120456458), nearest to monster: [&apos;monster&apos;, &apos;sword&apos;, &apos;plain&apos;, &apos;mouse&apos;, &apos;tale&apos;, &apos;hero&apos;, &apos;brand&apos;, &apos;patent&apos;, &apos;tail&apos;, &apos;ghost&apos;]

epoch: 0, iter: 28100, loss: 31.732290267944336
epoch: 0, iter: 28200, loss: 31.56043243408203
epoch: 0, iter: 28300, loss: 32.17532730102539
epoch: 0, iter: 28400, loss: 32.34858322143555
epoch: 0, iter: 28500, loss: 31.321521759033203
epoch: 0, iter: 28600, loss: 31.24187469482422
epoch: 0, iter: 28700, loss: 31.808574676513672
epoch: 0, iter: 28800, loss: 31.126705169677734
epoch: 0, iter: 28900, loss: 32.27989959716797
epoch: 0, iter: 29000, loss: 31.510923385620117
epoch: 0, iter: 29100, loss: 32.18346405029297
epoch: 0, iter: 29200, loss: 31.19722557067871
epoch: 0, iter: 29300, loss: 31.348796844482422
epoch: 0, iter: 29400, loss: 32.03580856323242
epoch: 0, iter: 29500, loss: 31.560871124267578
epoch: 0, iter: 29600, loss: 32.1707763671875
epoch: 0, iter: 29700, loss: 31.646257400512695
epoch: 0, iter: 29800, loss: 32.089317321777344
epoch: 0, iter: 29900, loss: 31.21417808532715
epoch: 0, iter: 30000, loss: 31.596721649169922
epoch: 0, iteration: 30000, simlex-999: SpearmanrResult(correlation=0.0908617796694403, pvalue=0.00490825911563686), men: SpearmanrResult(correlation=0.09006953525508496, pvalue=4.393754176783815e-06), sim353: SpearmanrResult(correlation=0.06781615126644898, pvalue=0.22783225512951796), nearest to monster: [&apos;monster&apos;, &apos;hero&apos;, &apos;sword&apos;, &apos;mouse&apos;, &apos;nickname&apos;, &apos;tale&apos;, &apos;plain&apos;, &apos;ghost&apos;, &apos;expedition&apos;, &apos;tube&apos;]

epoch: 0, iter: 30100, loss: 31.1719970703125
epoch: 0, iter: 30200, loss: 31.563777923583984
epoch: 0, iter: 30300, loss: 31.362476348876953
epoch: 0, iter: 30400, loss: 31.93914222717285
epoch: 0, iter: 30500, loss: 31.46084213256836
epoch: 0, iter: 30600, loss: 31.61031723022461
epoch: 0, iter: 30700, loss: 32.19886779785156
epoch: 0, iter: 30800, loss: 31.53145980834961
epoch: 0, iter: 30900, loss: 32.54494094848633
epoch: 0, iter: 31000, loss: 31.622350692749023
epoch: 0, iter: 31100, loss: 31.624622344970703
epoch: 0, iter: 31200, loss: 32.21925354003906
epoch: 0, iter: 31300, loss: 31.355022430419922
epoch: 0, iter: 31400, loss: 31.890806198120117
epoch: 0, iter: 31500, loss: 31.63449478149414
epoch: 0, iter: 31600, loss: 31.105436325073242
epoch: 0, iter: 31700, loss: 31.645238876342773
epoch: 0, iter: 31800, loss: 31.808307647705078
epoch: 0, iter: 31900, loss: 31.988243103027344
epoch: 0, iter: 32000, loss: 31.83148193359375
epoch: 0, iteration: 32000, simlex-999: SpearmanrResult(correlation=0.0910634737532756, pvalue=0.004813389051845152), men: SpearmanrResult(correlation=0.09222228979601282, pvalue=2.5756952028504964e-06), sim353: SpearmanrResult(correlation=0.07123238137344272, pvalue=0.20520429313982647), nearest to monster: [&apos;monster&apos;, &apos;hero&apos;, &apos;nickname&apos;, &apos;sword&apos;, &apos;tale&apos;, &apos;plain&apos;, &apos;mouse&apos;, &apos;ghost&apos;, &apos;tail&apos;, &apos;tube&apos;]

epoch: 0, iter: 32100, loss: 31.311588287353516
epoch: 0, iter: 32200, loss: 31.257244110107422
epoch: 0, iter: 32300, loss: 31.649892807006836
epoch: 0, iter: 32400, loss: 31.635969161987305
epoch: 0, iter: 32500, loss: 31.34613037109375
epoch: 0, iter: 32600, loss: 31.666229248046875
epoch: 0, iter: 32700, loss: 31.63262176513672
epoch: 0, iter: 32800, loss: 31.727909088134766
epoch: 0, iter: 32900, loss: 32.014007568359375
epoch: 0, iter: 33000, loss: 31.64935302734375
epoch: 0, iter: 33100, loss: 31.75027084350586
epoch: 0, iter: 33200, loss: 30.913625717163086
epoch: 0, iter: 33300, loss: 32.485591888427734
epoch: 0, iter: 33400, loss: 30.946617126464844
epoch: 0, iter: 33500, loss: 31.906150817871094
epoch: 0, iter: 33600, loss: 31.456090927124023
epoch: 0, iter: 33700, loss: 31.70574188232422
epoch: 0, iter: 33800, loss: 31.611658096313477
epoch: 0, iter: 33900, loss: 31.901599884033203
epoch: 0, iter: 34000, loss: 30.904211044311523
epoch: 0, iteration: 34000, simlex-999: SpearmanrResult(correlation=0.09100297719676911, pvalue=0.004841669162232116), men: SpearmanrResult(correlation=0.09320434671047619, pvalue=2.0108242547194325e-06), sim353: SpearmanrResult(correlation=0.07643014870593084, pvalue=0.1739669852121724), nearest to monster: [&apos;monster&apos;, &apos;nickname&apos;, &apos;tale&apos;, &apos;mouse&apos;, &apos;brand&apos;, &apos;hero&apos;, &apos;plain&apos;, &apos;partner&apos;, &apos;owner&apos;, &apos;cave&apos;]

epoch: 0, iter: 34100, loss: 30.875843048095703
epoch: 0, iter: 34200, loss: 31.53815460205078
epoch: 0, iter: 34300, loss: 31.3868465423584
epoch: 0, iter: 34400, loss: 31.618576049804688
epoch: 0, iter: 34500, loss: 31.38482666015625
epoch: 0, iter: 34600, loss: 31.517066955566406
epoch: 0, iter: 34700, loss: 31.297931671142578
epoch: 0, iter: 34800, loss: 31.131715774536133
epoch: 0, iter: 34900, loss: 31.34206199645996
epoch: 0, iter: 35000, loss: 31.198501586914062
epoch: 0, iter: 35100, loss: 31.92325782775879
epoch: 0, iter: 35200, loss: 31.495628356933594
epoch: 0, iter: 35300, loss: 31.19044303894043
epoch: 0, iter: 35400, loss: 31.896709442138672
epoch: 0, iter: 35500, loss: 31.638015747070312
epoch: 0, iter: 35600, loss: 31.722248077392578
epoch: 0, iter: 35700, loss: 31.750402450561523
epoch: 0, iter: 35800, loss: 31.107473373413086
epoch: 0, iter: 35900, loss: 31.830018997192383
epoch: 0, iter: 36000, loss: 31.638286590576172
epoch: 0, iteration: 36000, simlex-999: SpearmanrResult(correlation=0.09287122747290566, pvalue=0.004034242774511441), men: SpearmanrResult(correlation=0.09638454243178861, pvalue=8.867706115523595e-07), sim353: SpearmanrResult(correlation=0.08196414667104787, pvalue=0.14474986358858538), nearest to monster: [&apos;monster&apos;, &apos;nickname&apos;, &apos;plain&apos;, &apos;sword&apos;, &apos;tail&apos;, &apos;mouse&apos;, &apos;brand&apos;, &apos;hero&apos;, &apos;tale&apos;, &apos;shell&apos;]

epoch: 0, iter: 36100, loss: 30.960386276245117
epoch: 0, iter: 36200, loss: 31.629940032958984
epoch: 0, iter: 36300, loss: 31.541032791137695
epoch: 0, iter: 36400, loss: 31.05801773071289
epoch: 0, iter: 36500, loss: 31.969802856445312
epoch: 0, iter: 36600, loss: 31.290489196777344
epoch: 0, iter: 36700, loss: 31.409465789794922
epoch: 0, iter: 36800, loss: 31.444076538085938
epoch: 0, iter: 36900, loss: 31.494474411010742
epoch: 0, iter: 37000, loss: 31.12554931640625
epoch: 0, iter: 37100, loss: 31.744049072265625
epoch: 0, iter: 37200, loss: 31.608917236328125
epoch: 0, iter: 37300, loss: 31.441722869873047
epoch: 0, iter: 37400, loss: 31.544227600097656
epoch: 0, iter: 37500, loss: 31.359806060791016
epoch: 0, iter: 37600, loss: 31.130847930908203
epoch: 0, iter: 37700, loss: 32.14916229248047
epoch: 0, iter: 37800, loss: 31.148212432861328
epoch: 0, iter: 37900, loss: 31.835248947143555
epoch: 0, iter: 38000, loss: 31.421974182128906
epoch: 0, iteration: 38000, simlex-999: SpearmanrResult(correlation=0.09401565185194706, pvalue=0.003602024110356835), men: SpearmanrResult(correlation=0.09723017395213002, pvalue=7.101718335843492e-07), sim353: SpearmanrResult(correlation=0.08744795260499, pvalue=0.1196457667795805), nearest to monster: [&apos;monster&apos;, &apos;nickname&apos;, &apos;plain&apos;, &apos;sword&apos;, &apos;tail&apos;, &apos;hero&apos;, &apos;shell&apos;, &apos;brand&apos;, &apos;mouse&apos;, &apos;cave&apos;]

epoch: 0, iter: 38100, loss: 30.881635665893555
epoch: 0, iter: 38200, loss: 31.16852569580078
epoch: 0, iter: 38300, loss: 31.63935089111328
epoch: 0, iter: 38400, loss: 31.25921058654785
epoch: 0, iter: 38500, loss: 31.57360076904297
epoch: 0, iter: 38600, loss: 31.1456298828125
epoch: 0, iter: 38700, loss: 31.269453048706055
epoch: 0, iter: 38800, loss: 31.55490493774414
epoch: 0, iter: 38900, loss: 31.626995086669922
epoch: 0, iter: 39000, loss: 31.255146026611328
epoch: 0, iter: 39100, loss: 31.211166381835938
epoch: 0, iter: 39200, loss: 31.450740814208984
epoch: 0, iter: 39300, loss: 31.785430908203125
epoch: 0, iter: 39400, loss: 30.558988571166992
epoch: 0, iter: 39500, loss: 31.32469940185547
epoch: 0, iter: 39600, loss: 31.5628604888916
epoch: 0, iter: 39700, loss: 31.271900177001953
epoch: 0, iter: 39800, loss: 31.499229431152344
epoch: 0, iter: 39900, loss: 31.45954704284668
epoch: 0, iter: 40000, loss: 30.844253540039062
epoch: 0, iteration: 40000, simlex-999: SpearmanrResult(correlation=0.09659229076909075, pvalue=0.0027787052714036363), men: SpearmanrResult(correlation=0.09859835382112378, pvalue=4.938796863215718e-07), sim353: SpearmanrResult(correlation=0.09091941260502377, pvalue=0.10559925075777613), nearest to monster: [&apos;monster&apos;, &apos;nickname&apos;, &apos;plain&apos;, &apos;hero&apos;, &apos;cave&apos;, &apos;sword&apos;, &apos;tail&apos;, &apos;owner&apos;, &apos;dialogue&apos;, &apos;mouse&apos;]

epoch: 0, iter: 40100, loss: 31.289958953857422
epoch: 0, iter: 40200, loss: 31.427631378173828
epoch: 0, iter: 40300, loss: 30.93175506591797
epoch: 0, iter: 40400, loss: 31.097423553466797
epoch: 0, iter: 40500, loss: 31.367881774902344
epoch: 0, iter: 40600, loss: 30.997957229614258
epoch: 0, iter: 40700, loss: 31.378498077392578
epoch: 0, iter: 40800, loss: 31.591278076171875
epoch: 0, iter: 40900, loss: 31.236934661865234
epoch: 0, iter: 41000, loss: 31.594310760498047
epoch: 0, iter: 41100, loss: 31.448932647705078
epoch: 0, iter: 41200, loss: 30.75921058654785
epoch: 0, iter: 41300, loss: 31.807411193847656
epoch: 0, iter: 41400, loss: 30.96005630493164
epoch: 0, iter: 41500, loss: 31.805885314941406
epoch: 0, iter: 41600, loss: 31.190258026123047
epoch: 0, iter: 41700, loss: 31.110252380371094
epoch: 0, iter: 41800, loss: 31.04319190979004
epoch: 0, iter: 41900, loss: 30.97702407836914
epoch: 0, iter: 42000, loss: 31.0760440826416
epoch: 0, iteration: 42000, simlex-999: SpearmanrResult(correlation=0.09849883639358918, pvalue=0.0022842954860782523), men: SpearmanrResult(correlation=0.09878607981201826, pvalue=4.696928075901211e-07), sim353: SpearmanrResult(correlation=0.09607407823044349, pvalue=0.08718116473821737), nearest to monster: [&apos;monster&apos;, &apos;cave&apos;, &apos;plain&apos;, &apos;mouse&apos;, &apos;nickname&apos;, &apos;diamond&apos;, &apos;dialogue&apos;, &apos;partner&apos;, &apos;hero&apos;, &apos;signature&apos;]

epoch: 0, iter: 42100, loss: 31.264083862304688
epoch: 0, iter: 42200, loss: 31.662830352783203
epoch: 0, iter: 42300, loss: 30.693662643432617
epoch: 0, iter: 42400, loss: 31.405860900878906
epoch: 0, iter: 42500, loss: 30.938379287719727
epoch: 0, iter: 42600, loss: 31.08720588684082
epoch: 0, iter: 42700, loss: 31.5560302734375
epoch: 0, iter: 42800, loss: 31.49104881286621
epoch: 0, iter: 42900, loss: 31.75652503967285
epoch: 0, iter: 43000, loss: 31.436534881591797
epoch: 0, iter: 43100, loss: 31.30294418334961
epoch: 0, iter: 43200, loss: 30.177589416503906
epoch: 0, iter: 43300, loss: 31.117063522338867
epoch: 0, iter: 43400, loss: 30.985565185546875
epoch: 0, iter: 43500, loss: 30.83687973022461
epoch: 0, iter: 43600, loss: 31.235471725463867
epoch: 0, iter: 43700, loss: 31.702655792236328
epoch: 0, iter: 43800, loss: 31.2994441986084
epoch: 0, iter: 43900, loss: 30.892574310302734
epoch: 0, iter: 44000, loss: 31.143707275390625
epoch: 0, iteration: 44000, simlex-999: SpearmanrResult(correlation=0.1008180671120631, pvalue=0.001791807298558225), men: SpearmanrResult(correlation=0.09959304604435494, pvalue=3.781135368008198e-07), sim353: SpearmanrResult(correlation=0.10352488934150769, pvalue=0.06521224014872654), nearest to monster: [&apos;monster&apos;, &apos;mouse&apos;, &apos;plain&apos;, &apos;nickname&apos;, &apos;cave&apos;, &apos;sword&apos;, &apos;boat&apos;, &apos;dialogue&apos;, &apos;partner&apos;, &apos;signature&apos;]

epoch: 0, iter: 44100, loss: 30.712566375732422
epoch: 0, iter: 44200, loss: 31.178279876708984
epoch: 0, iter: 44300, loss: 31.13910484313965
epoch: 0, iter: 44400, loss: 31.227054595947266
epoch: 0, iter: 44500, loss: 31.095703125
epoch: 0, iter: 44600, loss: 31.12378692626953
epoch: 0, iter: 44700, loss: 31.701740264892578
epoch: 0, iter: 44800, loss: 30.913339614868164
epoch: 0, iter: 44900, loss: 31.539695739746094
epoch: 0, iter: 45000, loss: 31.188980102539062
epoch: 0, iter: 45100, loss: 30.845016479492188
epoch: 0, iter: 45200, loss: 30.882841110229492
epoch: 0, iter: 45300, loss: 31.02661895751953
epoch: 0, iter: 45400, loss: 31.336511611938477
epoch: 0, iter: 45500, loss: 31.420623779296875
epoch: 0, iter: 45600, loss: 31.25517463684082
epoch: 0, iter: 45700, loss: 31.28260040283203
epoch: 0, iter: 45800, loss: 31.164663314819336
epoch: 0, iter: 45900, loss: 31.538354873657227
epoch: 0, iter: 46000, loss: 30.74416732788086
epoch: 0, iteration: 46000, simlex-999: SpearmanrResult(correlation=0.10155071688717769, pvalue=0.0016577893244519035), men: SpearmanrResult(correlation=0.10163250557010925, pvalue=2.1692539986465238e-07), sim353: SpearmanrResult(correlation=0.10604008852454751, pvalue=0.058912219453194116), nearest to monster: [&apos;monster&apos;, &apos;plain&apos;, &apos;nickname&apos;, &apos;sword&apos;, &apos;parent&apos;, &apos;mouse&apos;, &apos;dialogue&apos;, &apos;blade&apos;, &apos;boat&apos;, &apos;cave&apos;]

epoch: 0, iter: 46100, loss: 31.790225982666016
epoch: 0, iter: 46200, loss: 31.57557487487793
epoch: 0, iter: 46300, loss: 31.457191467285156
epoch: 0, iter: 46400, loss: 31.26146697998047
epoch: 0, iter: 46500, loss: 31.269039154052734
epoch: 0, iter: 46600, loss: 31.480712890625
epoch: 0, iter: 46700, loss: 31.428335189819336
epoch: 0, iter: 46800, loss: 30.886512756347656
epoch: 0, iter: 46900, loss: 31.599918365478516
epoch: 0, iter: 47000, loss: 31.132366180419922
epoch: 0, iter: 47100, loss: 30.962696075439453
epoch: 0, iter: 47200, loss: 31.67426300048828
epoch: 0, iter: 47300, loss: 30.73318862915039
epoch: 0, iter: 47400, loss: 31.673181533813477
epoch: 0, iter: 47500, loss: 31.30075454711914
epoch: 0, iter: 47600, loss: 31.427719116210938
epoch: 0, iter: 47700, loss: 31.111129760742188
epoch: 0, iter: 47800, loss: 31.14962387084961
epoch: 0, iter: 47900, loss: 31.174724578857422
epoch: 0, iter: 48000, loss: 31.114784240722656
epoch: 0, iteration: 48000, simlex-999: SpearmanrResult(correlation=0.10335720444738267, pvalue=0.0013657280173897167), men: SpearmanrResult(correlation=0.10165177484469035, pvalue=2.157784847483254e-07), sim353: SpearmanrResult(correlation=0.11041761731040038, pvalue=0.049149368437484346), nearest to monster: [&apos;monster&apos;, &apos;plain&apos;, &apos;nickname&apos;, &apos;parent&apos;, &apos;sword&apos;, &apos;blade&apos;, &apos;tail&apos;, &apos;dialogue&apos;, &apos;cave&apos;, &apos;boat&apos;]

epoch: 0, iter: 48100, loss: 31.263320922851562
epoch: 0, iter: 48200, loss: 31.267719268798828
epoch: 0, iter: 48300, loss: 31.259817123413086
epoch: 0, iter: 48400, loss: 30.922761917114258
epoch: 0, iter: 48500, loss: 31.48914909362793
epoch: 0, iter: 48600, loss: 31.45376205444336
epoch: 0, iter: 48700, loss: 30.948339462280273
epoch: 0, iter: 48800, loss: 30.842824935913086
epoch: 0, iter: 48900, loss: 30.931697845458984
epoch: 0, iter: 49000, loss: 31.468204498291016
epoch: 0, iter: 49100, loss: 31.04726791381836
epoch: 0, iter: 49200, loss: 31.148698806762695
epoch: 0, iter: 49300, loss: 31.295198440551758
epoch: 0, iter: 49400, loss: 31.415983200073242
epoch: 0, iter: 49500, loss: 31.53121566772461
epoch: 0, iter: 49600, loss: 30.391773223876953
epoch: 0, iter: 49700, loss: 31.365924835205078
epoch: 0, iter: 49800, loss: 30.920448303222656
epoch: 0, iter: 49900, loss: 30.881540298461914
epoch: 0, iter: 50000, loss: 31.272510528564453
epoch: 0, iteration: 50000, simlex-999: SpearmanrResult(correlation=0.10413335271622073, pvalue=0.0012554545146236879), men: SpearmanrResult(correlation=0.10361287469529604, pvalue=1.251734153196469e-07), sim353: SpearmanrResult(correlation=0.11252176428274015, pvalue=0.04496085066226139), nearest to monster: [&apos;monster&apos;, &apos;parent&apos;, &apos;sword&apos;, &apos;nickname&apos;, &apos;boat&apos;, &apos;plain&apos;, &apos;tail&apos;, &apos;leg&apos;, &apos;mouse&apos;, &apos;blade&apos;]

epoch: 0, iter: 50100, loss: 31.378141403198242
epoch: 0, iter: 50200, loss: 30.816102981567383
epoch: 0, iter: 50300, loss: 30.845239639282227
epoch: 0, iter: 50400, loss: 30.991004943847656
epoch: 0, iter: 50500, loss: 30.891719818115234
epoch: 0, iter: 50600, loss: 31.482940673828125
epoch: 0, iter: 50700, loss: 31.31090545654297
epoch: 0, iter: 50800, loss: 31.34703826904297
epoch: 0, iter: 50900, loss: 31.271032333374023
epoch: 0, iter: 51000, loss: 31.262798309326172
epoch: 0, iter: 51100, loss: 31.295764923095703
epoch: 0, iter: 51200, loss: 31.204692840576172
epoch: 0, iter: 51300, loss: 31.768779754638672
epoch: 0, iter: 51400, loss: 30.988128662109375
epoch: 0, iter: 51500, loss: 31.494434356689453
epoch: 0, iter: 51600, loss: 31.034160614013672
epoch: 0, iter: 51700, loss: 31.57693099975586
epoch: 0, iter: 51800, loss: 31.073469161987305
epoch: 0, iter: 51900, loss: 30.947439193725586
epoch: 0, iter: 52000, loss: 31.44693374633789
epoch: 0, iteration: 52000, simlex-999: SpearmanrResult(correlation=0.10512161472015334, pvalue=0.0011269247727856127), men: SpearmanrResult(correlation=0.10482327567400625, pvalue=8.899831939238548e-08), sim353: SpearmanrResult(correlation=0.11557228293582009, pvalue=0.03942341315579966), nearest to monster: [&apos;monster&apos;, &apos;parent&apos;, &apos;plain&apos;, &apos;nickname&apos;, &apos;tail&apos;, &apos;blade&apos;, &apos;leg&apos;, &apos;sword&apos;, &apos;mouse&apos;, &apos;signature&apos;]

epoch: 0, iter: 52100, loss: 31.460336685180664
epoch: 0, iter: 52200, loss: 31.668170928955078
epoch: 0, iter: 52300, loss: 30.479820251464844
epoch: 0, iter: 52400, loss: 31.74282455444336
epoch: 0, iter: 52500, loss: 30.803956985473633
epoch: 0, iter: 52600, loss: 30.81981658935547
epoch: 0, iter: 52700, loss: 31.19491958618164
epoch: 0, iter: 52800, loss: 31.08262825012207
epoch: 0, iter: 52900, loss: 31.718488693237305
epoch: 0, iter: 53000, loss: 30.93524932861328
epoch: 0, iter: 53100, loss: 31.013084411621094
epoch: 0, iter: 53200, loss: 30.91759490966797
epoch: 0, iter: 53300, loss: 31.814682006835938
epoch: 0, iter: 53400, loss: 30.962512969970703
epoch: 0, iter: 53500, loss: 30.939342498779297
epoch: 0, iter: 53600, loss: 31.67828369140625
epoch: 0, iter: 53700, loss: 31.302024841308594
epoch: 0, iter: 53800, loss: 30.955366134643555
epoch: 0, iter: 53900, loss: 31.510623931884766
epoch: 0, iter: 54000, loss: 30.676362991333008
epoch: 0, iteration: 54000, simlex-999: SpearmanrResult(correlation=0.10549625057039985, pvalue=0.0010814608115210568), men: SpearmanrResult(correlation=0.10721055714110006, pvalue=4.4913037691946484e-08), sim353: SpearmanrResult(correlation=0.11769079714384932, pvalue=0.03592648928681193), nearest to monster: [&apos;monster&apos;, &apos;cave&apos;, &apos;plain&apos;, &apos;tail&apos;, &apos;nickname&apos;, &apos;leg&apos;, &apos;parent&apos;, &apos;blade&apos;, &apos;ghost&apos;, &apos;sword&apos;]

epoch: 0, iter: 54100, loss: 30.862953186035156
epoch: 0, iter: 54200, loss: 31.025741577148438
epoch: 0, iter: 54300, loss: 31.63794708251953
epoch: 0, iter: 54400, loss: 31.128381729125977
epoch: 0, iter: 54500, loss: 31.111047744750977
epoch: 0, iter: 54600, loss: 31.013673782348633
epoch: 0, iter: 54700, loss: 31.087692260742188
epoch: 0, iter: 54800, loss: 31.236892700195312
epoch: 0, iter: 54900, loss: 31.256492614746094
epoch: 0, iter: 55000, loss: 31.33741569519043
epoch: 0, iter: 55100, loss: 30.920692443847656
epoch: 0, iter: 55200, loss: 30.418161392211914
epoch: 0, iter: 55300, loss: 31.029638290405273
epoch: 0, iter: 55400, loss: 30.816083908081055
epoch: 0, iter: 55500, loss: 31.28571891784668
epoch: 0, iter: 55600, loss: 30.913631439208984
epoch: 0, iter: 55700, loss: 31.216411590576172
epoch: 0, iter: 55800, loss: 30.906490325927734
epoch: 0, iter: 55900, loss: 31.138019561767578
epoch: 0, iter: 56000, loss: 31.294254302978516
epoch: 0, iteration: 56000, simlex-999: SpearmanrResult(correlation=0.10747042881452093, pvalue=0.000868628570627656), men: SpearmanrResult(correlation=0.10889856420740306, pvalue=2.7445360530151176e-08), sim353: SpearmanrResult(correlation=0.12059633732769914, pvalue=0.03156119729084597), nearest to monster: [&apos;monster&apos;, &apos;leg&apos;, &apos;tail&apos;, &apos;plain&apos;, &apos;nickname&apos;, &apos;sword&apos;, &apos;cave&apos;, &apos;parent&apos;, &apos;signature&apos;, &apos;blade&apos;]

epoch: 0, iter: 56100, loss: 31.756465911865234
epoch: 0, iter: 56200, loss: 30.841522216796875
epoch: 0, iter: 56300, loss: 31.101408004760742
epoch: 0, iter: 56400, loss: 30.875770568847656
epoch: 0, iter: 56500, loss: 31.620193481445312
epoch: 0, iter: 56600, loss: 31.299055099487305
epoch: 0, iter: 56700, loss: 31.310087203979492
epoch: 0, iter: 56800, loss: 31.34184455871582
epoch: 0, iter: 56900, loss: 31.240936279296875
epoch: 0, iter: 57000, loss: 30.90418815612793
epoch: 0, iter: 57100, loss: 31.257062911987305
epoch: 0, iter: 57200, loss: 31.695873260498047
epoch: 0, iter: 57300, loss: 30.94247055053711
epoch: 0, iter: 57400, loss: 30.684371948242188
epoch: 0, iter: 57500, loss: 31.504837036132812
epoch: 0, iter: 57600, loss: 31.262527465820312
epoch: 0, iter: 57700, loss: 31.38228988647461
epoch: 0, iter: 57800, loss: 31.27547836303711
epoch: 0, iter: 57900, loss: 30.56299591064453
epoch: 0, iter: 58000, loss: 30.96505355834961
epoch: 0, iteration: 58000, simlex-999: SpearmanrResult(correlation=0.10726587013548153, pvalue=0.0008887284442590619), men: SpearmanrResult(correlation=0.1093378019236274, pvalue=2.411455598304873e-08), sim353: SpearmanrResult(correlation=0.12085774638225694, pvalue=0.031191656645013426), nearest to monster: [&apos;monster&apos;, &apos;tail&apos;, &apos;mouse&apos;, &apos;plain&apos;, &apos;blade&apos;, &apos;cave&apos;, &apos;signature&apos;, &apos;dubbed&apos;, &apos;angel&apos;, &apos;leg&apos;]

epoch: 0, iter: 58100, loss: 30.774898529052734
epoch: 0, iter: 58200, loss: 31.404376983642578
epoch: 0, iter: 58300, loss: 31.18744659423828
epoch: 0, iter: 58400, loss: 30.777774810791016
epoch: 0, iter: 58500, loss: 31.30267333984375
epoch: 0, iter: 58600, loss: 31.17416763305664
epoch: 0, iter: 58700, loss: 30.96724510192871
epoch: 0, iter: 58800, loss: 31.406417846679688
epoch: 0, iter: 58900, loss: 30.588558197021484
epoch: 0, iter: 59000, loss: 30.476577758789062
epoch: 0, iter: 59100, loss: 30.78055763244629
epoch: 0, iter: 59200, loss: 31.018653869628906
epoch: 0, iter: 59300, loss: 31.332136154174805
epoch: 0, iter: 59400, loss: 31.59677505493164
epoch: 0, iter: 59500, loss: 31.528217315673828
epoch: 0, iter: 59600, loss: 30.44378662109375
epoch: 0, iter: 59700, loss: 30.718303680419922
epoch: 0, iter: 59800, loss: 30.775535583496094
epoch: 0, iter: 59900, loss: 31.164199829101562
epoch: 0, iter: 60000, loss: 31.144628524780273
epoch: 0, iteration: 60000, simlex-999: SpearmanrResult(correlation=0.10787079798749166, pvalue=0.0008305009335772662), men: SpearmanrResult(correlation=0.11117447271855486, pvalue=1.3962055373097627e-08), sim353: SpearmanrResult(correlation=0.12349889211192522, pvalue=0.027660751055994845), nearest to monster: [&apos;monster&apos;, &apos;tail&apos;, &apos;signature&apos;, &apos;blade&apos;, &apos;angel&apos;, &apos;dubbed&apos;, &apos;mouse&apos;, &apos;pole&apos;, &apos;owner&apos;, &apos;plain&apos;]

epoch: 0, iter: 60100, loss: 31.20758056640625
epoch: 0, iter: 60200, loss: 30.711570739746094
epoch: 0, iter: 60300, loss: 30.836360931396484
epoch: 0, iter: 60400, loss: 30.35114097595215
epoch: 0, iter: 60500, loss: 30.544240951538086
epoch: 0, iter: 60600, loss: 31.264543533325195
epoch: 0, iter: 60700, loss: 31.218517303466797
epoch: 0, iter: 60800, loss: 31.23360824584961
epoch: 0, iter: 60900, loss: 30.85096549987793
epoch: 0, iter: 61000, loss: 30.768386840820312
epoch: 0, iter: 61100, loss: 31.50748634338379
epoch: 0, iter: 61200, loss: 30.46345329284668
epoch: 0, iter: 61300, loss: 30.543607711791992
epoch: 0, iter: 61400, loss: 30.628982543945312
epoch: 0, iter: 61500, loss: 31.45627784729004
epoch: 0, iter: 61600, loss: 31.070459365844727
epoch: 0, iter: 61700, loss: 30.569217681884766
epoch: 0, iter: 61800, loss: 30.83639907836914
epoch: 0, iter: 61900, loss: 31.005922317504883
epoch: 0, iter: 62000, loss: 31.41488265991211
epoch: 0, iteration: 62000, simlex-999: SpearmanrResult(correlation=0.11119875283206068, pvalue=0.0005685786512505508), men: SpearmanrResult(correlation=0.11318488733549789, pvalue=7.599257092187759e-09), sim353: SpearmanrResult(correlation=0.12779805415765372, pvalue=0.022646548827240445), nearest to monster: [&apos;monster&apos;, &apos;tail&apos;, &apos;blade&apos;, &apos;signature&apos;, &apos;plain&apos;, &apos;mouse&apos;, &apos;pole&apos;, &apos;boat&apos;, &apos;owner&apos;, &apos;leg&apos;]

epoch: 0, iter: 62100, loss: 31.053007125854492
epoch: 0, iter: 62200, loss: 30.765029907226562
epoch: 0, iter: 62300, loss: 31.114418029785156
epoch: 0, iter: 62400, loss: 30.98143768310547
epoch: 0, iter: 62500, loss: 31.071922302246094
epoch: 0, iter: 62600, loss: 31.17368507385254
epoch: 0, iter: 62700, loss: 31.177242279052734
epoch: 0, iter: 62800, loss: 31.408926010131836
epoch: 0, iter: 62900, loss: 30.88961410522461
epoch: 0, iter: 63000, loss: 30.848337173461914
epoch: 0, iter: 63100, loss: 30.798885345458984
epoch: 0, iter: 63200, loss: 30.96042251586914
epoch: 0, iter: 63300, loss: 30.656030654907227
epoch: 0, iter: 63400, loss: 31.166887283325195
epoch: 0, iter: 63500, loss: 30.926340103149414
epoch: 0, iter: 63600, loss: 31.11106300354004
epoch: 0, iter: 63700, loss: 31.001605987548828
epoch: 0, iter: 63800, loss: 30.872831344604492
epoch: 0, iter: 63900, loss: 31.2712345123291
epoch: 0, iter: 64000, loss: 31.084636688232422
epoch: 0, iteration: 64000, simlex-999: SpearmanrResult(correlation=0.11095436079645714, pvalue=0.0005848267448603055), men: SpearmanrResult(correlation=0.11504369222990082, pvalue=4.28960553784828e-09), sim353: SpearmanrResult(correlation=0.1318387925484482, pvalue=0.01867078025276011), nearest to monster: [&apos;monster&apos;, &apos;blade&apos;, &apos;tail&apos;, &apos;boat&apos;, &apos;nickname&apos;, &apos;owner&apos;, &apos;leg&apos;, &apos;plain&apos;, &apos;pole&apos;, &apos;ghost&apos;]

epoch: 0, iter: 64100, loss: 30.403200149536133
epoch: 0, iter: 64200, loss: 31.01869010925293
epoch: 0, iter: 64300, loss: 30.85900115966797
epoch: 0, iter: 64400, loss: 31.06339454650879
epoch: 0, iter: 64500, loss: 31.443498611450195
epoch: 0, iter: 64600, loss: 30.922685623168945
epoch: 0, iter: 64700, loss: 30.92823028564453
epoch: 0, iter: 64800, loss: 30.95685577392578
epoch: 0, iter: 64900, loss: 31.249370574951172
epoch: 0, iter: 65000, loss: 31.283973693847656
epoch: 0, iter: 65100, loss: 31.421056747436523
epoch: 0, iter: 65200, loss: 31.271799087524414
epoch: 0, iter: 65300, loss: 31.055686950683594
epoch: 0, iter: 65400, loss: 31.06484603881836
epoch: 0, iter: 65500, loss: 31.523380279541016
epoch: 0, iter: 65600, loss: 30.86985969543457
epoch: 0, iter: 65700, loss: 31.431381225585938
epoch: 0, iter: 65800, loss: 30.828258514404297
epoch: 0, iter: 65900, loss: 30.777324676513672
epoch: 0, iter: 66000, loss: 30.793434143066406
epoch: 0, iteration: 66000, simlex-999: SpearmanrResult(correlation=0.11336484593643655, pvalue=0.000441848134889879), men: SpearmanrResult(correlation=0.11607771416192764, pvalue=3.108529516895944e-09), sim353: SpearmanrResult(correlation=0.13567689693977825, pvalue=0.015471766603603733), nearest to monster: [&apos;monster&apos;, &apos;blade&apos;, &apos;tail&apos;, &apos;boat&apos;, &apos;plain&apos;, &apos;mouse&apos;, &apos;leg&apos;, &apos;dubbed&apos;, &apos;angel&apos;, &apos;signature&apos;]

epoch: 0, iter: 66100, loss: 30.86065101623535
epoch: 0, iter: 66200, loss: 30.876815795898438
epoch: 0, iter: 66300, loss: 31.069660186767578
epoch: 0, iter: 66400, loss: 30.777523040771484
epoch: 0, iter: 66500, loss: 31.19533920288086
epoch: 0, iter: 66600, loss: 30.554855346679688
epoch: 0, iter: 66700, loss: 30.99230194091797
epoch: 0, iter: 66800, loss: 31.07242202758789
epoch: 0, iter: 66900, loss: 30.73615264892578
epoch: 0, iter: 67000, loss: 31.139455795288086
epoch: 0, iter: 67100, loss: 30.713550567626953
epoch: 0, iter: 67200, loss: 31.377769470214844
epoch: 0, iter: 67300, loss: 31.406234741210938
epoch: 0, iter: 67400, loss: 30.695165634155273
epoch: 0, iter: 67500, loss: 31.24422836303711
epoch: 0, iter: 67600, loss: 30.47709083557129
epoch: 0, iter: 67700, loss: 30.54576301574707
epoch: 0, iter: 67800, loss: 31.31440544128418
epoch: 0, iter: 67900, loss: 30.91347312927246
epoch: 0, iter: 68000, loss: 31.376529693603516
epoch: 0, iteration: 68000, simlex-999: SpearmanrResult(correlation=0.11452703856698428, pvalue=0.00038523421818805733), men: SpearmanrResult(correlation=0.11616177986176482, pvalue=3.027823236694649e-09), sim353: SpearmanrResult(correlation=0.13998669447166648, pvalue=0.012461259148780957), nearest to monster: [&apos;monster&apos;, &apos;boat&apos;, &apos;blade&apos;, &apos;signature&apos;, &apos;tail&apos;, &apos;dubbed&apos;, &apos;mouse&apos;, &apos;cave&apos;, &apos;angel&apos;, &apos;replacing&apos;]

epoch: 0, iter: 68100, loss: 30.246761322021484
epoch: 0, iter: 68200, loss: 31.031679153442383
epoch: 0, iter: 68300, loss: 31.415462493896484
epoch: 0, iter: 68400, loss: 30.809803009033203
epoch: 0, iter: 68500, loss: 31.034048080444336
epoch: 0, iter: 68600, loss: 31.134660720825195
epoch: 0, iter: 68700, loss: 31.344093322753906
epoch: 0, iter: 68800, loss: 31.488487243652344
epoch: 0, iter: 68900, loss: 31.44832992553711
epoch: 0, iter: 69000, loss: 30.69766616821289
epoch: 0, iter: 69100, loss: 31.20623016357422
epoch: 0, iter: 69200, loss: 31.305984497070312
epoch: 0, iter: 69300, loss: 30.8837947845459
epoch: 0, iter: 69400, loss: 30.787147521972656
epoch: 0, iter: 69500, loss: 30.73443603515625
epoch: 0, iter: 69600, loss: 30.5230712890625
epoch: 0, iter: 69700, loss: 30.885122299194336
epoch: 0, iter: 69800, loss: 30.608633041381836
epoch: 0, iter: 69900, loss: 31.044784545898438
epoch: 0, iter: 70000, loss: 30.79353141784668
epoch: 0, iteration: 70000, simlex-999: SpearmanrResult(correlation=0.11447412364241306, pvalue=0.00038765739483096796), men: SpearmanrResult(correlation=0.11756886224004129, pvalue=1.944056651206125e-09), sim353: SpearmanrResult(correlation=0.1422861985318877, pvalue=0.011076632617477473), nearest to monster: [&apos;monster&apos;, &apos;boat&apos;, &apos;mouse&apos;, &apos;blade&apos;, &apos;angel&apos;, &apos;replacing&apos;, &apos;leg&apos;, &apos;signature&apos;, &apos;legendary&apos;, &apos;tail&apos;]

epoch: 0, iter: 70100, loss: 31.068965911865234
epoch: 0, iter: 70200, loss: 31.09067153930664
epoch: 0, iter: 70300, loss: 30.815410614013672
epoch: 0, iter: 70400, loss: 31.200820922851562
epoch: 0, iter: 70500, loss: 30.970481872558594
epoch: 0, iter: 70600, loss: 30.677066802978516
epoch: 0, iter: 70700, loss: 31.553955078125
epoch: 0, iter: 70800, loss: 30.71514892578125
epoch: 0, iter: 70900, loss: 30.628828048706055
epoch: 0, iter: 71000, loss: 30.579029083251953
epoch: 0, iter: 71100, loss: 30.71631622314453
epoch: 0, iter: 71200, loss: 31.383621215820312
epoch: 0, iter: 71300, loss: 30.815113067626953
epoch: 0, iter: 71400, loss: 31.219520568847656
epoch: 0, iter: 71500, loss: 30.879241943359375
epoch: 0, iter: 71600, loss: 30.864967346191406
epoch: 0, iter: 71700, loss: 31.130123138427734
epoch: 0, iter: 71800, loss: 31.275012969970703
epoch: 0, iter: 71900, loss: 30.728538513183594
epoch: 0, iter: 72000, loss: 30.295608520507812
epoch: 0, iteration: 72000, simlex-999: SpearmanrResult(correlation=0.11509252437598057, pvalue=0.00036020627011201973), men: SpearmanrResult(correlation=0.11721625827550092, pvalue=2.173406746828479e-09), sim353: SpearmanrResult(correlation=0.15003741548009542, pvalue=0.007357771752434236), nearest to monster: [&apos;monster&apos;, &apos;boat&apos;, &apos;leg&apos;, &apos;angel&apos;, &apos;replacing&apos;, &apos;pole&apos;, &apos;legendary&apos;, &apos;tail&apos;, &apos;mouse&apos;, &apos;signature&apos;]

epoch: 0, iter: 72100, loss: 30.66713523864746
epoch: 0, iter: 72200, loss: 30.61351776123047
epoch: 0, iter: 72300, loss: 31.320636749267578
epoch: 0, iter: 72400, loss: 31.034809112548828
epoch: 0, iter: 72500, loss: 31.062036514282227
epoch: 0, iter: 72600, loss: 30.442829132080078
epoch: 0, iter: 72700, loss: 30.91510581970215
epoch: 0, iter: 72800, loss: 30.70620346069336
epoch: 0, iter: 72900, loss: 30.421703338623047
epoch: 0, iter: 73000, loss: 30.53826141357422
epoch: 0, iter: 73100, loss: 30.770679473876953
epoch: 0, iter: 73200, loss: 31.04900360107422
epoch: 0, iter: 73300, loss: 30.795854568481445
epoch: 0, iter: 73400, loss: 31.299104690551758
epoch: 0, iter: 73500, loss: 30.484947204589844
epoch: 0, iter: 73600, loss: 30.79161834716797
epoch: 0, iter: 73700, loss: 30.636621475219727
epoch: 0, iter: 73800, loss: 31.00129508972168
epoch: 0, iter: 73900, loss: 30.91973114013672
epoch: 0, iter: 74000, loss: 31.55290985107422
epoch: 0, iteration: 74000, simlex-999: SpearmanrResult(correlation=0.11672803148915531, pvalue=0.0002961005658581428), men: SpearmanrResult(correlation=0.11817601695076835, pvalue=1.6031687449902205e-09), sim353: SpearmanrResult(correlation=0.15298232562148392, pvalue=0.006267834790300931), nearest to monster: [&apos;monster&apos;, &apos;angel&apos;, &apos;legendary&apos;, &apos;leg&apos;, &apos;boat&apos;, &apos;replacing&apos;, &apos;tail&apos;, &apos;dubbed&apos;, &apos;cave&apos;, &apos;pole&apos;]

epoch: 0, iter: 74100, loss: 31.169639587402344
epoch: 0, iter: 74200, loss: 30.829368591308594
epoch: 0, iter: 74300, loss: 30.788063049316406
epoch: 0, iter: 74400, loss: 30.632108688354492
epoch: 0, iter: 74500, loss: 30.72389030456543
epoch: 0, iter: 74600, loss: 30.648719787597656
epoch: 0, iter: 74700, loss: 31.583736419677734
epoch: 0, iter: 74800, loss: 30.765384674072266
epoch: 0, iter: 74900, loss: 30.931472778320312
epoch: 0, iter: 75000, loss: 30.993127822875977
epoch: 0, iter: 75100, loss: 30.643539428710938
epoch: 0, iter: 75200, loss: 30.458595275878906
epoch: 0, iter: 75300, loss: 30.298744201660156
epoch: 0, iter: 75400, loss: 30.824840545654297
epoch: 0, iter: 75500, loss: 31.22673797607422
epoch: 0, iter: 75600, loss: 30.644241333007812
epoch: 0, iter: 75700, loss: 30.66327667236328
epoch: 0, iter: 75800, loss: 31.115676879882812
epoch: 0, iter: 75900, loss: 30.466846466064453
epoch: 0, iter: 76000, loss: 30.9396915435791
epoch: 0, iteration: 76000, simlex-999: SpearmanrResult(correlation=0.11596381543200739, pvalue=0.00032459762253928627), men: SpearmanrResult(correlation=0.11879493045996324, pvalue=1.3158101883470934e-09), sim353: SpearmanrResult(correlation=0.15550872005311292, pvalue=0.005450575993741813), nearest to monster: [&apos;monster&apos;, &apos;angel&apos;, &apos;blade&apos;, &apos;replacing&apos;, &apos;boat&apos;, &apos;legendary&apos;, &apos;dubbed&apos;, &apos;leg&apos;, &apos;pole&apos;, &apos;tail&apos;]

epoch: 0, iter: 76100, loss: 30.884302139282227
epoch: 0, iter: 76200, loss: 30.992034912109375
epoch: 0, iter: 76300, loss: 30.93535041809082
epoch: 0, iter: 76400, loss: 31.227296829223633
epoch: 0, iter: 76500, loss: 30.600688934326172
epoch: 0, iter: 76600, loss: 30.734973907470703
epoch: 0, iter: 76700, loss: 31.285720825195312
epoch: 0, iter: 76800, loss: 30.783761978149414
epoch: 0, iter: 76900, loss: 31.069557189941406
epoch: 0, iter: 77000, loss: 31.12335205078125
epoch: 0, iter: 77100, loss: 30.547470092773438
epoch: 0, iter: 77200, loss: 30.63747215270996
epoch: 0, iter: 77300, loss: 30.923892974853516
epoch: 0, iter: 77400, loss: 30.970041275024414
epoch: 0, iter: 77500, loss: 31.035385131835938
epoch: 0, iter: 77600, loss: 30.704097747802734
epoch: 0, iter: 77700, loss: 30.910247802734375
epoch: 0, iter: 77800, loss: 30.7044734954834
epoch: 0, iter: 77900, loss: 30.660982131958008
epoch: 0, iter: 78000, loss: 30.560914993286133
epoch: 0, iteration: 78000, simlex-999: SpearmanrResult(correlation=0.11666061112723143, pvalue=0.00029851725007368435), men: SpearmanrResult(correlation=0.12038941573817902, pvalue=7.873392979563775e-10), sim353: SpearmanrResult(correlation=0.15946717336677685, pvalue=0.004361303455838544), nearest to monster: [&apos;monster&apos;, &apos;angel&apos;, &apos;legendary&apos;, &apos;blade&apos;, &apos;boat&apos;, &apos;leg&apos;, &apos;replacing&apos;, &apos;signature&apos;, &apos;tail&apos;, &apos;epic&apos;]

epoch: 0, iter: 78100, loss: 31.013874053955078
epoch: 0, iter: 78200, loss: 30.866724014282227
epoch: 0, iter: 78300, loss: 30.68968391418457
epoch: 0, iter: 78400, loss: 31.183582305908203
epoch: 0, iter: 78500, loss: 31.27776336669922
epoch: 0, iter: 78600, loss: 30.777645111083984
epoch: 0, iter: 78700, loss: 31.012584686279297
epoch: 0, iter: 78800, loss: 31.096542358398438
epoch: 0, iter: 78900, loss: 31.362171173095703
epoch: 0, iter: 79000, loss: 30.738359451293945
epoch: 0, iter: 79100, loss: 31.230934143066406
epoch: 0, iter: 79200, loss: 31.539630889892578
epoch: 0, iter: 79300, loss: 30.64007568359375
epoch: 0, iter: 79400, loss: 30.633243560791016
epoch: 0, iter: 79500, loss: 30.83672332763672
epoch: 0, iter: 79600, loss: 30.83807945251465
epoch: 0, iter: 79700, loss: 30.863502502441406
epoch: 0, iter: 79800, loss: 31.011892318725586
epoch: 0, iter: 79900, loss: 30.918609619140625
epoch: 0, iter: 80000, loss: 30.889875411987305
epoch: 0, iteration: 80000, simlex-999: SpearmanrResult(correlation=0.11674166145672565, pvalue=0.0002956142280039947), men: SpearmanrResult(correlation=0.12076227513300175, pvalue=6.975679989657089e-10), sim353: SpearmanrResult(correlation=0.16263542871751985, pvalue=0.00363550206573609), nearest to monster: [&apos;monster&apos;, &apos;blade&apos;, &apos;boat&apos;, &apos;leg&apos;, &apos;legendary&apos;, &apos;bird&apos;, &apos;angel&apos;, &apos;tail&apos;, &apos;signature&apos;, &apos;replacing&apos;]

epoch: 0, iter: 80100, loss: 30.875946044921875
epoch: 0, iter: 80200, loss: 31.290252685546875
epoch: 0, iter: 80300, loss: 30.575260162353516
epoch: 0, iter: 80400, loss: 31.11186981201172
epoch: 0, iter: 80500, loss: 30.141450881958008
epoch: 0, iter: 80600, loss: 30.628923416137695
epoch: 0, iter: 80700, loss: 29.730871200561523
epoch: 0, iter: 80800, loss: 30.972171783447266
epoch: 0, iter: 80900, loss: 30.97983169555664
epoch: 0, iter: 81000, loss: 31.120412826538086
epoch: 0, iter: 81100, loss: 31.14563751220703
epoch: 0, iter: 81200, loss: 30.718021392822266
epoch: 0, iter: 81300, loss: 31.257009506225586
epoch: 0, iter: 81400, loss: 30.679397583007812
epoch: 0, iter: 81500, loss: 30.84437370300293
epoch: 0, iter: 81600, loss: 31.431678771972656
epoch: 0, iter: 81700, loss: 30.983497619628906
epoch: 0, iter: 81800, loss: 30.411563873291016
epoch: 0, iter: 81900, loss: 30.98554801940918
epoch: 0, iter: 82000, loss: 30.836700439453125
epoch: 0, iteration: 82000, simlex-999: SpearmanrResult(correlation=0.11957387123233242, pvalue=0.0002092803818694985), men: SpearmanrResult(correlation=0.12247627027840459, pvalue=3.979672482221647e-10), sim353: SpearmanrResult(correlation=0.16490918370598598, pvalue=0.0031840146668676477), nearest to monster: [&apos;monster&apos;, &apos;blade&apos;, &apos;bird&apos;, &apos;boat&apos;, &apos;leg&apos;, &apos;legendary&apos;, &apos;angel&apos;, &apos;tail&apos;, &apos;signature&apos;, &apos;brand&apos;]

epoch: 0, iter: 82100, loss: 31.439632415771484
epoch: 0, iter: 82200, loss: 31.005916595458984
epoch: 0, iter: 82300, loss: 30.159162521362305
epoch: 0, iter: 82400, loss: 31.28647232055664
epoch: 0, iter: 82500, loss: 31.14252281188965
epoch: 0, iter: 82600, loss: 30.4515323638916
epoch: 0, iter: 82700, loss: 31.12192153930664
epoch: 0, iter: 82800, loss: 31.36789894104004
epoch: 0, iter: 82900, loss: 30.705039978027344
epoch: 0, iter: 83000, loss: 30.586198806762695
epoch: 0, iter: 83100, loss: 30.898677825927734
epoch: 0, iter: 83200, loss: 30.465381622314453
epoch: 0, iter: 83300, loss: 30.524826049804688
epoch: 0, iter: 83400, loss: 30.76988410949707
epoch: 0, iter: 83500, loss: 31.108976364135742
epoch: 0, iter: 83600, loss: 30.428987503051758
epoch: 0, iter: 83700, loss: 31.005210876464844
epoch: 0, iter: 83800, loss: 30.407583236694336
epoch: 0, iter: 83900, loss: 30.31291961669922
epoch: 0, iter: 84000, loss: 30.384244918823242
epoch: 0, iteration: 84000, simlex-999: SpearmanrResult(correlation=0.11985731732845852, pvalue=0.00020208569630055274), men: SpearmanrResult(correlation=0.12383970685303505, pvalue=2.5324734087303694e-10), sim353: SpearmanrResult(correlation=0.16816942668263687, pvalue=0.002625098004568829), nearest to monster: [&apos;monster&apos;, &apos;blade&apos;, &apos;leg&apos;, &apos;angel&apos;, &apos;boat&apos;, &apos;legendary&apos;, &apos;bird&apos;, &apos;tail&apos;, &apos;signature&apos;, &apos;epic&apos;]

epoch: 0, iter: 84100, loss: 30.388629913330078
epoch: 0, iter: 84200, loss: 30.61890983581543
epoch: 0, iter: 84300, loss: 30.860755920410156
epoch: 0, iter: 84400, loss: 30.6082763671875
epoch: 0, iter: 84500, loss: 30.51343536376953
epoch: 0, iter: 84600, loss: 31.175479888916016
epoch: 0, iter: 84700, loss: 30.97124481201172
epoch: 0, iter: 84800, loss: 30.84918975830078
epoch: 0, iter: 84900, loss: 30.95672035217285
epoch: 0, iter: 85000, loss: 31.12570571899414
epoch: 0, iter: 85100, loss: 31.057252883911133
epoch: 0, iter: 85200, loss: 30.39339828491211
epoch: 0, iter: 85300, loss: 30.523571014404297
epoch: 0, iter: 85400, loss: 30.765701293945312
epoch: 0, iter: 85500, loss: 30.65972137451172
epoch: 0, iter: 85600, loss: 30.2365779876709
epoch: 0, iter: 85700, loss: 31.060688018798828
epoch: 0, iter: 85800, loss: 31.084121704101562
epoch: 0, iter: 85900, loss: 30.77812957763672
epoch: 0, iter: 86000, loss: 30.55185890197754
epoch: 0, iteration: 86000, simlex-999: SpearmanrResult(correlation=0.12072190676944367, pvalue=0.00018154682975915078), men: SpearmanrResult(correlation=0.1252523395746619, pvalue=1.577244824410371e-10), sim353: SpearmanrResult(correlation=0.1690460146471711, pvalue=0.002490881483585671), nearest to monster: [&apos;monster&apos;, &apos;blade&apos;, &apos;leg&apos;, &apos;angel&apos;, &apos;boat&apos;, &apos;tail&apos;, &apos;bird&apos;, &apos;mirror&apos;, &apos;legendary&apos;, &apos;signature&apos;]

epoch: 0, iter: 86100, loss: 30.656890869140625
epoch: 0, iter: 86200, loss: 30.80274200439453
epoch: 0, iter: 86300, loss: 30.992799758911133
epoch: 0, iter: 86400, loss: 30.460365295410156
epoch: 0, iter: 86500, loss: 30.55353546142578
epoch: 0, iter: 86600, loss: 31.388164520263672
epoch: 0, iter: 86700, loss: 30.856948852539062
epoch: 0, iter: 86800, loss: 30.76443099975586
epoch: 0, iter: 86900, loss: 30.570655822753906
epoch: 0, iter: 87000, loss: 30.948423385620117
epoch: 0, iter: 87100, loss: 30.856409072875977
epoch: 0, iter: 87200, loss: 30.930587768554688
epoch: 0, iter: 87300, loss: 30.785308837890625
epoch: 0, iter: 87400, loss: 30.77594757080078
epoch: 0, iter: 87500, loss: 30.602954864501953
epoch: 0, iter: 87600, loss: 31.219999313354492
epoch: 0, iter: 87700, loss: 30.640804290771484
epoch: 0, iter: 87800, loss: 31.12940788269043
epoch: 0, iter: 87900, loss: 30.826904296875
epoch: 0, iter: 88000, loss: 30.990097045898438
epoch: 0, iteration: 88000, simlex-999: SpearmanrResult(correlation=0.12147024702674274, pvalue=0.0001653680990202469), men: SpearmanrResult(correlation=0.12604951290417668, pvalue=1.2045725972710165e-10), sim353: SpearmanrResult(correlation=0.1693454875469321, pvalue=0.002446480354677961), nearest to monster: [&apos;monster&apos;, &apos;blade&apos;, &apos;leg&apos;, &apos;angel&apos;, &apos;bird&apos;, &apos;boat&apos;, &apos;signature&apos;, &apos;legendary&apos;, &apos;mirror&apos;, &apos;owner&apos;]

epoch: 0, iter: 88100, loss: 30.87779426574707
epoch: 0, iter: 88200, loss: 30.53211212158203
epoch: 0, iter: 88300, loss: 30.86421012878418
epoch: 0, iter: 88400, loss: 30.66036605834961
epoch: 0, iter: 88500, loss: 30.340608596801758
epoch: 0, iter: 88600, loss: 30.53420639038086
epoch: 0, iter: 88700, loss: 31.032270431518555
epoch: 0, iter: 88800, loss: 30.652175903320312
epoch: 0, iter: 88900, loss: 31.2420654296875
epoch: 0, iter: 89000, loss: 31.169876098632812
epoch: 0, iter: 89100, loss: 30.760807037353516
epoch: 0, iter: 89200, loss: 31.122560501098633
epoch: 0, iter: 89300, loss: 30.895538330078125
epoch: 0, iter: 89400, loss: 30.56373405456543
epoch: 0, iter: 89500, loss: 30.996185302734375
epoch: 0, iter: 89600, loss: 30.380939483642578
epoch: 0, iter: 89700, loss: 31.11984634399414
epoch: 0, iter: 89800, loss: 30.738248825073242
epoch: 0, iter: 89900, loss: 30.822444915771484
epoch: 0, iter: 90000, loss: 31.190614700317383
epoch: 0, iteration: 90000, simlex-999: SpearmanrResult(correlation=0.12146351761533054, pvalue=0.00016550735138900002), men: SpearmanrResult(correlation=0.12797080964385293, pvalue=6.246963375652522e-11), sim353: SpearmanrResult(correlation=0.1730852603381537, pvalue=0.0019495550082259915), nearest to monster: [&apos;monster&apos;, &apos;blade&apos;, &apos;leg&apos;, &apos;angel&apos;, &apos;signature&apos;, &apos;bird&apos;, &apos;boat&apos;, &apos;tail&apos;, &apos;legendary&apos;, &apos;mirror&apos;]

epoch: 0, iter: 90100, loss: 31.01602554321289
epoch: 0, iter: 90200, loss: 30.99297523498535
epoch: 0, iter: 90300, loss: 31.247032165527344
epoch: 0, iter: 90400, loss: 31.122554779052734
epoch: 0, iter: 90500, loss: 30.871871948242188
epoch: 0, iter: 90600, loss: 30.537988662719727
epoch: 0, iter: 90700, loss: 30.66657066345215
epoch: 0, iter: 90800, loss: 30.967605590820312
epoch: 0, iter: 90900, loss: 30.71727180480957
epoch: 0, iter: 91000, loss: 30.835491180419922
epoch: 0, iter: 91100, loss: 30.330137252807617
epoch: 0, iter: 91200, loss: 30.791658401489258
epoch: 0, iter: 91300, loss: 31.337520599365234
epoch: 0, iter: 91400, loss: 30.702518463134766
epoch: 0, iter: 91500, loss: 30.312820434570312
epoch: 0, iter: 91600, loss: 30.737586975097656
epoch: 0, iter: 91700, loss: 30.993764877319336
epoch: 0, iter: 91800, loss: 30.754323959350586
epoch: 0, iter: 91900, loss: 30.35256004333496
epoch: 0, iter: 92000, loss: 31.06475257873535
epoch: 0, iteration: 92000, simlex-999: SpearmanrResult(correlation=0.12049492050252572, pvalue=0.00018674058599766732), men: SpearmanrResult(correlation=0.1287830873875849, pvalue=4.7187134855481034e-11), sim353: SpearmanrResult(correlation=0.17778969038281117, pvalue=0.0014557762316129456), nearest to monster: [&apos;monster&apos;, &apos;leg&apos;, &apos;blade&apos;, &apos;angel&apos;, &apos;mirror&apos;, &apos;signature&apos;, &apos;tail&apos;, &apos;bird&apos;, &apos;boat&apos;, &apos;legendary&apos;]

epoch: 0, iter: 92100, loss: 30.96708869934082
epoch: 0, iter: 92200, loss: 30.94014549255371
epoch: 0, iter: 92300, loss: 30.560161590576172
epoch: 0, iter: 92400, loss: 30.880233764648438
epoch: 0, iter: 92500, loss: 30.613208770751953
epoch: 0, iter: 92600, loss: 30.25086212158203
epoch: 0, iter: 92700, loss: 30.913589477539062
epoch: 0, iter: 92800, loss: 30.727537155151367
epoch: 0, iter: 92900, loss: 30.70943832397461
epoch: 0, iter: 93000, loss: 30.733348846435547
epoch: 0, iter: 93100, loss: 31.102148056030273
epoch: 0, iter: 93200, loss: 31.044496536254883
epoch: 0, iter: 93300, loss: 30.72943115234375
epoch: 0, iter: 93400, loss: 30.99721336364746
epoch: 0, iter: 93500, loss: 30.689409255981445
epoch: 0, iter: 93600, loss: 31.005870819091797
epoch: 0, iter: 93700, loss: 30.852521896362305
epoch: 0, iter: 93800, loss: 31.096954345703125
epoch: 0, iter: 93900, loss: 30.707332611083984
epoch: 0, iter: 94000, loss: 31.069786071777344
epoch: 0, iteration: 94000, simlex-999: SpearmanrResult(correlation=0.12324490593078305, pvalue=0.0001322502794958162), men: SpearmanrResult(correlation=0.13004699883992418, pvalue=3.038858436063571e-11), sim353: SpearmanrResult(correlation=0.17592250950560948, pvalue=0.0016361090865993084), nearest to monster: [&apos;monster&apos;, &apos;blade&apos;, &apos;leg&apos;, &apos;angel&apos;, &apos;mirror&apos;, &apos;tail&apos;, &apos;signature&apos;, &apos;bird&apos;, &apos;boat&apos;, &apos;legendary&apos;]

epoch: 0, iter: 94100, loss: 31.337974548339844
epoch: 0, iter: 94200, loss: 30.744091033935547
epoch: 0, iter: 94300, loss: 30.94021987915039
epoch: 0, iter: 94400, loss: 30.518966674804688
epoch: 0, iter: 94500, loss: 30.852733612060547
epoch: 0, iter: 94600, loss: 30.059051513671875
epoch: 0, iter: 94700, loss: 31.083663940429688
epoch: 0, iter: 94800, loss: 30.699934005737305
epoch: 0, iter: 94900, loss: 30.6217041015625
epoch: 0, iter: 95000, loss: 30.698474884033203
epoch: 0, iter: 95100, loss: 30.759702682495117
epoch: 0, iter: 95200, loss: 31.207561492919922
epoch: 0, iter: 95300, loss: 30.635595321655273
epoch: 0, iter: 95400, loss: 30.824134826660156
epoch: 0, iter: 95500, loss: 30.59402847290039
epoch: 0, iter: 95600, loss: 29.989118576049805
epoch: 0, iter: 95700, loss: 31.089012145996094
epoch: 0, iter: 95800, loss: 30.574230194091797
epoch: 0, iter: 95900, loss: 31.234333038330078
epoch: 0, iter: 96000, loss: 30.883787155151367
epoch: 0, iteration: 96000, simlex-999: SpearmanrResult(correlation=0.12461901598392905, pvalue=0.00011100775081614734), men: SpearmanrResult(correlation=0.13040298596867406, pvalue=2.6825511352924495e-11), sim353: SpearmanrResult(correlation=0.17773632636453598, pvalue=0.0014606662591213036), nearest to monster: [&apos;monster&apos;, &apos;blade&apos;, &apos;angel&apos;, &apos;leg&apos;, &apos;mirror&apos;, &apos;bird&apos;, &apos;boat&apos;, &apos;tail&apos;, &apos;signature&apos;, &apos;harp&apos;]

epoch: 0, iter: 96100, loss: 30.677898406982422
epoch: 0, iter: 96200, loss: 30.666500091552734
epoch: 0, iter: 96300, loss: 30.890090942382812
epoch: 0, iter: 96400, loss: 30.862751007080078
epoch: 0, iter: 96500, loss: 30.74241828918457
epoch: 0, iter: 96600, loss: 31.067392349243164
epoch: 0, iter: 96700, loss: 30.09185791015625
epoch: 0, iter: 96800, loss: 30.830251693725586
epoch: 0, iter: 96900, loss: 30.507057189941406
epoch: 0, iter: 97000, loss: 30.755821228027344
epoch: 0, iter: 97100, loss: 30.22985076904297
epoch: 0, iter: 97200, loss: 30.947574615478516
epoch: 0, iter: 97300, loss: 30.583507537841797
epoch: 0, iter: 97400, loss: 30.67584991455078
epoch: 0, iter: 97500, loss: 31.08060073852539
epoch: 0, iter: 97600, loss: 30.564102172851562
epoch: 0, iter: 97700, loss: 30.59963607788086
epoch: 0, iter: 97800, loss: 31.315624237060547
epoch: 0, iter: 97900, loss: 31.017738342285156
epoch: 0, iter: 98000, loss: 30.729049682617188
epoch: 0, iteration: 98000, simlex-999: SpearmanrResult(correlation=0.1246043454563031, pvalue=0.00011121651888022881), men: SpearmanrResult(correlation=0.13216585436099, pvalue=1.4393399261301587e-11), sim353: SpearmanrResult(correlation=0.17839479356905732, pvalue=0.001401368292639592), nearest to monster: [&apos;monster&apos;, &apos;blade&apos;, &apos;angel&apos;, &apos;leg&apos;, &apos;mirror&apos;, &apos;shield&apos;, &apos;bird&apos;, &apos;tail&apos;, &apos;boat&apos;, &apos;signature&apos;]

epoch: 0, iter: 98100, loss: 31.072677612304688
epoch: 0, iter: 98200, loss: 30.446178436279297
epoch: 0, iter: 98300, loss: 30.861177444458008
epoch: 0, iter: 98400, loss: 31.360382080078125
epoch: 0, iter: 98500, loss: 31.02007293701172
epoch: 0, iter: 98600, loss: 30.365102767944336
epoch: 0, iter: 98700, loss: 30.617948532104492
epoch: 0, iter: 98800, loss: 30.435665130615234
epoch: 0, iter: 98900, loss: 30.99168586730957
epoch: 0, iter: 99000, loss: 30.720821380615234
epoch: 0, iter: 99100, loss: 30.526050567626953
epoch: 0, iter: 99200, loss: 30.532978057861328
epoch: 0, iter: 99300, loss: 30.427440643310547
epoch: 0, iter: 99400, loss: 30.835657119750977
epoch: 0, iter: 99500, loss: 30.73276138305664
epoch: 0, iter: 99600, loss: 30.321819305419922
epoch: 0, iter: 99700, loss: 30.551624298095703
epoch: 0, iter: 99800, loss: 30.387161254882812
epoch: 0, iter: 99900, loss: 30.957223892211914
epoch: 0, iter: 100000, loss: 30.369234085083008
epoch: 0, iteration: 100000, simlex-999: SpearmanrResult(correlation=0.12513772832585443, pvalue=0.00010385909688140838), men: SpearmanrResult(correlation=0.13298891292774756, pvalue=1.0732164702661443e-11), sim353: SpearmanrResult(correlation=0.18029779924174447, pvalue=0.0012421912502034132), nearest to monster: [&apos;monster&apos;, &apos;blade&apos;, &apos;angel&apos;, &apos;leg&apos;, &apos;bird&apos;, &apos;boat&apos;, &apos;mirror&apos;, &apos;wheel&apos;, &apos;shield&apos;, &apos;tail&apos;]

epoch: 0, iter: 100100, loss: 30.41366958618164
epoch: 0, iter: 100200, loss: 30.84021759033203
epoch: 0, iter: 100300, loss: 30.72504234313965
epoch: 0, iter: 100400, loss: 30.86883544921875
epoch: 0, iter: 100500, loss: 30.47562026977539
epoch: 0, iter: 100600, loss: 30.675384521484375
epoch: 0, iter: 100700, loss: 30.502666473388672
epoch: 0, iter: 100800, loss: 30.754810333251953
epoch: 0, iter: 100900, loss: 30.797780990600586
epoch: 0, iter: 101000, loss: 30.811765670776367
epoch: 0, iter: 101100, loss: 30.670616149902344
epoch: 0, iter: 101200, loss: 30.43963050842285
epoch: 0, iter: 101300, loss: 30.744415283203125
epoch: 0, iter: 101400, loss: 30.76150894165039
epoch: 0, iter: 101500, loss: 30.75295066833496
epoch: 0, iter: 101600, loss: 30.873519897460938
epoch: 0, iter: 101700, loss: 30.82119369506836
epoch: 0, iter: 101800, loss: 30.61273193359375
epoch: 0, iter: 101900, loss: 30.886390686035156
epoch: 0, iter: 102000, loss: 30.50117301940918
epoch: 0, iteration: 102000, simlex-999: SpearmanrResult(correlation=0.12658345688637404, pvalue=8.615546339741256e-05), men: SpearmanrResult(correlation=0.13482600193860372, pvalue=5.5374783924142395e-12), sim353: SpearmanrResult(correlation=0.18156136963251196, pvalue=0.0011458677738041929), nearest to monster: [&apos;monster&apos;, &apos;blade&apos;, &apos;angel&apos;, &apos;leg&apos;, &apos;boat&apos;, &apos;shield&apos;, &apos;mirror&apos;, &apos;bird&apos;, &apos;wheel&apos;, &apos;harp&apos;]

epoch: 0, iter: 102100, loss: 31.035795211791992
epoch: 0, iter: 102200, loss: 30.630266189575195
epoch: 0, iter: 102300, loss: 30.40416717529297
epoch: 0, iter: 102400, loss: 30.68378448486328
epoch: 0, iter: 102500, loss: 30.96162986755371
epoch: 0, iter: 102600, loss: 30.965835571289062
epoch: 0, iter: 102700, loss: 30.67343521118164
epoch: 0, iter: 102800, loss: 30.438232421875
epoch: 0, iter: 102900, loss: 30.58956527709961
epoch: 0, iter: 103000, loss: 30.720691680908203
epoch: 0, iter: 103100, loss: 31.140766143798828
epoch: 0, iter: 103200, loss: 30.85077667236328
epoch: 0, iter: 103300, loss: 30.857940673828125
epoch: 0, iter: 103400, loss: 30.87981414794922
epoch: 0, iter: 103500, loss: 30.996490478515625
epoch: 0, iter: 103600, loss: 30.80972671508789
epoch: 0, iter: 103700, loss: 30.426483154296875
epoch: 0, iter: 103800, loss: 30.21402931213379
epoch: 0, iter: 103900, loss: 30.624500274658203
epoch: 0, iter: 104000, loss: 30.79989242553711
epoch: 0, iteration: 104000, simlex-999: SpearmanrResult(correlation=0.12685808215829938, pvalue=8.313188130474721e-05), men: SpearmanrResult(correlation=0.13579942549332547, pvalue=3.885480680746222e-12), sim353: SpearmanrResult(correlation=0.18599692712355217, pvalue=0.0008595570664566224), nearest to monster: [&apos;monster&apos;, &apos;blade&apos;, &apos;angel&apos;, &apos;leg&apos;, &apos;camera&apos;, &apos;shield&apos;, &apos;boat&apos;, &apos;mirror&apos;, &apos;harp&apos;, &apos;elephant&apos;]

epoch: 0, iter: 104100, loss: 30.856924057006836
epoch: 0, iter: 104200, loss: 30.86968994140625
epoch: 0, iter: 104300, loss: 31.017091751098633
epoch: 0, iter: 104400, loss: 31.06365394592285
epoch: 0, iter: 104500, loss: 31.071155548095703
epoch: 0, iter: 104600, loss: 31.20572280883789
epoch: 0, iter: 104700, loss: 30.488605499267578
epoch: 0, iter: 104800, loss: 30.928695678710938
epoch: 0, iter: 104900, loss: 30.158926010131836
epoch: 0, iter: 105000, loss: 30.583736419677734
epoch: 0, iter: 105100, loss: 30.513572692871094
epoch: 0, iter: 105200, loss: 30.47650718688965
epoch: 0, iter: 105300, loss: 30.839195251464844
epoch: 0, iter: 105400, loss: 30.879192352294922
epoch: 0, iter: 105500, loss: 30.75198745727539
epoch: 0, iter: 105600, loss: 30.641521453857422
epoch: 0, iter: 105700, loss: 30.412517547607422
epoch: 0, iter: 105800, loss: 30.99948501586914
epoch: 0, iter: 105900, loss: 30.492664337158203
epoch: 0, iter: 106000, loss: 30.717350006103516
epoch: 0, iteration: 106000, simlex-999: SpearmanrResult(correlation=0.12920315538833363, pvalue=6.109467641248834e-05), men: SpearmanrResult(correlation=0.13611945131945707, pvalue=3.4563464313462454e-12), sim353: SpearmanrResult(correlation=0.18889686968310743, pvalue=0.0007097623503415265), nearest to monster: [&apos;monster&apos;, &apos;blade&apos;, &apos;leg&apos;, &apos;angel&apos;, &apos;shield&apos;, &apos;camera&apos;, &apos;mirror&apos;, &apos;elephant&apos;, &apos;tube&apos;, &apos;boat&apos;]

epoch: 0, iter: 106100, loss: 31.023670196533203
epoch: 0, iter: 106200, loss: 30.297914505004883
epoch: 0, iter: 106300, loss: 30.34014320373535
epoch: 0, iter: 106400, loss: 30.92735481262207
epoch: 0, iter: 106500, loss: 31.24557113647461
epoch: 0, iter: 106600, loss: 30.648012161254883
epoch: 0, iter: 106700, loss: 30.329431533813477
epoch: 0, iter: 106800, loss: 30.593647003173828
epoch: 0, iter: 106900, loss: 30.407344818115234
epoch: 0, iter: 107000, loss: 30.82100486755371
epoch: 0, iter: 107100, loss: 30.74463653564453
epoch: 0, iter: 107200, loss: 30.87047576904297
epoch: 0, iter: 107300, loss: 30.84787940979004
epoch: 0, iter: 107400, loss: 30.763612747192383
epoch: 0, iter: 107500, loss: 30.8703670501709
epoch: 0, iter: 107600, loss: 30.84850311279297
epoch: 0, iter: 107700, loss: 30.902305603027344
epoch: 0, iter: 107800, loss: 30.58749771118164
epoch: 0, iter: 107900, loss: 30.673856735229492
epoch: 0, iter: 108000, loss: 30.83418846130371
epoch: 0, iteration: 108000, simlex-999: SpearmanrResult(correlation=0.13051674310394015, pvalue=5.129496444866806e-05), men: SpearmanrResult(correlation=0.1375467936587797, pvalue=2.0439266128547674e-12), sim353: SpearmanrResult(correlation=0.19379982880274665, pvalue=0.0005102019337649919), nearest to monster: [&apos;monster&apos;, &apos;blade&apos;, &apos;leg&apos;, &apos;camera&apos;, &apos;shield&apos;, &apos;angel&apos;, &apos;elephant&apos;, &apos;mirror&apos;, &apos;ghost&apos;, &apos;boat&apos;]

epoch: 0, iter: 108100, loss: 31.052444458007812
epoch: 0, iter: 108200, loss: 30.354251861572266
epoch: 0, iter: 108300, loss: 31.330671310424805
epoch: 0, iter: 108400, loss: 30.84612274169922
epoch: 0, iter: 108500, loss: 30.641740798950195
epoch: 0, iter: 108600, loss: 30.20119857788086
epoch: 0, iter: 108700, loss: 30.793170928955078
epoch: 0, iter: 108800, loss: 30.220489501953125
epoch: 0, iter: 108900, loss: 30.999284744262695
epoch: 0, iter: 109000, loss: 31.053329467773438
epoch: 0, iter: 109100, loss: 30.955081939697266
epoch: 0, iter: 109200, loss: 30.715665817260742
epoch: 0, iter: 109300, loss: 30.646869659423828
epoch: 0, iter: 109400, loss: 30.617048263549805
epoch: 0, iter: 109500, loss: 31.204490661621094
epoch: 0, iter: 109600, loss: 30.811479568481445
epoch: 0, iter: 109700, loss: 30.87088394165039
epoch: 0, iter: 109800, loss: 30.969287872314453
epoch: 0, iter: 109900, loss: 30.64400291442871
epoch: 0, iter: 110000, loss: 30.75538444519043
epoch: 0, iteration: 110000, simlex-999: SpearmanrResult(correlation=0.13088839031890218, pvalue=4.880473123942339e-05), men: SpearmanrResult(correlation=0.13896681910256206, pvalue=1.2053636316763994e-12), sim353: SpearmanrResult(correlation=0.20021881116883977, pvalue=0.0003271445558931211), nearest to monster: [&apos;monster&apos;, &apos;blade&apos;, &apos;camera&apos;, &apos;leg&apos;, &apos;shield&apos;, &apos;elephant&apos;, &apos;tube&apos;, &apos;mirror&apos;, &apos;angel&apos;, &apos;harp&apos;]

epoch: 0, iter: 110100, loss: 30.84221076965332
epoch: 0, iter: 110200, loss: 30.212867736816406
epoch: 0, iter: 110300, loss: 30.807170867919922
epoch: 0, iter: 110400, loss: 30.793251037597656
epoch: 0, iter: 110500, loss: 30.331619262695312
epoch: 0, iter: 110600, loss: 31.10693359375
epoch: 0, iter: 110700, loss: 30.852031707763672
epoch: 0, iter: 110800, loss: 30.546293258666992
epoch: 0, iter: 110900, loss: 30.63283920288086
epoch: 0, iter: 111000, loss: 30.74249839782715
epoch: 0, iter: 111100, loss: 30.533628463745117
epoch: 0, iter: 111200, loss: 30.584836959838867
epoch: 0, iter: 111300, loss: 31.051198959350586
epoch: 0, iter: 111400, loss: 30.349002838134766
epoch: 0, iter: 111500, loss: 31.08504295349121
epoch: 0, iter: 111600, loss: 30.422914505004883
epoch: 0, iter: 111700, loss: 30.63071632385254
epoch: 0, iter: 111800, loss: 30.566604614257812
epoch: 0, iter: 111900, loss: 30.24704360961914
epoch: 0, iter: 112000, loss: 30.796680450439453
epoch: 0, iteration: 112000, simlex-999: SpearmanrResult(correlation=0.13249184544404854, pvalue=3.931449417827171e-05), men: SpearmanrResult(correlation=0.13926854062627636, pvalue=1.0766712070412453e-12), sim353: SpearmanrResult(correlation=0.20095993662544465, pvalue=0.00031050339150380995), nearest to monster: [&apos;monster&apos;, &apos;blade&apos;, &apos;camera&apos;, &apos;tube&apos;, &apos;leg&apos;, &apos;shield&apos;, &apos;boat&apos;, &apos;belt&apos;, &apos;elephant&apos;, &apos;mirror&apos;]

epoch: 0, iter: 112100, loss: 30.549213409423828
epoch: 0, iter: 112200, loss: 30.482145309448242
epoch: 0, iter: 112300, loss: 30.55345344543457
epoch: 0, iter: 112400, loss: 30.55514907836914
epoch: 0, iter: 112500, loss: 31.361774444580078
epoch: 0, iter: 112600, loss: 30.88861083984375
epoch: 0, iter: 112700, loss: 30.742958068847656
epoch: 0, iter: 112800, loss: 30.31718635559082
epoch: 0, iter: 112900, loss: 30.65258026123047
epoch: 0, iter: 113000, loss: 30.942604064941406
epoch: 0, iter: 113100, loss: 30.787303924560547
epoch: 0, iter: 113200, loss: 30.986019134521484
epoch: 0, iter: 113300, loss: 30.715797424316406
epoch: 0, iter: 113400, loss: 31.07750701904297
epoch: 0, iter: 113500, loss: 30.899442672729492
epoch: 0, iter: 113600, loss: 30.954410552978516
epoch: 0, iter: 113700, loss: 30.243022918701172
epoch: 0, iter: 113800, loss: 30.855615615844727
epoch: 0, iter: 113900, loss: 31.091819763183594
epoch: 0, iter: 114000, loss: 30.61470603942871
epoch: 0, iteration: 114000, simlex-999: SpearmanrResult(correlation=0.1336421747742616, pvalue=3.3613887168809165e-05), men: SpearmanrResult(correlation=0.14026092296561493, pvalue=7.413973866366574e-13), sim353: SpearmanrResult(correlation=0.1998161553945816, pvalue=0.00033653037818287643), nearest to monster: [&apos;monster&apos;, &apos;blade&apos;, &apos;camera&apos;, &apos;tube&apos;, &apos;shield&apos;, &apos;belt&apos;, &apos;harp&apos;, &apos;leg&apos;, &apos;robot&apos;, &apos;elephant&apos;]

epoch: 0, iter: 114100, loss: 31.07042121887207
epoch: 0, iter: 114200, loss: 30.676921844482422
epoch: 0, iter: 114300, loss: 30.714401245117188
epoch: 0, iter: 114400, loss: 30.74327850341797
epoch: 0, iter: 114500, loss: 30.866933822631836
epoch: 0, iter: 114600, loss: 30.79370880126953
epoch: 0, iter: 114700, loss: 30.732254028320312
epoch: 0, iter: 114800, loss: 30.914697647094727
epoch: 0, iter: 114900, loss: 29.94063949584961
epoch: 0, iter: 115000, loss: 31.382705688476562
epoch: 0, iter: 115100, loss: 30.673114776611328
epoch: 0, iter: 115200, loss: 30.157482147216797
epoch: 0, iter: 115300, loss: 30.431161880493164
epoch: 0, iter: 115400, loss: 30.79012680053711
epoch: 0, iter: 115500, loss: 30.708446502685547
epoch: 0, iter: 115600, loss: 30.697505950927734
epoch: 0, iter: 115700, loss: 30.43924331665039
epoch: 0, iter: 115800, loss: 30.20172119140625
epoch: 0, iter: 115900, loss: 30.414777755737305
epoch: 0, iter: 116000, loss: 30.588607788085938
epoch: 0, iteration: 116000, simlex-999: SpearmanrResult(correlation=0.13466175544946068, pvalue=2.922495994353612e-05), men: SpearmanrResult(correlation=0.1408288689686598, pvalue=5.981353363565132e-13), sim353: SpearmanrResult(correlation=0.20141259784340124, pvalue=0.0003007309889417829), nearest to monster: [&apos;monster&apos;, &apos;blade&apos;, &apos;camera&apos;, &apos;tube&apos;, &apos;harp&apos;, &apos;shield&apos;, &apos;belt&apos;, &apos;wheel&apos;, &apos;elephant&apos;, &apos;robot&apos;]

epoch: 0, iter: 116100, loss: 31.112529754638672
epoch: 0, iter: 116200, loss: 30.93574333190918
epoch: 0, iter: 116300, loss: 30.862598419189453
epoch: 0, iter: 116400, loss: 30.959362030029297
epoch: 0, iter: 116500, loss: 30.90961456298828
epoch: 0, iter: 116600, loss: 30.95020294189453
epoch: 0, iter: 116700, loss: 30.564823150634766
epoch: 0, iter: 116800, loss: 31.208728790283203
epoch: 0, iter: 116900, loss: 30.43271255493164
epoch: 0, iter: 117000, loss: 30.27477264404297
epoch: 0, iter: 117100, loss: 31.123092651367188
epoch: 0, iter: 117200, loss: 30.779741287231445
epoch: 0, iter: 117300, loss: 30.509449005126953
epoch: 0, iter: 117400, loss: 30.905853271484375
epoch: 0, iter: 117500, loss: 30.46228790283203
epoch: 0, iter: 117600, loss: 30.66716766357422
epoch: 0, iter: 117700, loss: 30.286834716796875
epoch: 0, iter: 117800, loss: 30.221879959106445
epoch: 0, iter: 117900, loss: 30.67200469970703
epoch: 0, iter: 118000, loss: 30.90604019165039
epoch: 0, iteration: 118000, simlex-999: SpearmanrResult(correlation=0.13486929926002347, pvalue=2.8400848845646733e-05), men: SpearmanrResult(correlation=0.14109607175728955, pvalue=5.405006472637779e-13), sim353: SpearmanrResult(correlation=0.20408826225621957, pvalue=0.00024858062563288326), nearest to monster: [&apos;monster&apos;, &apos;blade&apos;, &apos;camera&apos;, &apos;tube&apos;, &apos;harp&apos;, &apos;module&apos;, &apos;robot&apos;, &apos;elephant&apos;, &apos;mirror&apos;, &apos;wheel&apos;]

epoch: 0, iter: 118100, loss: 31.184240341186523
epoch: 0, iter: 118200, loss: 29.67246437072754
epoch: 0, iter: 118300, loss: 30.052947998046875
epoch: 0, iter: 118400, loss: 30.136735916137695
epoch: 0, iter: 118500, loss: 31.110488891601562
epoch: 0, iter: 118600, loss: 30.923221588134766
epoch: 0, iter: 118700, loss: 30.17416763305664
epoch: 0, iter: 118800, loss: 30.757675170898438
epoch: 0, iter: 118900, loss: 30.749191284179688
epoch: 0, iter: 119000, loss: 30.639251708984375
epoch: 0, iter: 119100, loss: 30.50901985168457
epoch: 0, iter: 119200, loss: 30.917224884033203
epoch: 0, iter: 119300, loss: 30.909788131713867
epoch: 0, iter: 119400, loss: 30.55084228515625
epoch: 0, iter: 119500, loss: 30.522607803344727
epoch: 1, iter: 0, loss: 30.452777862548828
epoch: 1, iteration: 0, simlex-999: SpearmanrResult(correlation=0.13584630300275116, pvalue=2.480900559909309e-05), men: SpearmanrResult(correlation=0.14149671571670952, pvalue=4.641524444417543e-13), sim353: SpearmanrResult(correlation=0.20420525260397665, pvalue=0.00024650552323513203), nearest to monster: [&apos;monster&apos;, &apos;blade&apos;, &apos;camera&apos;, &apos;robot&apos;, &apos;leg&apos;, &apos;module&apos;, &apos;harp&apos;, &apos;mirror&apos;, &apos;boat&apos;, &apos;elephant&apos;]

epoch: 1, iter: 100, loss: 30.471439361572266
epoch: 1, iter: 200, loss: 30.330595016479492
epoch: 1, iter: 300, loss: 30.57529067993164
epoch: 1, iter: 400, loss: 30.718156814575195
epoch: 1, iter: 500, loss: 30.709121704101562
epoch: 1, iter: 600, loss: 30.22405242919922
epoch: 1, iter: 700, loss: 31.00029945373535
epoch: 1, iter: 800, loss: 30.500652313232422
epoch: 1, iter: 900, loss: 30.64475440979004
epoch: 1, iter: 1000, loss: 30.245718002319336
epoch: 1, iter: 1100, loss: 30.46042251586914
epoch: 1, iter: 1200, loss: 30.88376235961914
epoch: 1, iter: 1300, loss: 30.545751571655273
epoch: 1, iter: 1400, loss: 30.541282653808594
epoch: 1, iter: 1500, loss: 30.788883209228516
epoch: 1, iter: 1600, loss: 30.412235260009766
epoch: 1, iter: 1700, loss: 30.570415496826172
epoch: 1, iter: 1800, loss: 30.742263793945312
epoch: 1, iter: 1900, loss: 30.20556640625
epoch: 1, iter: 2000, loss: 30.579498291015625
epoch: 1, iteration: 2000, simlex-999: SpearmanrResult(correlation=0.13750886561871162, pvalue=1.9667970854520583e-05), men: SpearmanrResult(correlation=0.14216903853907206, pvalue=3.5913225784003253e-13), sim353: SpearmanrResult(correlation=0.20737145549247832, pvalue=0.00019612168069552233), nearest to monster: [&apos;monster&apos;, &apos;blade&apos;, &apos;camera&apos;, &apos;module&apos;, &apos;robot&apos;, &apos;boat&apos;, &apos;leg&apos;, &apos;elephant&apos;, &apos;harp&apos;, &apos;pen&apos;]

epoch: 1, iter: 2100, loss: 31.068511962890625
epoch: 1, iter: 2200, loss: 30.329666137695312
epoch: 1, iter: 2300, loss: 30.718788146972656
epoch: 1, iter: 2400, loss: 30.20919418334961
epoch: 1, iter: 2500, loss: 30.841068267822266
epoch: 1, iter: 2600, loss: 30.234155654907227
epoch: 1, iter: 2700, loss: 30.538684844970703
epoch: 1, iter: 2800, loss: 30.410411834716797
epoch: 1, iter: 2900, loss: 30.57469940185547
epoch: 1, iter: 3000, loss: 30.982160568237305
epoch: 1, iter: 3100, loss: 30.552490234375
epoch: 1, iter: 3200, loss: 30.447053909301758
epoch: 1, iter: 3300, loss: 30.97784996032715
epoch: 1, iter: 3400, loss: 30.28424072265625
epoch: 1, iter: 3500, loss: 30.430091857910156
epoch: 1, iter: 3600, loss: 30.772613525390625
epoch: 1, iter: 3700, loss: 30.817935943603516
epoch: 1, iter: 3800, loss: 31.377342224121094
epoch: 1, iter: 3900, loss: 30.153400421142578
epoch: 1, iter: 4000, loss: 30.621929168701172
epoch: 1, iteration: 4000, simlex-999: SpearmanrResult(correlation=0.13830312006033485, pvalue=1.7586105251314907e-05), men: SpearmanrResult(correlation=0.14305092230879102, pvalue=2.5604571360787646e-13), sim353: SpearmanrResult(correlation=0.21378372070941798, pvalue=0.0001221329627483309), nearest to monster: [&apos;monster&apos;, &apos;blade&apos;, &apos;camera&apos;, &apos;leg&apos;, &apos;robot&apos;, &apos;boat&apos;, &apos;mirror&apos;, &apos;module&apos;, &apos;harp&apos;, &apos;elephant&apos;]

epoch: 1, iter: 4100, loss: 30.806547164916992
epoch: 1, iter: 4200, loss: 30.223846435546875
epoch: 1, iter: 4300, loss: 30.79652214050293
epoch: 1, iter: 4400, loss: 30.826208114624023
epoch: 1, iter: 4500, loss: 30.39246940612793
epoch: 1, iter: 4600, loss: 30.316673278808594
epoch: 1, iter: 4700, loss: 30.22970962524414
epoch: 1, iter: 4800, loss: 30.382505416870117
epoch: 1, iter: 4900, loss: 30.906177520751953
epoch: 1, iter: 5000, loss: 30.31850814819336
epoch: 1, iter: 5100, loss: 30.42485809326172
epoch: 1, iter: 5200, loss: 30.402118682861328
epoch: 1, iter: 5300, loss: 30.640960693359375
epoch: 1, iter: 5400, loss: 30.709318161010742
epoch: 1, iter: 5500, loss: 30.756460189819336
epoch: 1, iter: 5600, loss: 30.85149574279785
epoch: 1, iter: 5700, loss: 30.148801803588867
epoch: 1, iter: 5800, loss: 30.126773834228516
epoch: 1, iter: 5900, loss: 29.931812286376953
epoch: 1, iter: 6000, loss: 30.726451873779297
epoch: 1, iteration: 6000, simlex-999: SpearmanrResult(correlation=0.13881144733907377, pvalue=1.6365610755549817e-05), men: SpearmanrResult(correlation=0.14301940225728388, pvalue=2.5917019394881986e-13), sim353: SpearmanrResult(correlation=0.21994801117010684, pvalue=7.642841580302375e-05), nearest to monster: [&apos;monster&apos;, &apos;blade&apos;, &apos;camera&apos;, &apos;robot&apos;, &apos;harp&apos;, &apos;bird&apos;, &apos;boat&apos;, &apos;leg&apos;, &apos;module&apos;, &apos;elephant&apos;]

epoch: 1, iter: 6100, loss: 30.620433807373047
epoch: 1, iter: 6200, loss: 30.196815490722656
epoch: 1, iter: 6300, loss: 30.643386840820312
epoch: 1, iter: 6400, loss: 30.523494720458984
epoch: 1, iter: 6500, loss: 30.796640396118164
epoch: 1, iter: 6600, loss: 31.071800231933594
epoch: 1, iter: 6700, loss: 30.77219009399414
epoch: 1, iter: 6800, loss: 30.722896575927734
epoch: 1, iter: 6900, loss: 30.279769897460938
epoch: 1, iter: 7000, loss: 30.445314407348633
epoch: 1, iter: 7100, loss: 30.531850814819336
epoch: 1, iter: 7200, loss: 30.295429229736328
epoch: 1, iter: 7300, loss: 30.24776840209961
epoch: 1, iter: 7400, loss: 30.302000045776367
epoch: 1, iter: 7500, loss: 30.31418800354004
epoch: 1, iter: 7600, loss: 30.70012664794922
epoch: 1, iter: 7700, loss: 30.701053619384766
epoch: 1, iter: 7800, loss: 30.10363006591797
epoch: 1, iter: 7900, loss: 30.491683959960938
epoch: 1, iter: 8000, loss: 30.433917999267578
epoch: 1, iteration: 8000, simlex-999: SpearmanrResult(correlation=0.13980285510214574, pvalue=1.4213297877880229e-05), men: SpearmanrResult(correlation=0.143487017095893, pvalue=2.1643054711694094e-13), sim353: SpearmanrResult(correlation=0.22035570480623015, pvalue=7.40606141206957e-05), nearest to monster: [&apos;monster&apos;, &apos;blade&apos;, &apos;robot&apos;, &apos;camera&apos;, &apos;bird&apos;, &apos;elephant&apos;, &apos;module&apos;, &apos;mine&apos;, &apos;harp&apos;, &apos;boat&apos;]

epoch: 1, iter: 8100, loss: 30.638187408447266
epoch: 1, iter: 8200, loss: 30.01629638671875
epoch: 1, iter: 8300, loss: 30.81504249572754
epoch: 1, iter: 8400, loss: 30.378347396850586
epoch: 1, iter: 8500, loss: 30.630828857421875
epoch: 1, iter: 8600, loss: 30.271350860595703
epoch: 1, iter: 8700, loss: 30.721881866455078
epoch: 1, iter: 8800, loss: 30.455726623535156
epoch: 1, iter: 8900, loss: 31.070642471313477
epoch: 1, iter: 9000, loss: 30.86322593688965
epoch: 1, iter: 9100, loss: 30.664705276489258
epoch: 1, iter: 9200, loss: 30.42993927001953
epoch: 1, iter: 9300, loss: 31.07258415222168
epoch: 1, iter: 9400, loss: 30.924095153808594
epoch: 1, iter: 9500, loss: 30.60555076599121
epoch: 1, iter: 9600, loss: 30.54239845275879
epoch: 1, iter: 9700, loss: 30.296348571777344
epoch: 1, iter: 9800, loss: 30.616439819335938
epoch: 1, iter: 9900, loss: 30.72933578491211
epoch: 1, iter: 10000, loss: 30.84246253967285
epoch: 1, iteration: 10000, simlex-999: SpearmanrResult(correlation=0.13918559761336866, pvalue=1.5519239995073766e-05), men: SpearmanrResult(correlation=0.14349097227167568, pvalue=2.1610034817411894e-13), sim353: SpearmanrResult(correlation=0.22154967806826809, pvalue=6.751724609450489e-05), nearest to monster: [&apos;monster&apos;, &apos;blade&apos;, &apos;bird&apos;, &apos;camera&apos;, &apos;robot&apos;, &apos;elephant&apos;, &apos;mine&apos;, &apos;module&apos;, &apos;leg&apos;, &apos;harp&apos;]

epoch: 1, iter: 10100, loss: 30.592771530151367
epoch: 1, iter: 10200, loss: 30.892162322998047
epoch: 1, iter: 10300, loss: 31.060081481933594
epoch: 1, iter: 10400, loss: 30.355026245117188
epoch: 1, iter: 10500, loss: 30.683738708496094
epoch: 1, iter: 10600, loss: 30.789752960205078
epoch: 1, iter: 10700, loss: 30.287927627563477
epoch: 1, iter: 10800, loss: 30.678464889526367
epoch: 1, iter: 10900, loss: 30.326526641845703
epoch: 1, iter: 11000, loss: 30.644237518310547
epoch: 1, iter: 11100, loss: 30.537033081054688
epoch: 1, iter: 11200, loss: 30.29986572265625
epoch: 1, iter: 11300, loss: 30.58269500732422
epoch: 1, iter: 11400, loss: 30.60637664794922
epoch: 1, iter: 11500, loss: 30.54550552368164
epoch: 1, iter: 11600, loss: 30.504114151000977
epoch: 1, iter: 11700, loss: 30.25569725036621
epoch: 1, iter: 11800, loss: 30.455440521240234
epoch: 1, iter: 11900, loss: 30.485092163085938
epoch: 1, iter: 12000, loss: 30.729618072509766
epoch: 1, iteration: 12000, simlex-999: SpearmanrResult(correlation=0.14006879986730683, pvalue=1.3683503449346063e-05), men: SpearmanrResult(correlation=0.14369428471253842, pvalue=1.9977679774445803e-13), sim353: SpearmanrResult(correlation=0.22291381910886188, pvalue=6.0708601908239216e-05), nearest to monster: [&apos;monster&apos;, &apos;blade&apos;, &apos;bird&apos;, &apos;robot&apos;, &apos;camera&apos;, &apos;boat&apos;, &apos;harp&apos;, &apos;mine&apos;, &apos;module&apos;, &apos;elephant&apos;]

epoch: 1, iter: 12100, loss: 31.003379821777344
epoch: 1, iter: 12200, loss: 30.895633697509766
epoch: 1, iter: 12300, loss: 30.73479461669922
epoch: 1, iter: 12400, loss: 30.40711212158203
epoch: 1, iter: 12500, loss: 30.21118927001953
epoch: 1, iter: 12600, loss: 30.281837463378906
epoch: 1, iter: 12700, loss: 30.417930603027344
epoch: 1, iter: 12800, loss: 29.946765899658203
epoch: 1, iter: 12900, loss: 30.75798797607422
epoch: 1, iter: 13000, loss: 31.17790985107422
epoch: 1, iter: 13100, loss: 30.745189666748047
epoch: 1, iter: 13200, loss: 30.8111572265625
epoch: 1, iter: 13300, loss: 30.84844207763672
epoch: 1, iter: 13400, loss: 30.430782318115234
epoch: 1, iter: 13500, loss: 30.365447998046875
epoch: 1, iter: 13600, loss: 30.273536682128906
epoch: 1, iter: 13700, loss: 30.858108520507812
epoch: 1, iter: 13800, loss: 30.77298927307129
epoch: 1, iter: 13900, loss: 31.031143188476562
epoch: 1, iter: 14000, loss: 30.615827560424805
epoch: 1, iteration: 14000, simlex-999: SpearmanrResult(correlation=0.1403276597185061, pvalue=1.3185922971315998e-05), men: SpearmanrResult(correlation=0.14529215462232734, pvalue=1.0734198813575383e-13), sim353: SpearmanrResult(correlation=0.22418410664878283, pvalue=5.495482632416603e-05), nearest to monster: [&apos;monster&apos;, &apos;blade&apos;, &apos;bird&apos;, &apos;boat&apos;, &apos;robot&apos;, &apos;mine&apos;, &apos;module&apos;, &apos;camera&apos;, &apos;giant&apos;, &apos;harp&apos;]

epoch: 1, iter: 14100, loss: 30.48816680908203
epoch: 1, iter: 14200, loss: 30.806354522705078
epoch: 1, iter: 14300, loss: 29.96129035949707
epoch: 1, iter: 14400, loss: 30.932781219482422
epoch: 1, iter: 14500, loss: 30.7196102142334
epoch: 1, iter: 14600, loss: 30.22078514099121
epoch: 1, iter: 14700, loss: 31.128929138183594
epoch: 1, iter: 14800, loss: 29.683853149414062
epoch: 1, iter: 14900, loss: 30.984813690185547
epoch: 1, iter: 15000, loss: 30.461803436279297
epoch: 1, iter: 15100, loss: 30.73785400390625
epoch: 1, iter: 15200, loss: 30.712100982666016
epoch: 1, iter: 15300, loss: 30.466516494750977
epoch: 1, iter: 15400, loss: 30.314067840576172
epoch: 1, iter: 15500, loss: 30.55801773071289
epoch: 1, iter: 15600, loss: 30.428577423095703
epoch: 1, iter: 15700, loss: 31.00663185119629
epoch: 1, iter: 15800, loss: 30.276063919067383
epoch: 1, iter: 15900, loss: 30.50764274597168
epoch: 1, iter: 16000, loss: 30.646820068359375
epoch: 1, iteration: 16000, simlex-999: SpearmanrResult(correlation=0.13965840845801117, pvalue=1.4509180896993527e-05), men: SpearmanrResult(correlation=0.14605203149208384, pvalue=7.969295632331052e-14), sim353: SpearmanrResult(correlation=0.22762533947762578, pvalue=4.1841877221568024e-05), nearest to monster: [&apos;monster&apos;, &apos;blade&apos;, &apos;robot&apos;, &apos;bird&apos;, &apos;boat&apos;, &apos;camera&apos;, &apos;mine&apos;, &apos;harp&apos;, &apos;pen&apos;, &apos;module&apos;]

epoch: 1, iter: 16100, loss: 30.518444061279297
epoch: 1, iter: 16200, loss: 30.669857025146484
epoch: 1, iter: 16300, loss: 30.55596351623535
epoch: 1, iter: 16400, loss: 30.038494110107422
epoch: 1, iter: 16500, loss: 30.565519332885742
epoch: 1, iter: 16600, loss: 30.724658966064453
epoch: 1, iter: 16700, loss: 30.56858253479004
epoch: 1, iter: 16800, loss: 30.65789031982422
epoch: 1, iter: 16900, loss: 30.921201705932617
epoch: 1, iter: 17000, loss: 30.809478759765625
epoch: 1, iter: 17100, loss: 30.42205238342285
epoch: 1, iter: 17200, loss: 30.399263381958008
epoch: 1, iter: 17300, loss: 31.051158905029297
epoch: 1, iter: 17400, loss: 30.570343017578125
epoch: 1, iter: 17500, loss: 30.416711807250977
epoch: 1, iter: 17600, loss: 30.80805778503418
epoch: 1, iter: 17700, loss: 30.25259780883789
epoch: 1, iter: 17800, loss: 30.286285400390625
epoch: 1, iter: 17900, loss: 30.53017807006836
epoch: 1, iter: 18000, loss: 30.383750915527344
epoch: 1, iteration: 18000, simlex-999: SpearmanrResult(correlation=0.1394123296964421, pvalue=1.5026797423339371e-05), men: SpearmanrResult(correlation=0.14637243756010673, pvalue=7.025458630976344e-14), sim353: SpearmanrResult(correlation=0.22732157506590558, pvalue=4.2868219856181685e-05), nearest to monster: [&apos;monster&apos;, &apos;blade&apos;, &apos;camera&apos;, &apos;robot&apos;, &apos;bird&apos;, &apos;harp&apos;, &apos;boat&apos;, &apos;mine&apos;, &apos;pen&apos;, &apos;triangle&apos;]

epoch: 1, iter: 18100, loss: 30.563156127929688
epoch: 1, iter: 18200, loss: 30.877361297607422
epoch: 1, iter: 18300, loss: 30.779144287109375
epoch: 1, iter: 18400, loss: 30.79215431213379
epoch: 1, iter: 18500, loss: 30.42022705078125
epoch: 1, iter: 18600, loss: 31.03143310546875
epoch: 1, iter: 18700, loss: 31.016183853149414
epoch: 1, iter: 18800, loss: 31.11318016052246
epoch: 1, iter: 18900, loss: 30.872020721435547
epoch: 1, iter: 19000, loss: 30.474287033081055
epoch: 1, iter: 19100, loss: 31.39320945739746
epoch: 1, iter: 19200, loss: 31.03207015991211
epoch: 1, iter: 19300, loss: 30.487464904785156
epoch: 1, iter: 19400, loss: 30.420028686523438
epoch: 1, iter: 19500, loss: 30.75209617614746
epoch: 1, iter: 19600, loss: 29.88393783569336
epoch: 1, iter: 19700, loss: 30.5760440826416
epoch: 1, iter: 19800, loss: 30.56399917602539
epoch: 1, iter: 19900, loss: 30.256702423095703
epoch: 1, iter: 20000, loss: 30.273393630981445
epoch: 1, iteration: 20000, simlex-999: SpearmanrResult(correlation=0.14223311686242013, pvalue=1.0018980772884857e-05), men: SpearmanrResult(correlation=0.14753960651141446, pvalue=4.428169020670323e-14), sim353: SpearmanrResult(correlation=0.22932179281384227, pvalue=3.652359446164985e-05), nearest to monster: [&apos;monster&apos;, &apos;blade&apos;, &apos;bird&apos;, &apos;robot&apos;, &apos;mine&apos;, &apos;harp&apos;, &apos;boat&apos;, &apos;triangle&apos;, &apos;camera&apos;, &apos;giant&apos;]

epoch: 1, iter: 20100, loss: 30.554931640625
epoch: 1, iter: 20200, loss: 30.8705997467041
epoch: 1, iter: 20300, loss: 30.918720245361328
epoch: 1, iter: 20400, loss: 30.68167495727539
epoch: 1, iter: 20500, loss: 30.421613693237305
epoch: 1, iter: 20600, loss: 30.65481948852539
epoch: 1, iter: 20700, loss: 30.48784828186035
epoch: 1, iter: 20800, loss: 30.522476196289062
epoch: 1, iter: 20900, loss: 30.651445388793945
epoch: 1, iter: 21000, loss: 30.873241424560547
epoch: 1, iter: 21100, loss: 30.824703216552734
epoch: 1, iter: 21200, loss: 30.80352783203125
epoch: 1, iter: 21300, loss: 31.21027183532715
epoch: 1, iter: 21400, loss: 30.860301971435547
epoch: 1, iter: 21500, loss: 30.478717803955078
epoch: 1, iter: 21600, loss: 30.538753509521484
epoch: 1, iter: 21700, loss: 31.42760467529297
epoch: 1, iter: 21800, loss: 31.005023956298828
epoch: 1, iter: 21900, loss: 30.42320442199707
epoch: 1, iter: 22000, loss: 30.21063232421875
epoch: 1, iteration: 22000, simlex-999: SpearmanrResult(correlation=0.1428147967792888, pvalue=9.206660938754159e-06), men: SpearmanrResult(correlation=0.14807291207730797, pvalue=3.5817711226237477e-14), sim353: SpearmanrResult(correlation=0.22970919127508704, pvalue=3.5402130162864466e-05), nearest to monster: [&apos;monster&apos;, &apos;blade&apos;, &apos;bird&apos;, &apos;mine&apos;, &apos;robot&apos;, &apos;giant&apos;, &apos;boat&apos;, &apos;harp&apos;, &apos;camera&apos;, &apos;pen&apos;]

epoch: 1, iter: 22100, loss: 30.796628952026367
epoch: 1, iter: 22200, loss: 30.456802368164062
epoch: 1, iter: 22300, loss: 30.246700286865234
epoch: 1, iter: 22400, loss: 30.443458557128906
epoch: 1, iter: 22500, loss: 30.49130630493164
epoch: 1, iter: 22600, loss: 30.463035583496094
epoch: 1, iter: 22700, loss: 30.521265029907227
epoch: 1, iter: 22800, loss: 30.170909881591797
epoch: 1, iter: 22900, loss: 30.44857406616211
epoch: 1, iter: 23000, loss: 30.58160972595215
epoch: 1, iter: 23100, loss: 30.80916404724121
epoch: 1, iter: 23200, loss: 30.507898330688477
epoch: 1, iter: 23300, loss: 30.540969848632812
epoch: 1, iter: 23400, loss: 30.272123336791992
epoch: 1, iter: 23500, loss: 30.770973205566406
epoch: 1, iter: 23600, loss: 30.44808006286621
epoch: 1, iter: 23700, loss: 30.266027450561523
epoch: 1, iter: 23800, loss: 30.43990707397461
epoch: 1, iter: 23900, loss: 30.382156372070312
epoch: 1, iter: 24000, loss: 30.580440521240234
epoch: 1, iteration: 24000, simlex-999: SpearmanrResult(correlation=0.14287100755966056, pvalue=9.131580692652058e-06), men: SpearmanrResult(correlation=0.14868537859505318, pvalue=2.804668835689092e-14), sim353: SpearmanrResult(correlation=0.2263985268197262, pvalue=4.613476464710824e-05), nearest to monster: [&apos;monster&apos;, &apos;bird&apos;, &apos;blade&apos;, &apos;giant&apos;, &apos;robot&apos;, &apos;mine&apos;, &apos;pen&apos;, &apos;camera&apos;, &apos;triangle&apos;, &apos;boat&apos;]

epoch: 1, iter: 24100, loss: 30.35135841369629
epoch: 1, iter: 24200, loss: 30.669477462768555
epoch: 1, iter: 24300, loss: 30.54084587097168
epoch: 1, iter: 24400, loss: 30.29105567932129
epoch: 1, iter: 24500, loss: 30.614038467407227
epoch: 1, iter: 24600, loss: 30.688926696777344
epoch: 1, iter: 24700, loss: 30.522205352783203
epoch: 1, iter: 24800, loss: 30.543987274169922
epoch: 1, iter: 24900, loss: 31.09662628173828
epoch: 1, iter: 25000, loss: 30.970827102661133
epoch: 1, iter: 25100, loss: 30.467174530029297
epoch: 1, iter: 25200, loss: 30.76879119873047
epoch: 1, iter: 25300, loss: 30.6207218170166
epoch: 1, iter: 25400, loss: 30.490726470947266
epoch: 1, iter: 25500, loss: 30.458398818969727
epoch: 1, iter: 25600, loss: 30.703933715820312
epoch: 1, iter: 25700, loss: 30.121395111083984
epoch: 1, iter: 25800, loss: 30.44470977783203
epoch: 1, iter: 25900, loss: 30.887786865234375
epoch: 1, iter: 26000, loss: 30.558914184570312
epoch: 1, iteration: 26000, simlex-999: SpearmanrResult(correlation=0.1440751174505626, pvalue=7.656767087120004e-06), men: SpearmanrResult(correlation=0.1491477742745481, pvalue=2.3302203512655484e-14), sim353: SpearmanrResult(correlation=0.23077736171791446, pvalue=3.247610265381441e-05), nearest to monster: [&apos;monster&apos;, &apos;blade&apos;, &apos;bird&apos;, &apos;robot&apos;, &apos;mine&apos;, &apos;elephant&apos;, &apos;harp&apos;, &apos;triangle&apos;, &apos;pen&apos;, &apos;reed&apos;]

epoch: 1, iter: 26100, loss: 30.59500503540039
epoch: 1, iter: 26200, loss: 30.334857940673828
epoch: 1, iter: 26300, loss: 30.802188873291016
epoch: 1, iter: 26400, loss: 30.327043533325195
epoch: 1, iter: 26500, loss: 30.643577575683594
epoch: 1, iter: 26600, loss: 30.822498321533203
epoch: 1, iter: 26700, loss: 30.609739303588867
epoch: 1, iter: 26800, loss: 30.348751068115234
epoch: 1, iter: 26900, loss: 30.833683013916016
epoch: 1, iter: 27000, loss: 30.705856323242188
epoch: 1, iter: 27100, loss: 30.677705764770508
epoch: 1, iter: 27200, loss: 30.479990005493164
epoch: 1, iter: 27300, loss: 30.481945037841797
epoch: 1, iter: 27400, loss: 30.73859214782715
epoch: 1, iter: 27500, loss: 30.516708374023438
epoch: 1, iter: 27600, loss: 30.041893005371094
epoch: 1, iter: 27700, loss: 30.019962310791016
epoch: 1, iter: 27800, loss: 30.198917388916016
epoch: 1, iter: 27900, loss: 31.1048583984375
epoch: 1, iter: 28000, loss: 30.839107513427734
epoch: 1, iteration: 28000, simlex-999: SpearmanrResult(correlation=0.14549903097533917, pvalue=6.205612354950464e-06), men: SpearmanrResult(correlation=0.14966965306971067, pvalue=1.8890981268115036e-14), sim353: SpearmanrResult(correlation=0.23346739336642225, pvalue=2.6087214499345555e-05), nearest to monster: [&apos;monster&apos;, &apos;robot&apos;, &apos;blade&apos;, &apos;bird&apos;, &apos;mine&apos;, &apos;reed&apos;, &apos;triangle&apos;, &apos;ghost&apos;, &apos;pen&apos;, &apos;harp&apos;]

epoch: 1, iter: 28100, loss: 30.756732940673828
epoch: 1, iter: 28200, loss: 30.536327362060547
epoch: 1, iter: 28300, loss: 31.24537467956543
epoch: 1, iter: 28400, loss: 30.601255416870117
epoch: 1, iter: 28500, loss: 30.263526916503906
epoch: 1, iter: 28600, loss: 30.142976760864258
epoch: 1, iter: 28700, loss: 30.747371673583984
epoch: 1, iter: 28800, loss: 30.630146026611328
epoch: 1, iter: 28900, loss: 30.744644165039062
epoch: 1, iter: 29000, loss: 30.7540283203125
epoch: 1, iter: 29100, loss: 30.279701232910156
epoch: 1, iter: 29200, loss: 30.66191291809082
epoch: 1, iter: 29300, loss: 30.671695709228516
epoch: 1, iter: 29400, loss: 30.434926986694336
epoch: 1, iter: 29500, loss: 30.72023582458496
epoch: 1, iter: 29600, loss: 30.603559494018555
epoch: 1, iter: 29700, loss: 30.372743606567383
epoch: 1, iter: 29800, loss: 30.525760650634766
epoch: 1, iter: 29900, loss: 30.840803146362305
epoch: 1, iter: 30000, loss: 30.364925384521484
epoch: 1, iteration: 30000, simlex-999: SpearmanrResult(correlation=0.14568623594428892, pvalue=6.035624214317192e-06), men: SpearmanrResult(correlation=0.1509883492958195, pvalue=1.1079359739743368e-14), sim353: SpearmanrResult(correlation=0.23492314885797097, pvalue=2.3145855166212694e-05), nearest to monster: [&apos;monster&apos;, &apos;robot&apos;, &apos;blade&apos;, &apos;reed&apos;, &apos;mine&apos;, &apos;rod&apos;, &apos;pen&apos;, &apos;bird&apos;, &apos;enigma&apos;, &apos;horn&apos;]

epoch: 1, iter: 30100, loss: 30.345247268676758
epoch: 1, iter: 30200, loss: 30.77888298034668
epoch: 1, iter: 30300, loss: 30.320167541503906
epoch: 1, iter: 30400, loss: 30.234546661376953
epoch: 1, iter: 30500, loss: 30.409408569335938
epoch: 1, iter: 30600, loss: 30.470806121826172
epoch: 1, iter: 30700, loss: 30.835254669189453
epoch: 1, iter: 30800, loss: 30.207406997680664
epoch: 1, iter: 30900, loss: 30.916057586669922
epoch: 1, iter: 31000, loss: 30.66683006286621
epoch: 1, iter: 31100, loss: 30.577659606933594
epoch: 1, iter: 31200, loss: 30.580257415771484
epoch: 1, iter: 31300, loss: 30.935575485229492
epoch: 1, iter: 31400, loss: 30.697229385375977
epoch: 1, iter: 31500, loss: 30.42900848388672
epoch: 1, iter: 31600, loss: 30.660232543945312
epoch: 1, iter: 31700, loss: 30.72662353515625
epoch: 1, iter: 31800, loss: 29.9237060546875
epoch: 1, iter: 31900, loss: 30.48178482055664
epoch: 1, iter: 32000, loss: 30.498600006103516
epoch: 1, iteration: 32000, simlex-999: SpearmanrResult(correlation=0.14636174425669718, pvalue=5.458483599637883e-06), men: SpearmanrResult(correlation=0.15148407706724934, pvalue=9.054505875289788e-15), sim353: SpearmanrResult(correlation=0.23389001400066448, pvalue=2.51987417090668e-05), nearest to monster: [&apos;monster&apos;, &apos;blade&apos;, &apos;robot&apos;, &apos;reed&apos;, &apos;mine&apos;, &apos;bird&apos;, &apos;rod&apos;, &apos;giant&apos;, &apos;enigma&apos;, &apos;pen&apos;]

epoch: 1, iter: 32100, loss: 31.0069580078125
epoch: 1, iter: 32200, loss: 30.799236297607422
epoch: 1, iter: 32300, loss: 30.04582405090332
epoch: 1, iter: 32400, loss: 30.311925888061523
epoch: 1, iter: 32500, loss: 30.556798934936523
epoch: 1, iter: 32600, loss: 30.72783660888672
epoch: 1, iter: 32700, loss: 30.761629104614258
epoch: 1, iter: 32800, loss: 30.628524780273438
epoch: 1, iter: 32900, loss: 30.40520477294922
epoch: 1, iter: 33000, loss: 30.739730834960938
epoch: 1, iter: 33100, loss: 30.354564666748047
epoch: 1, iter: 33200, loss: 30.47233009338379
epoch: 1, iter: 33300, loss: 31.133811950683594
epoch: 1, iter: 33400, loss: 30.499738693237305
epoch: 1, iter: 33500, loss: 30.797992706298828
epoch: 1, iter: 33600, loss: 30.39028549194336
epoch: 1, iter: 33700, loss: 30.41649627685547
epoch: 1, iter: 33800, loss: 30.45271873474121
epoch: 1, iter: 33900, loss: 30.6096248626709
epoch: 1, iter: 34000, loss: 30.480247497558594
epoch: 1, iteration: 34000, simlex-999: SpearmanrResult(correlation=0.1473754688161237, pvalue=4.69031176866327e-06), men: SpearmanrResult(correlation=0.1529620524813248, pvalue=4.940947898179605e-15), sim353: SpearmanrResult(correlation=0.2346744277518143, pvalue=2.362507911995377e-05), nearest to monster: [&apos;monster&apos;, &apos;blade&apos;, &apos;robot&apos;, &apos;mine&apos;, &apos;bird&apos;, &apos;giant&apos;, &apos;reed&apos;, &apos;rod&apos;, &apos;clown&apos;, &apos;enigma&apos;]

epoch: 1, iter: 34100, loss: 30.54153823852539
epoch: 1, iter: 34200, loss: 30.292634963989258
epoch: 1, iter: 34300, loss: 30.777355194091797
epoch: 1, iter: 34400, loss: 30.559667587280273
epoch: 1, iter: 34500, loss: 29.992252349853516
epoch: 1, iter: 34600, loss: 30.286727905273438
epoch: 1, iter: 34700, loss: 30.328699111938477
epoch: 1, iter: 34800, loss: 30.475990295410156
epoch: 1, iter: 34900, loss: 30.71676254272461
epoch: 1, iter: 35000, loss: 30.722000122070312
epoch: 1, iter: 35100, loss: 30.39300537109375
epoch: 1, iter: 35200, loss: 30.483230590820312
epoch: 1, iter: 35300, loss: 30.614459991455078
epoch: 1, iter: 35400, loss: 29.942462921142578
epoch: 1, iter: 35500, loss: 30.2659854888916
epoch: 1, iter: 35600, loss: 30.37142562866211
epoch: 1, iter: 35700, loss: 29.414955139160156
epoch: 1, iter: 35800, loss: 30.6357479095459
epoch: 1, iter: 35900, loss: 30.29971694946289
epoch: 1, iter: 36000, loss: 30.45860481262207
epoch: 1, iteration: 36000, simlex-999: SpearmanrResult(correlation=0.1479277049908929, pvalue=4.316518848130977e-06), men: SpearmanrResult(correlation=0.1534592647777511, pvalue=4.024677001188328e-15), sim353: SpearmanrResult(correlation=0.23541144828393648, pvalue=2.22316752869189e-05), nearest to monster: [&apos;monster&apos;, &apos;blade&apos;, &apos;giant&apos;, &apos;mine&apos;, &apos;bird&apos;, &apos;reed&apos;, &apos;robot&apos;, &apos;rod&apos;, &apos;clown&apos;, &apos;enigma&apos;]

epoch: 1, iter: 36100, loss: 30.45232391357422
epoch: 1, iter: 36200, loss: 30.192720413208008
epoch: 1, iter: 36300, loss: 30.173755645751953
epoch: 1, iter: 36400, loss: 30.194917678833008
epoch: 1, iter: 36500, loss: 30.539600372314453
epoch: 1, iter: 36600, loss: 30.462547302246094
epoch: 1, iter: 36700, loss: 30.38189697265625
epoch: 1, iter: 36800, loss: 30.229658126831055
epoch: 1, iter: 36900, loss: 30.55349349975586
epoch: 1, iter: 37000, loss: 30.451251983642578
epoch: 1, iter: 37100, loss: 30.258930206298828
epoch: 1, iter: 37200, loss: 30.375003814697266
epoch: 1, iter: 37300, loss: 30.661685943603516
epoch: 1, iter: 37400, loss: 30.22052001953125
epoch: 1, iter: 37500, loss: 30.464017868041992
epoch: 1, iter: 37600, loss: 30.531139373779297
epoch: 1, iter: 37700, loss: 30.161556243896484
epoch: 1, iter: 37800, loss: 30.280288696289062
epoch: 1, iter: 37900, loss: 30.579071044921875
epoch: 1, iter: 38000, loss: 30.68809700012207
epoch: 1, iteration: 38000, simlex-999: SpearmanrResult(correlation=0.14842186148466413, pvalue=4.006345881913544e-06), men: SpearmanrResult(correlation=0.1535692327051543, pvalue=3.845826952077038e-15), sim353: SpearmanrResult(correlation=0.24061126807866595, pvalue=1.4397516474117237e-05), nearest to monster: [&apos;monster&apos;, &apos;blade&apos;, &apos;bird&apos;, &apos;mine&apos;, &apos;robot&apos;, &apos;reed&apos;, &apos;giant&apos;, &apos;ghost&apos;, &apos;enigma&apos;, &apos;harp&apos;]

epoch: 1, iter: 38100, loss: 30.60862922668457
epoch: 1, iter: 38200, loss: 30.42845916748047
epoch: 1, iter: 38300, loss: 30.334047317504883
epoch: 1, iter: 38400, loss: 30.224014282226562
epoch: 1, iter: 38500, loss: 30.38711166381836
epoch: 1, iter: 38600, loss: 30.579326629638672
epoch: 1, iter: 38700, loss: 30.49921417236328
epoch: 1, iter: 38800, loss: 30.80820083618164
epoch: 1, iter: 38900, loss: 31.00635528564453
epoch: 1, iter: 39000, loss: 30.365596771240234
epoch: 1, iter: 39100, loss: 30.78212547302246
epoch: 1, iter: 39200, loss: 30.845741271972656
epoch: 1, iter: 39300, loss: 30.45212173461914
epoch: 1, iter: 39400, loss: 30.290306091308594
epoch: 1, iter: 39500, loss: 30.514169692993164
epoch: 1, iter: 39600, loss: 30.55576515197754
epoch: 1, iter: 39700, loss: 31.103591918945312
epoch: 1, iter: 39800, loss: 30.821365356445312
epoch: 1, iter: 39900, loss: 30.697998046875
epoch: 1, iter: 40000, loss: 30.70787811279297
epoch: 1, iteration: 40000, simlex-999: SpearmanrResult(correlation=0.14840725941493657, pvalue=4.015197419076501e-06), men: SpearmanrResult(correlation=0.15357136041153727, pvalue=3.842444769779768e-15), sim353: SpearmanrResult(correlation=0.24335858208245514, pvalue=1.1399247512803185e-05), nearest to monster: [&apos;monster&apos;, &apos;blade&apos;, &apos;bird&apos;, &apos;reed&apos;, &apos;robot&apos;, &apos;ghost&apos;, &apos;mine&apos;, &apos;enigma&apos;, &apos;rod&apos;, &apos;giant&apos;]

epoch: 1, iter: 40100, loss: 30.93113899230957
epoch: 1, iter: 40200, loss: 30.542682647705078
epoch: 1, iter: 40300, loss: 30.148162841796875
epoch: 1, iter: 40400, loss: 30.786636352539062
epoch: 1, iter: 40500, loss: 30.51471710205078
epoch: 1, iter: 40600, loss: 30.892784118652344
epoch: 1, iter: 40700, loss: 30.750877380371094
epoch: 1, iter: 40800, loss: 30.910011291503906
epoch: 1, iter: 40900, loss: 30.417316436767578
epoch: 1, iter: 41000, loss: 30.877635955810547
epoch: 1, iter: 41100, loss: 31.21525764465332
epoch: 1, iter: 41200, loss: 30.61797332763672
epoch: 1, iter: 41300, loss: 29.999229431152344
epoch: 1, iter: 41400, loss: 30.419879913330078
epoch: 1, iter: 41500, loss: 30.497615814208984
epoch: 1, iter: 41600, loss: 31.086910247802734
epoch: 1, iter: 41700, loss: 31.012123107910156
epoch: 1, iter: 41800, loss: 30.67609977722168
epoch: 1, iter: 41900, loss: 30.800514221191406
epoch: 1, iter: 42000, loss: 30.609895706176758
epoch: 1, iteration: 42000, simlex-999: SpearmanrResult(correlation=0.15010702795733427, pvalue=3.101153292343823e-06), men: SpearmanrResult(correlation=0.15464227684833873, pvalue=2.4637832880461148e-15), sim353: SpearmanrResult(correlation=0.2412430532740497, pvalue=1.3648100607040392e-05), nearest to monster: [&apos;monster&apos;, &apos;blade&apos;, &apos;ghost&apos;, &apos;bird&apos;, &apos;robot&apos;, &apos;mine&apos;, &apos;reed&apos;, &apos;enigma&apos;, &apos;tiger&apos;, &apos;reads&apos;]

epoch: 1, iter: 42100, loss: 30.431297302246094
epoch: 1, iter: 42200, loss: 31.107322692871094
epoch: 1, iter: 42300, loss: 30.734027862548828
epoch: 1, iter: 42400, loss: 30.269119262695312
epoch: 1, iter: 42500, loss: 30.10508155822754
epoch: 1, iter: 42600, loss: 30.353633880615234
epoch: 1, iter: 42700, loss: 30.593158721923828
epoch: 1, iter: 42800, loss: 30.670429229736328
epoch: 1, iter: 42900, loss: 30.44912338256836
epoch: 1, iter: 43000, loss: 30.147382736206055
epoch: 1, iter: 43100, loss: 30.15931510925293
epoch: 1, iter: 43200, loss: 30.213638305664062
epoch: 1, iter: 43300, loss: 30.583301544189453
epoch: 1, iter: 43400, loss: 30.891719818115234
epoch: 1, iter: 43500, loss: 30.8968448638916
epoch: 1, iter: 43600, loss: 30.235538482666016
epoch: 1, iter: 43700, loss: 30.293712615966797
epoch: 1, iter: 43800, loss: 30.817930221557617
epoch: 1, iter: 43900, loss: 30.755901336669922
epoch: 1, iter: 44000, loss: 30.5270938873291
epoch: 1, iteration: 44000, simlex-999: SpearmanrResult(correlation=0.15040984462509274, pvalue=2.9607903887451717e-06), men: SpearmanrResult(correlation=0.15462659465737233, pvalue=2.479925723603805e-15), sim353: SpearmanrResult(correlation=0.24184031978628343, pvalue=1.29738022188437e-05), nearest to monster: [&apos;monster&apos;, &apos;ghost&apos;, &apos;robot&apos;, &apos;blade&apos;, &apos;bird&apos;, &apos;reed&apos;, &apos;enigma&apos;, &apos;mine&apos;, &apos;pen&apos;, &apos;giant&apos;]

epoch: 1, iter: 44100, loss: 30.67951011657715
epoch: 1, iter: 44200, loss: 30.391963958740234
epoch: 1, iter: 44300, loss: 30.283348083496094
epoch: 1, iter: 44400, loss: 30.486854553222656
epoch: 1, iter: 44500, loss: 29.995058059692383
epoch: 1, iter: 44600, loss: 29.919422149658203
epoch: 1, iter: 44700, loss: 30.681249618530273
epoch: 1, iter: 44800, loss: 30.176651000976562
epoch: 1, iter: 44900, loss: 31.011629104614258
epoch: 1, iter: 45000, loss: 30.286090850830078
epoch: 1, iter: 45100, loss: 30.60489273071289
epoch: 1, iter: 45200, loss: 30.306713104248047
epoch: 1, iter: 45300, loss: 30.408802032470703
epoch: 1, iter: 45400, loss: 30.530902862548828
epoch: 1, iter: 45500, loss: 30.43999481201172
epoch: 1, iter: 45600, loss: 30.87363624572754
epoch: 1, iter: 45700, loss: 30.33321762084961
epoch: 1, iter: 45800, loss: 30.374361038208008
epoch: 1, iter: 45900, loss: 30.656036376953125
epoch: 1, iter: 46000, loss: 30.552873611450195
epoch: 1, iteration: 46000, simlex-999: SpearmanrResult(correlation=0.1506515900016934, pvalue=2.8531245356788605e-06), men: SpearmanrResult(correlation=0.15536691339728612, pvalue=1.820665609880339e-15), sim353: SpearmanrResult(correlation=0.24012203571531798, pvalue=1.5004498206655285e-05), nearest to monster: [&apos;monster&apos;, &apos;robot&apos;, &apos;bird&apos;, &apos;blade&apos;, &apos;mine&apos;, &apos;ghost&apos;, &apos;enigma&apos;, &apos;triangle&apos;, &apos;reed&apos;, &apos;pen&apos;]

epoch: 1, iter: 46100, loss: 30.70708656311035
epoch: 1, iter: 46200, loss: 30.461339950561523
epoch: 1, iter: 46300, loss: 30.217098236083984
epoch: 1, iter: 46400, loss: 30.840656280517578
epoch: 1, iter: 46500, loss: 30.493894577026367
epoch: 1, iter: 46600, loss: 30.61757469177246
epoch: 1, iter: 46700, loss: 30.641674041748047
epoch: 1, iter: 46800, loss: 30.795719146728516
epoch: 1, iter: 46900, loss: 30.51910400390625
epoch: 1, iter: 47000, loss: 30.65304946899414
epoch: 1, iter: 47100, loss: 30.11658477783203
epoch: 1, iter: 47200, loss: 30.48131561279297
epoch: 1, iter: 47300, loss: 30.761260986328125
epoch: 1, iter: 47400, loss: 30.574722290039062
epoch: 1, iter: 47500, loss: 30.653053283691406
epoch: 1, iter: 47600, loss: 30.582984924316406
epoch: 1, iter: 47700, loss: 30.558650970458984
epoch: 1, iter: 47800, loss: 30.786725997924805
epoch: 1, iter: 47900, loss: 30.690078735351562
epoch: 1, iter: 48000, loss: 30.940021514892578
epoch: 1, iteration: 48000, simlex-999: SpearmanrResult(correlation=0.1512458867092362, pvalue=2.6041543817031568e-06), men: SpearmanrResult(correlation=0.1557567213740912, pvalue=1.5463267464958923e-15), sim353: SpearmanrResult(correlation=0.24230566895264108, pvalue=1.247048744341614e-05), nearest to monster: [&apos;monster&apos;, &apos;robot&apos;, &apos;mine&apos;, &apos;blade&apos;, &apos;ghost&apos;, &apos;bird&apos;, &apos;triangle&apos;, &apos;pen&apos;, &apos;mirror&apos;, &apos;reed&apos;]

epoch: 1, iter: 48100, loss: 30.569496154785156
epoch: 1, iter: 48200, loss: 30.488758087158203
epoch: 1, iter: 48300, loss: 30.724409103393555
epoch: 1, iter: 48400, loss: 30.37924575805664
epoch: 1, iter: 48500, loss: 30.688404083251953
epoch: 1, iter: 48600, loss: 31.19538688659668
epoch: 1, iter: 48700, loss: 31.03116226196289
epoch: 1, iter: 48800, loss: 30.401390075683594
epoch: 1, iter: 48900, loss: 30.71941566467285
epoch: 1, iter: 49000, loss: 30.60601043701172
epoch: 1, iter: 49100, loss: 30.593238830566406
epoch: 1, iter: 49200, loss: 30.418926239013672
epoch: 1, iter: 49300, loss: 30.479490280151367
epoch: 1, iter: 49400, loss: 30.65471649169922
epoch: 1, iter: 49500, loss: 31.25054359436035
epoch: 1, iter: 49600, loss: 30.942890167236328
epoch: 1, iter: 49700, loss: 30.643369674682617
epoch: 1, iter: 49800, loss: 30.203311920166016
epoch: 1, iter: 49900, loss: 30.408056259155273
epoch: 1, iter: 50000, loss: 30.49826431274414
epoch: 1, iteration: 50000, simlex-999: SpearmanrResult(correlation=0.1525143841833279, pvalue=2.140523243449846e-06), men: SpearmanrResult(correlation=0.15705781004352684, pvalue=8.938073731951767e-16), sim353: SpearmanrResult(correlation=0.24303933091018648, pvalue=1.1714453427503608e-05), nearest to monster: [&apos;monster&apos;, &apos;robot&apos;, &apos;mine&apos;, &apos;ghost&apos;, &apos;blade&apos;, &apos;triangle&apos;, &apos;bird&apos;, &apos;mirror&apos;, &apos;pen&apos;, &apos;trilogy&apos;]

epoch: 1, iter: 50100, loss: 30.582521438598633
epoch: 1, iter: 50200, loss: 30.166404724121094
epoch: 1, iter: 50300, loss: 30.79269790649414
epoch: 1, iter: 50400, loss: 30.7398738861084
epoch: 1, iter: 50500, loss: 30.5670108795166
epoch: 1, iter: 50600, loss: 30.718910217285156
epoch: 1, iter: 50700, loss: 30.94159507751465
epoch: 1, iter: 50800, loss: 30.046207427978516
epoch: 1, iter: 50900, loss: 30.098331451416016
epoch: 1, iter: 51000, loss: 29.920578002929688
epoch: 1, iter: 51100, loss: 30.722366333007812
epoch: 1, iter: 51200, loss: 30.87830352783203
epoch: 1, iter: 51300, loss: 30.5864200592041
epoch: 1, iter: 51400, loss: 30.589786529541016
epoch: 1, iter: 51500, loss: 30.31692886352539
epoch: 1, iter: 51600, loss: 30.661989212036133
epoch: 1, iter: 51700, loss: 30.14481544494629
epoch: 1, iter: 51800, loss: 30.520448684692383
epoch: 1, iter: 51900, loss: 30.797508239746094
epoch: 1, iter: 52000, loss: 29.767147064208984
epoch: 1, iteration: 52000, simlex-999: SpearmanrResult(correlation=0.15373255141051995, pvalue=1.7705182477987938e-06), men: SpearmanrResult(correlation=0.1585061793559117, pvalue=4.829025134421122e-16), sim353: SpearmanrResult(correlation=0.24509030045172758, pvalue=9.825140052862682e-06), nearest to monster: [&apos;monster&apos;, &apos;robot&apos;, &apos;blade&apos;, &apos;mine&apos;, &apos;ghost&apos;, &apos;triangle&apos;, &apos;pen&apos;, &apos;reed&apos;, &apos;mirror&apos;, &apos;storyline&apos;]

epoch: 1, iter: 52100, loss: 30.682071685791016
epoch: 1, iter: 52200, loss: 30.016061782836914
epoch: 1, iter: 52300, loss: 30.534984588623047
epoch: 1, iter: 52400, loss: 30.56295394897461
epoch: 1, iter: 52500, loss: 30.030967712402344
epoch: 1, iter: 52600, loss: 30.3729305267334
epoch: 1, iter: 52700, loss: 30.135780334472656
epoch: 1, iter: 52800, loss: 30.442276000976562
epoch: 1, iter: 52900, loss: 30.262731552124023
epoch: 1, iter: 53000, loss: 30.31039047241211
epoch: 1, iter: 53100, loss: 31.22720718383789
epoch: 1, iter: 53200, loss: 30.35527801513672
epoch: 1, iter: 53300, loss: 30.849201202392578
epoch: 1, iter: 53400, loss: 30.32939910888672
epoch: 1, iter: 53500, loss: 30.342052459716797
epoch: 1, iter: 53600, loss: 30.57972526550293
epoch: 1, iter: 53700, loss: 29.98486328125
epoch: 1, iter: 53800, loss: 30.098655700683594
epoch: 1, iter: 53900, loss: 30.3535213470459
epoch: 1, iter: 54000, loss: 30.042743682861328
epoch: 1, iteration: 54000, simlex-999: SpearmanrResult(correlation=0.1536032686694018, pvalue=1.8066656752367236e-06), men: SpearmanrResult(correlation=0.1585990419061131, pvalue=4.641201609227958e-16), sim353: SpearmanrResult(correlation=0.25060825189386676, pvalue=6.074459332227015e-06), nearest to monster: [&apos;monster&apos;, &apos;robot&apos;, &apos;ghost&apos;, &apos;blade&apos;, &apos;mine&apos;, &apos;triangle&apos;, &apos;bird&apos;, &apos;pen&apos;, &apos;giant&apos;, &apos;trilogy&apos;]

epoch: 1, iter: 54100, loss: 30.99134063720703
epoch: 1, iter: 54200, loss: 30.4207820892334
epoch: 1, iter: 54300, loss: 29.99266815185547
epoch: 1, iter: 54400, loss: 30.288175582885742
epoch: 1, iter: 54500, loss: 30.557764053344727
epoch: 1, iter: 54600, loss: 30.272037506103516
epoch: 1, iter: 54700, loss: 30.78939437866211
epoch: 1, iter: 54800, loss: 30.318763732910156
epoch: 1, iter: 54900, loss: 30.315868377685547
epoch: 1, iter: 55000, loss: 30.52359962463379
epoch: 1, iter: 55100, loss: 30.504047393798828
epoch: 1, iter: 55200, loss: 30.359554290771484
epoch: 1, iter: 55300, loss: 30.453826904296875
epoch: 1, iter: 55400, loss: 30.830547332763672
epoch: 1, iter: 55500, loss: 30.533924102783203
epoch: 1, iter: 55600, loss: 30.538036346435547
epoch: 1, iter: 55700, loss: 30.60943603515625
epoch: 1, iter: 55800, loss: 30.83778190612793
epoch: 1, iter: 55900, loss: 30.720783233642578
epoch: 1, iter: 56000, loss: 30.150545120239258
epoch: 1, iteration: 56000, simlex-999: SpearmanrResult(correlation=0.1549713586940779, pvalue=1.457568750556874e-06), men: SpearmanrResult(correlation=0.15844155102847718, pvalue=4.96414018000047e-16), sim353: SpearmanrResult(correlation=0.24769058352404524, pvalue=7.843960247464432e-06), nearest to monster: [&apos;monster&apos;, &apos;robot&apos;, &apos;ghost&apos;, &apos;giant&apos;, &apos;pen&apos;, &apos;mine&apos;, &apos;bird&apos;, &apos;blade&apos;, &apos;storyline&apos;, &apos;triangle&apos;]

epoch: 1, iter: 56100, loss: 30.171369552612305
epoch: 1, iter: 56200, loss: 30.735065460205078
epoch: 1, iter: 56300, loss: 30.493385314941406
epoch: 1, iter: 56400, loss: 30.45469856262207
epoch: 1, iter: 56500, loss: 30.479145050048828
epoch: 1, iter: 56600, loss: 30.517457962036133
epoch: 1, iter: 56700, loss: 30.358793258666992
epoch: 1, iter: 56800, loss: 30.938446044921875
epoch: 1, iter: 56900, loss: 30.858768463134766
epoch: 1, iter: 57000, loss: 30.36663818359375
epoch: 1, iter: 57100, loss: 30.539148330688477
epoch: 1, iter: 57200, loss: 30.83847427368164
epoch: 1, iter: 57300, loss: 30.67807388305664
epoch: 1, iter: 57400, loss: 30.247142791748047
epoch: 1, iter: 57500, loss: 30.21233367919922
epoch: 1, iter: 57600, loss: 30.43688201904297
epoch: 1, iter: 57700, loss: 30.402862548828125
epoch: 1, iter: 57800, loss: 30.552614212036133
epoch: 1, iter: 57900, loss: 30.985126495361328
epoch: 1, iter: 58000, loss: 30.84340476989746
epoch: 1, iteration: 58000, simlex-999: SpearmanrResult(correlation=0.1567349094604098, pvalue=1.1021343002031157e-06), men: SpearmanrResult(correlation=0.15902652554699048, pvalue=3.865317045437844e-16), sim353: SpearmanrResult(correlation=0.24948667457270562, pvalue=6.704232943262648e-06), nearest to monster: [&apos;monster&apos;, &apos;robot&apos;, &apos;giant&apos;, &apos;pen&apos;, &apos;ghost&apos;, &apos;mine&apos;, &apos;bird&apos;, &apos;clown&apos;, &apos;reed&apos;, &apos;mirror&apos;]

epoch: 1, iter: 58100, loss: 30.185630798339844
epoch: 1, iter: 58200, loss: 30.426095962524414
epoch: 1, iter: 58300, loss: 30.398298263549805
epoch: 1, iter: 58400, loss: 30.785690307617188
epoch: 1, iter: 58500, loss: 30.762544631958008
epoch: 1, iter: 58600, loss: 30.656660079956055
epoch: 1, iter: 58700, loss: 30.267963409423828
epoch: 1, iter: 58800, loss: 30.180572509765625
epoch: 1, iter: 58900, loss: 30.352771759033203
epoch: 1, iter: 59000, loss: 30.799413681030273
epoch: 1, iter: 59100, loss: 30.187427520751953
epoch: 1, iter: 59200, loss: 30.583654403686523
epoch: 1, iter: 59300, loss: 30.478708267211914
epoch: 1, iter: 59400, loss: 30.59113121032715
epoch: 1, iter: 59500, loss: 30.526996612548828
epoch: 1, iter: 59600, loss: 29.880691528320312
epoch: 1, iter: 59700, loss: 30.175827026367188
epoch: 1, iter: 59800, loss: 31.251361846923828
epoch: 1, iter: 59900, loss: 30.632102966308594
epoch: 1, iter: 60000, loss: 30.681888580322266
epoch: 1, iteration: 60000, simlex-999: SpearmanrResult(correlation=0.15766878472714893, pvalue=9.493068391257752e-07), men: SpearmanrResult(correlation=0.1599384328989951, pvalue=2.6120469754271507e-16), sim353: SpearmanrResult(correlation=0.2506047067318135, pvalue=6.0763581426292506e-06), nearest to monster: [&apos;monster&apos;, &apos;robot&apos;, &apos;giant&apos;, &apos;pen&apos;, &apos;ghost&apos;, &apos;cow&apos;, &apos;storyline&apos;, &apos;blade&apos;, &apos;bird&apos;, &apos;mine&apos;]

epoch: 1, iter: 60100, loss: 30.331680297851562
epoch: 1, iter: 60200, loss: 30.630077362060547
epoch: 1, iter: 60300, loss: 30.40506362915039
epoch: 1, iter: 60400, loss: 30.622928619384766
epoch: 1, iter: 60500, loss: 30.441761016845703
epoch: 1, iter: 60600, loss: 30.31716537475586
epoch: 1, iter: 60700, loss: 30.88729476928711
epoch: 1, iter: 60800, loss: 30.464248657226562
epoch: 1, iter: 60900, loss: 30.365467071533203
epoch: 1, iter: 61000, loss: 30.315540313720703
epoch: 1, iter: 61100, loss: 30.57969856262207
epoch: 1, iter: 61200, loss: 30.44414520263672
epoch: 1, iter: 61300, loss: 30.557884216308594
epoch: 1, iter: 61400, loss: 30.003623962402344
epoch: 1, iter: 61500, loss: 30.530445098876953
epoch: 1, iter: 61600, loss: 30.768972396850586
epoch: 1, iter: 61700, loss: 30.325469970703125
epoch: 1, iter: 61800, loss: 30.635547637939453
epoch: 1, iter: 61900, loss: 30.08446502685547
epoch: 1, iter: 62000, loss: 30.466468811035156
epoch: 1, iteration: 62000, simlex-999: SpearmanrResult(correlation=0.1572838048897373, pvalue=1.0096640956970655e-06), men: SpearmanrResult(correlation=0.16137007881415613, pvalue=1.405264112416452e-16), sim353: SpearmanrResult(correlation=0.25097676215992787, pvalue=5.8801359002641125e-06), nearest to monster: [&apos;monster&apos;, &apos;robot&apos;, &apos;pen&apos;, &apos;ghost&apos;, &apos;giant&apos;, &apos;cow&apos;, &apos;mine&apos;, &apos;storyline&apos;, &apos;bird&apos;, &apos;blade&apos;]

epoch: 1, iter: 62100, loss: 30.18429946899414
epoch: 1, iter: 62200, loss: 30.559833526611328
epoch: 1, iter: 62300, loss: 30.80440902709961
epoch: 1, iter: 62400, loss: 30.450206756591797
epoch: 1, iter: 62500, loss: 30.552818298339844
epoch: 1, iter: 62600, loss: 30.82094383239746
epoch: 1, iter: 62700, loss: 30.254344940185547
epoch: 1, iter: 62800, loss: 30.72846221923828
epoch: 1, iter: 62900, loss: 30.654434204101562
epoch: 1, iter: 63000, loss: 30.073328018188477
epoch: 1, iter: 63100, loss: 30.521087646484375
epoch: 1, iter: 63200, loss: 30.763505935668945
epoch: 1, iter: 63300, loss: 30.240577697753906
epoch: 1, iter: 63400, loss: 30.317813873291016
epoch: 1, iter: 63500, loss: 30.43919563293457
epoch: 1, iter: 63600, loss: 30.885873794555664
epoch: 1, iter: 63700, loss: 30.13918685913086
epoch: 1, iter: 63800, loss: 30.71306610107422
epoch: 1, iter: 63900, loss: 30.3992919921875
epoch: 1, iter: 64000, loss: 30.920867919921875
epoch: 1, iteration: 64000, simlex-999: SpearmanrResult(correlation=0.15698834744605628, pvalue=1.0584701505049052e-06), men: SpearmanrResult(correlation=0.1624835806175362, pvalue=8.642929806246007e-17), sim353: SpearmanrResult(correlation=0.24978148278555448, pvalue=6.532925572818505e-06), nearest to monster: [&apos;monster&apos;, &apos;robot&apos;, &apos;pen&apos;, &apos;storyline&apos;, &apos;giant&apos;, &apos;ghost&apos;, &apos;triangle&apos;, &apos;blade&apos;, &apos;cow&apos;, &apos;clown&apos;]

epoch: 1, iter: 64100, loss: 30.346240997314453
epoch: 1, iter: 64200, loss: 30.147878646850586
epoch: 1, iter: 64300, loss: 30.068510055541992
epoch: 1, iter: 64400, loss: 30.74982452392578
epoch: 1, iter: 64500, loss: 30.429706573486328
epoch: 1, iter: 64600, loss: 30.37489128112793
epoch: 1, iter: 64700, loss: 30.758684158325195
epoch: 1, iter: 64800, loss: 30.118595123291016
epoch: 1, iter: 64900, loss: 30.60893440246582
epoch: 1, iter: 65000, loss: 30.800472259521484
epoch: 1, iter: 65100, loss: 30.511920928955078
epoch: 1, iter: 65200, loss: 30.794395446777344
epoch: 1, iter: 65300, loss: 30.49081802368164
epoch: 1, iter: 65400, loss: 30.53462791442871
epoch: 1, iter: 65500, loss: 30.544513702392578
epoch: 1, iter: 65600, loss: 30.508167266845703
epoch: 1, iter: 65700, loss: 30.721145629882812
epoch: 1, iter: 65800, loss: 30.107677459716797
epoch: 1, iter: 65900, loss: 30.400524139404297
epoch: 1, iter: 66000, loss: 30.216068267822266
epoch: 1, iteration: 66000, simlex-999: SpearmanrResult(correlation=0.15907493049754254, pvalue=7.569762376508921e-07), men: SpearmanrResult(correlation=0.1622197008816302, pvalue=9.70111944701947e-17), sim353: SpearmanrResult(correlation=0.25537220334352345, pvalue=3.974470732999653e-06), nearest to monster: [&apos;monster&apos;, &apos;robot&apos;, &apos;giant&apos;, &apos;pen&apos;, &apos;storyline&apos;, &apos;triangle&apos;, &apos;clown&apos;, &apos;reed&apos;, &apos;ghost&apos;, &apos;blade&apos;]

epoch: 1, iter: 66100, loss: 30.723974227905273
epoch: 1, iter: 66200, loss: 30.21212387084961
epoch: 1, iter: 66300, loss: 30.621620178222656
epoch: 1, iter: 66400, loss: 31.26927947998047
epoch: 1, iter: 66500, loss: 30.06582260131836
epoch: 1, iter: 66600, loss: 30.753280639648438
epoch: 1, iter: 66700, loss: 30.463829040527344
epoch: 1, iter: 66800, loss: 30.498855590820312
epoch: 1, iter: 66900, loss: 30.842039108276367
epoch: 1, iter: 67000, loss: 29.559223175048828
epoch: 1, iter: 67100, loss: 30.619075775146484
epoch: 1, iter: 67200, loss: 30.44180679321289
epoch: 1, iter: 67300, loss: 30.190532684326172
epoch: 1, iter: 67400, loss: 30.0849552154541
epoch: 1, iter: 67500, loss: 30.109207153320312
epoch: 1, iter: 67600, loss: 30.49976348876953
epoch: 1, iter: 67700, loss: 30.740123748779297
epoch: 1, iter: 67800, loss: 30.926212310791016
epoch: 1, iter: 67900, loss: 30.441604614257812
epoch: 1, iter: 68000, loss: 30.73117446899414
epoch: 1, iteration: 68000, simlex-999: SpearmanrResult(correlation=0.15894696899386346, pvalue=7.727968829631284e-07), men: SpearmanrResult(correlation=0.16225855404638603, pvalue=9.537652911995123e-17), sim353: SpearmanrResult(correlation=0.2561217252365703, pvalue=3.715012465818261e-06), nearest to monster: [&apos;monster&apos;, &apos;robot&apos;, &apos;giant&apos;, &apos;clown&apos;, &apos;pen&apos;, &apos;triangle&apos;, &apos;killer&apos;, &apos;storyline&apos;, &apos;reed&apos;, &apos;vampire&apos;]

epoch: 1, iter: 68100, loss: 30.470741271972656
epoch: 1, iter: 68200, loss: 30.515743255615234
epoch: 1, iter: 68300, loss: 30.583833694458008
epoch: 1, iter: 68400, loss: 29.629898071289062
epoch: 1, iter: 68500, loss: 30.373931884765625
epoch: 1, iter: 68600, loss: 30.927085876464844
epoch: 1, iter: 68700, loss: 30.736282348632812
epoch: 1, iter: 68800, loss: 30.456012725830078
epoch: 1, iter: 68900, loss: 30.077430725097656
epoch: 1, iter: 69000, loss: 30.592777252197266
epoch: 1, iter: 69100, loss: 30.24420738220215
epoch: 1, iter: 69200, loss: 29.89266586303711
epoch: 1, iter: 69300, loss: 30.14252471923828
epoch: 1, iter: 69400, loss: 30.470155715942383
epoch: 1, iter: 69500, loss: 30.696094512939453
epoch: 1, iter: 69600, loss: 30.263404846191406
epoch: 1, iter: 69700, loss: 30.48387336730957
epoch: 1, iter: 69800, loss: 31.011812210083008
epoch: 1, iter: 69900, loss: 30.354785919189453
epoch: 1, iter: 70000, loss: 30.257991790771484
epoch: 1, iteration: 70000, simlex-999: SpearmanrResult(correlation=0.1599804231196073, pvalue=6.536001505651106e-07), men: SpearmanrResult(correlation=0.16235736400138848, pvalue=9.134061487743441e-17), sim353: SpearmanrResult(correlation=0.2579999147748847, pvalue=3.1339385169295858e-06), nearest to monster: [&apos;monster&apos;, &apos;robot&apos;, &apos;clown&apos;, &apos;giant&apos;, &apos;triangle&apos;, &apos;pen&apos;, &apos;storyline&apos;, &apos;vampire&apos;, &apos;ghost&apos;, &apos;killer&apos;]

epoch: 1, iter: 70100, loss: 30.840045928955078
epoch: 1, iter: 70200, loss: 30.389122009277344
epoch: 1, iter: 70300, loss: 31.05836296081543
epoch: 1, iter: 70400, loss: 30.655227661132812
epoch: 1, iter: 70500, loss: 30.400094985961914
epoch: 1, iter: 70600, loss: 30.757015228271484
epoch: 1, iter: 70700, loss: 30.292152404785156
epoch: 1, iter: 70800, loss: 30.084205627441406
epoch: 1, iter: 70900, loss: 30.660961151123047
epoch: 1, iter: 71000, loss: 30.49699592590332
epoch: 1, iter: 71100, loss: 30.42240333557129
epoch: 1, iter: 71200, loss: 29.931034088134766
epoch: 1, iter: 71300, loss: 30.057233810424805
epoch: 1, iter: 71400, loss: 30.548381805419922
epoch: 1, iter: 71500, loss: 30.618526458740234
epoch: 1, iter: 71600, loss: 30.42486000061035
epoch: 1, iter: 71700, loss: 30.71780014038086
epoch: 1, iter: 71800, loss: 30.28960418701172
epoch: 1, iter: 71900, loss: 30.402591705322266
epoch: 1, iter: 72000, loss: 30.8114013671875
epoch: 1, iteration: 72000, simlex-999: SpearmanrResult(correlation=0.16101441806274847, pvalue=5.521495930342118e-07), men: SpearmanrResult(correlation=0.1633857027791665, pvalue=5.814925831866898e-17), sim353: SpearmanrResult(correlation=0.25737857847818424, pvalue=3.3158227988787696e-06), nearest to monster: [&apos;monster&apos;, &apos;robot&apos;, &apos;clown&apos;, &apos;giant&apos;, &apos;triangle&apos;, &apos;killer&apos;, &apos;storyline&apos;, &apos;horn&apos;, &apos;bird&apos;, &apos;reed&apos;]

epoch: 1, iter: 72100, loss: 30.883159637451172
epoch: 1, iter: 72200, loss: 30.85186767578125
epoch: 1, iter: 72300, loss: 30.58966636657715
epoch: 1, iter: 72400, loss: 30.387969970703125
epoch: 1, iter: 72500, loss: 29.79990005493164
epoch: 1, iter: 72600, loss: 30.107318878173828
epoch: 1, iter: 72700, loss: 30.190916061401367
epoch: 1, iter: 72800, loss: 30.59153175354004
epoch: 1, iter: 72900, loss: 30.534543991088867
epoch: 1, iter: 73000, loss: 30.79871368408203
epoch: 1, iter: 73100, loss: 30.439369201660156
epoch: 1, iter: 73200, loss: 29.90771484375
epoch: 1, iter: 73300, loss: 29.77695083618164
epoch: 1, iter: 73400, loss: 30.999645233154297
epoch: 1, iter: 73500, loss: 30.230430603027344
epoch: 1, iter: 73600, loss: 30.236263275146484
epoch: 1, iter: 73700, loss: 29.803932189941406
epoch: 1, iter: 73800, loss: 30.766613006591797
epoch: 1, iter: 73900, loss: 30.48684310913086
epoch: 1, iter: 74000, loss: 30.37664222717285
epoch: 1, iteration: 74000, simlex-999: SpearmanrResult(correlation=0.1617221261012147, pvalue=4.916430156015046e-07), men: SpearmanrResult(correlation=0.16269528376899162, pvalue=7.876945371094953e-17), sim353: SpearmanrResult(correlation=0.2568775911038176, pvalue=3.4697706327940896e-06), nearest to monster: [&apos;monster&apos;, &apos;giant&apos;, &apos;clown&apos;, &apos;robot&apos;, &apos;triangle&apos;, &apos;killer&apos;, &apos;horn&apos;, &apos;storyline&apos;, &apos;bird&apos;, &apos;pen&apos;]

epoch: 1, iter: 74100, loss: 30.254886627197266
epoch: 1, iter: 74200, loss: 29.888710021972656
epoch: 1, iter: 74300, loss: 30.417236328125
epoch: 1, iter: 74400, loss: 30.457595825195312
epoch: 1, iter: 74500, loss: 31.00020980834961
epoch: 1, iter: 74600, loss: 30.30846405029297
epoch: 1, iter: 74700, loss: 30.387718200683594
epoch: 1, iter: 74800, loss: 30.376087188720703
epoch: 1, iter: 74900, loss: 30.061664581298828
epoch: 1, iter: 75000, loss: 30.370288848876953
epoch: 1, iter: 75100, loss: 30.63956642150879
epoch: 1, iter: 75200, loss: 30.442768096923828
epoch: 1, iter: 75300, loss: 30.677764892578125
epoch: 1, iter: 75400, loss: 30.916812896728516
epoch: 1, iter: 75500, loss: 30.47184944152832
epoch: 1, iter: 75600, loss: 30.52167510986328
epoch: 1, iter: 75700, loss: 30.56326675415039
epoch: 1, iter: 75800, loss: 29.92554473876953
epoch: 1, iter: 75900, loss: 30.573081970214844
epoch: 1, iter: 76000, loss: 30.38399314880371
epoch: 1, iteration: 76000, simlex-999: SpearmanrResult(correlation=0.16169187742526095, pvalue=4.940931234078511e-07), men: SpearmanrResult(correlation=0.1633295111707588, pvalue=5.960650109011135e-17), sim353: SpearmanrResult(correlation=0.25485181087160225, pvalue=4.16468278962377e-06), nearest to monster: [&apos;monster&apos;, &apos;triangle&apos;, &apos;robot&apos;, &apos;clown&apos;, &apos;giant&apos;, &apos;storyline&apos;, &apos;killer&apos;, &apos;pen&apos;, &apos;cow&apos;, &apos;shield&apos;]

epoch: 1, iter: 76100, loss: 30.317106246948242
epoch: 1, iter: 76200, loss: 30.010608673095703
epoch: 1, iter: 76300, loss: 29.795989990234375
epoch: 1, iter: 76400, loss: 29.63566780090332
epoch: 1, iter: 76500, loss: 30.61663055419922
epoch: 1, iter: 76600, loss: 30.317100524902344
epoch: 1, iter: 76700, loss: 30.912708282470703
epoch: 1, iter: 76800, loss: 30.493019104003906
epoch: 1, iter: 76900, loss: 30.66164779663086
epoch: 1, iter: 77000, loss: 30.15361213684082
epoch: 1, iter: 77100, loss: 30.159067153930664
epoch: 1, iter: 77200, loss: 30.553518295288086
epoch: 1, iter: 77300, loss: 30.48749542236328
epoch: 1, iter: 77400, loss: 30.554759979248047
epoch: 1, iter: 77500, loss: 30.53312110900879
epoch: 1, iter: 77600, loss: 30.761314392089844
epoch: 1, iter: 77700, loss: 31.00925064086914
epoch: 1, iter: 77800, loss: 30.383312225341797
epoch: 1, iter: 77900, loss: 30.3116455078125
epoch: 1, iter: 78000, loss: 30.420455932617188
epoch: 1, iteration: 78000, simlex-999: SpearmanrResult(correlation=0.16207894522857358, pvalue=4.6360979168089965e-07), men: SpearmanrResult(correlation=0.16372154472938574, pvalue=5.0143933981199564e-17), sim353: SpearmanrResult(correlation=0.2544614698707922, pvalue=4.313024622770899e-06), nearest to monster: [&apos;monster&apos;, &apos;triangle&apos;, &apos;robot&apos;, &apos;clown&apos;, &apos;giant&apos;, &apos;killer&apos;, &apos;pen&apos;, &apos;cow&apos;, &apos;rod&apos;, &apos;bird&apos;]

epoch: 1, iter: 78100, loss: 30.672090530395508
epoch: 1, iter: 78200, loss: 30.56073760986328
epoch: 1, iter: 78300, loss: 30.491344451904297
epoch: 1, iter: 78400, loss: 31.016584396362305
epoch: 1, iter: 78500, loss: 30.333904266357422
epoch: 1, iter: 78600, loss: 30.13150405883789
epoch: 1, iter: 78700, loss: 30.275697708129883
epoch: 1, iter: 78800, loss: 29.988712310791016
epoch: 1, iter: 78900, loss: 30.521224975585938
epoch: 1, iter: 79000, loss: 30.032791137695312
epoch: 1, iter: 79100, loss: 30.854816436767578
epoch: 1, iter: 79200, loss: 30.634782791137695
epoch: 1, iter: 79300, loss: 30.44858741760254
epoch: 1, iter: 79400, loss: 30.231046676635742
epoch: 1, iter: 79500, loss: 30.371126174926758
epoch: 1, iter: 79600, loss: 29.989742279052734
epoch: 1, iter: 79700, loss: 30.68444061279297
epoch: 1, iter: 79800, loss: 30.262882232666016
epoch: 1, iter: 79900, loss: 30.341434478759766
epoch: 1, iter: 80000, loss: 29.98185920715332
epoch: 1, iteration: 80000, simlex-999: SpearmanrResult(correlation=0.16252507166083122, pvalue=4.3072065008154985e-07), men: SpearmanrResult(correlation=0.1643962621991115, pvalue=3.720276950472103e-17), sim353: SpearmanrResult(correlation=0.25403922241150295, pvalue=4.479160666059968e-06), nearest to monster: [&apos;monster&apos;, &apos;triangle&apos;, &apos;robot&apos;, &apos;clown&apos;, &apos;giant&apos;, &apos;killer&apos;, &apos;rod&apos;, &apos;pen&apos;, &apos;bull&apos;, &apos;bird&apos;]

epoch: 1, iter: 80100, loss: 30.37212371826172
epoch: 1, iter: 80200, loss: 30.76333999633789
epoch: 1, iter: 80300, loss: 30.685781478881836
epoch: 1, iter: 80400, loss: 30.539093017578125
epoch: 1, iter: 80500, loss: 30.19442367553711
epoch: 1, iter: 80600, loss: 31.10211944580078
epoch: 1, iter: 80700, loss: 30.563905715942383
epoch: 1, iter: 80800, loss: 30.892704010009766
epoch: 1, iter: 80900, loss: 30.355934143066406
epoch: 1, iter: 81000, loss: 30.444995880126953
epoch: 1, iter: 81100, loss: 29.747629165649414
epoch: 1, iter: 81200, loss: 30.422718048095703
epoch: 1, iter: 81300, loss: 30.607858657836914
epoch: 1, iter: 81400, loss: 30.45087432861328
epoch: 1, iter: 81500, loss: 30.621387481689453
epoch: 1, iter: 81600, loss: 30.56511878967285
epoch: 1, iter: 81700, loss: 30.036951065063477
epoch: 1, iter: 81800, loss: 29.739484786987305
epoch: 1, iter: 81900, loss: 30.102828979492188
epoch: 1, iter: 82000, loss: 30.472938537597656
epoch: 1, iteration: 82000, simlex-999: SpearmanrResult(correlation=0.162995795015982, pvalue=3.984582727922843e-07), men: SpearmanrResult(correlation=0.16521711507501075, pvalue=2.5829343239938948e-17), sim353: SpearmanrResult(correlation=0.2553242503620664, pvalue=3.9916456335010605e-06), nearest to monster: [&apos;monster&apos;, &apos;triangle&apos;, &apos;giant&apos;, &apos;clown&apos;, &apos;robot&apos;, &apos;pen&apos;, &apos;cow&apos;, &apos;killer&apos;, &apos;bird&apos;, &apos;hammer&apos;]

epoch: 1, iter: 82100, loss: 30.87786102294922
epoch: 1, iter: 82200, loss: 30.369300842285156
epoch: 1, iter: 82300, loss: 30.81732749938965
epoch: 1, iter: 82400, loss: 30.576255798339844
epoch: 1, iter: 82500, loss: 30.190576553344727
epoch: 1, iter: 82600, loss: 30.46633529663086
epoch: 1, iter: 82700, loss: 30.395225524902344
epoch: 1, iter: 82800, loss: 30.413116455078125
epoch: 1, iter: 82900, loss: 30.11839485168457
epoch: 1, iter: 83000, loss: 30.749446868896484
epoch: 1, iter: 83100, loss: 30.579679489135742
epoch: 1, iter: 83200, loss: 30.242351531982422
epoch: 1, iter: 83300, loss: 30.643089294433594
epoch: 1, iter: 83400, loss: 31.025341033935547
epoch: 1, iter: 83500, loss: 30.11600112915039
epoch: 1, iter: 83600, loss: 30.297754287719727
epoch: 1, iter: 83700, loss: 30.28591537475586
epoch: 1, iter: 83800, loss: 30.40618896484375
epoch: 1, iter: 83900, loss: 30.353286743164062
epoch: 1, iter: 84000, loss: 29.93646240234375
epoch: 1, iteration: 84000, simlex-999: SpearmanrResult(correlation=0.16359501384641933, pvalue=3.6074490269985206e-07), men: SpearmanrResult(correlation=0.16651686883970906, pvalue=1.4439073198647016e-17), sim353: SpearmanrResult(correlation=0.2544400123109962, pvalue=4.321323577205728e-06), nearest to monster: [&apos;monster&apos;, &apos;triangle&apos;, &apos;clown&apos;, &apos;robot&apos;, &apos;killer&apos;, &apos;giant&apos;, &apos;pen&apos;, &apos;hammer&apos;, &apos;bull&apos;, &apos;cow&apos;]

epoch: 1, iter: 84100, loss: 30.29789161682129
epoch: 1, iter: 84200, loss: 30.246540069580078
epoch: 1, iter: 84300, loss: 30.901538848876953
epoch: 1, iter: 84400, loss: 30.117076873779297
epoch: 1, iter: 84500, loss: 30.69649887084961
epoch: 1, iter: 84600, loss: 30.63212776184082
epoch: 1, iter: 84700, loss: 29.883480072021484
epoch: 1, iter: 84800, loss: 30.262691497802734
epoch: 1, iter: 84900, loss: 29.80817413330078
epoch: 1, iter: 85000, loss: 30.048599243164062
epoch: 1, iter: 85100, loss: 29.657894134521484
epoch: 1, iter: 85200, loss: 30.541133880615234
epoch: 1, iter: 85300, loss: 29.884437561035156
epoch: 1, iter: 85400, loss: 29.609167098999023
epoch: 1, iter: 85500, loss: 30.771604537963867
epoch: 1, iter: 85600, loss: 30.919052124023438
epoch: 1, iter: 85700, loss: 30.89596939086914
epoch: 1, iter: 85800, loss: 30.242305755615234
epoch: 1, iter: 85900, loss: 30.13596534729004
epoch: 1, iter: 86000, loss: 30.44934844970703
epoch: 1, iteration: 86000, simlex-999: SpearmanrResult(correlation=0.1631675148083149, pvalue=3.8727894368921164e-07), men: SpearmanrResult(correlation=0.1671210510930885, pvalue=1.1001130410358958e-17), sim353: SpearmanrResult(correlation=0.25503748920366215, pvalue=4.095839170272771e-06), nearest to monster: [&apos;monster&apos;, &apos;triangle&apos;, &apos;clown&apos;, &apos;robot&apos;, &apos;killer&apos;, &apos;giant&apos;, &apos;bull&apos;, &apos;pen&apos;, &apos;cow&apos;, &apos;demon&apos;]

epoch: 1, iter: 86100, loss: 30.831403732299805
epoch: 1, iter: 86200, loss: 30.484277725219727
epoch: 1, iter: 86300, loss: 30.60747718811035
epoch: 1, iter: 86400, loss: 30.155363082885742
epoch: 1, iter: 86500, loss: 30.28110122680664
epoch: 1, iter: 86600, loss: 30.374900817871094
epoch: 1, iter: 86700, loss: 30.804969787597656
epoch: 1, iter: 86800, loss: 30.20755958557129
epoch: 1, iter: 86900, loss: 30.167919158935547
epoch: 1, iter: 87000, loss: 30.547744750976562
epoch: 1, iter: 87100, loss: 30.687185287475586
epoch: 1, iter: 87200, loss: 30.32683563232422
epoch: 1, iter: 87300, loss: 30.641101837158203
epoch: 1, iter: 87400, loss: 30.987831115722656
epoch: 1, iter: 87500, loss: 30.438377380371094
epoch: 1, iter: 87600, loss: 30.0216007232666
epoch: 1, iter: 87700, loss: 30.663925170898438
epoch: 1, iter: 87800, loss: 30.71135711669922
epoch: 1, iter: 87900, loss: 30.71870994567871
epoch: 1, iter: 88000, loss: 30.205699920654297
epoch: 1, iteration: 88000, simlex-999: SpearmanrResult(correlation=0.16425151824623327, pvalue=3.233772330038919e-07), men: SpearmanrResult(correlation=0.1685696359455271, pvalue=5.707930050658737e-18), sim353: SpearmanrResult(correlation=0.2556407027221877, pvalue=3.87959947193494e-06), nearest to monster: [&apos;monster&apos;, &apos;triangle&apos;, &apos;giant&apos;, &apos;clown&apos;, &apos;bull&apos;, &apos;robot&apos;, &apos;killer&apos;, &apos;pen&apos;, &apos;demon&apos;, &apos;hammer&apos;]

epoch: 1, iter: 88100, loss: 30.927677154541016
epoch: 1, iter: 88200, loss: 30.55514144897461
epoch: 1, iter: 88300, loss: 30.23815155029297
epoch: 1, iter: 88400, loss: 30.708179473876953
epoch: 1, iter: 88500, loss: 30.492023468017578
epoch: 1, iter: 88600, loss: 30.046836853027344
epoch: 1, iter: 88700, loss: 29.993240356445312
epoch: 1, iter: 88800, loss: 30.803783416748047
epoch: 1, iter: 88900, loss: 30.327960968017578
epoch: 1, iter: 89000, loss: 29.940567016601562
epoch: 1, iter: 89100, loss: 30.143478393554688
epoch: 1, iter: 89200, loss: 31.123132705688477
epoch: 1, iter: 89300, loss: 30.397336959838867
epoch: 1, iter: 89400, loss: 30.187965393066406
epoch: 1, iter: 89500, loss: 30.048961639404297
epoch: 1, iter: 89600, loss: 30.07036590576172
epoch: 1, iter: 89700, loss: 30.79546356201172
epoch: 1, iter: 89800, loss: 29.865171432495117
epoch: 1, iter: 89900, loss: 30.40514373779297
epoch: 1, iter: 90000, loss: 30.69794464111328
epoch: 1, iteration: 90000, simlex-999: SpearmanrResult(correlation=0.16514616029140247, pvalue=2.784127116501988e-07), men: SpearmanrResult(correlation=0.16954415194873998, pvalue=3.658872001083876e-18), sim353: SpearmanrResult(correlation=0.2555836069543828, pvalue=3.899590976356642e-06), nearest to monster: [&apos;monster&apos;, &apos;triangle&apos;, &apos;giant&apos;, &apos;bull&apos;, &apos;clown&apos;, &apos;robot&apos;, &apos;killer&apos;, &apos;hammer&apos;, &apos;cow&apos;, &apos;demon&apos;]

epoch: 1, iter: 90100, loss: 30.630199432373047
epoch: 1, iter: 90200, loss: 30.46442222595215
epoch: 1, iter: 90300, loss: 30.596553802490234
epoch: 1, iter: 90400, loss: 30.49251937866211
epoch: 1, iter: 90500, loss: 30.104934692382812
epoch: 1, iter: 90600, loss: 29.840747833251953
epoch: 1, iter: 90700, loss: 30.267772674560547
epoch: 1, iter: 90800, loss: 30.190765380859375
epoch: 1, iter: 90900, loss: 30.228517532348633
epoch: 1, iter: 91000, loss: 29.798009872436523
epoch: 1, iter: 91100, loss: 30.7586669921875
epoch: 1, iter: 91200, loss: 30.214977264404297
epoch: 1, iter: 91300, loss: 30.565635681152344
epoch: 1, iter: 91400, loss: 30.357879638671875
epoch: 1, iter: 91500, loss: 30.263957977294922
epoch: 1, iter: 91600, loss: 30.030994415283203
epoch: 1, iter: 91700, loss: 30.208396911621094
epoch: 1, iter: 91800, loss: 29.890247344970703
epoch: 1, iter: 91900, loss: 30.48164176940918
epoch: 1, iter: 92000, loss: 30.354490280151367
epoch: 1, iteration: 92000, simlex-999: SpearmanrResult(correlation=0.1653364321742003, pvalue=2.696593345422066e-07), men: SpearmanrResult(correlation=0.1688240673142005, pvalue=5.083536220386492e-18), sim353: SpearmanrResult(correlation=0.259105445573068, pvalue=2.8336170297537425e-06), nearest to monster: [&apos;monster&apos;, &apos;giant&apos;, &apos;triangle&apos;, &apos;clown&apos;, &apos;robot&apos;, &apos;bull&apos;, &apos;cow&apos;, &apos;demon&apos;, &apos;reed&apos;, &apos;rod&apos;]

epoch: 1, iter: 92100, loss: 30.68956756591797
epoch: 1, iter: 92200, loss: 30.422218322753906
epoch: 1, iter: 92300, loss: 30.578102111816406
epoch: 1, iter: 92400, loss: 30.647659301757812
epoch: 1, iter: 92500, loss: 30.45465850830078
epoch: 1, iter: 92600, loss: 30.237232208251953
epoch: 1, iter: 92700, loss: 30.611480712890625
epoch: 1, iter: 92800, loss: 30.068378448486328
epoch: 1, iter: 92900, loss: 30.449426651000977
epoch: 1, iter: 93000, loss: 30.518421173095703
epoch: 1, iter: 93100, loss: 30.57355308532715
epoch: 1, iter: 93200, loss: 30.197509765625
epoch: 1, iter: 93300, loss: 30.10198974609375
epoch: 1, iter: 93400, loss: 30.406944274902344
epoch: 1, iter: 93500, loss: 30.485071182250977
epoch: 1, iter: 93600, loss: 30.394821166992188
epoch: 1, iter: 93700, loss: 30.108301162719727
epoch: 1, iter: 93800, loss: 30.055160522460938
epoch: 1, iter: 93900, loss: 30.503068923950195
epoch: 1, iter: 94000, loss: 30.17776870727539
epoch: 1, iteration: 94000, simlex-999: SpearmanrResult(correlation=0.16452175580014577, pvalue=3.091050503921563e-07), men: SpearmanrResult(correlation=0.17031641900147176, pvalue=2.5673219220449705e-18), sim353: SpearmanrResult(correlation=0.25769279178858767, pvalue=3.22261890503053e-06), nearest to monster: [&apos;monster&apos;, &apos;giant&apos;, &apos;triangle&apos;, &apos;clown&apos;, &apos;robot&apos;, &apos;cow&apos;, &apos;bull&apos;, &apos;rod&apos;, &apos;reed&apos;, &apos;killer&apos;]

epoch: 1, iter: 94100, loss: 29.95470428466797
epoch: 1, iter: 94200, loss: 30.545040130615234
epoch: 1, iter: 94300, loss: 30.522565841674805
epoch: 1, iter: 94400, loss: 30.301044464111328
epoch: 1, iter: 94500, loss: 30.171085357666016
epoch: 1, iter: 94600, loss: 29.99677276611328
epoch: 1, iter: 94700, loss: 30.42511558532715
epoch: 1, iter: 94800, loss: 29.896169662475586
epoch: 1, iter: 94900, loss: 30.412710189819336
epoch: 1, iter: 95000, loss: 30.261638641357422
epoch: 1, iter: 95100, loss: 30.734485626220703
epoch: 1, iter: 95200, loss: 30.323421478271484
epoch: 1, iter: 95300, loss: 30.30504608154297
epoch: 1, iter: 95400, loss: 30.439306259155273
epoch: 1, iter: 95500, loss: 30.284433364868164
epoch: 1, iter: 95600, loss: 30.344554901123047
epoch: 1, iter: 95700, loss: 31.00472068786621
epoch: 1, iter: 95800, loss: 30.583446502685547
epoch: 1, iter: 95900, loss: 30.249595642089844
epoch: 1, iter: 96000, loss: 30.130508422851562
epoch: 1, iteration: 96000, simlex-999: SpearmanrResult(correlation=0.1658134194270542, pvalue=2.4886654321700024e-07), men: SpearmanrResult(correlation=0.16998066506649745, pvalue=2.9954700943620716e-18), sim353: SpearmanrResult(correlation=0.2615532866770961, pvalue=2.2634047025810973e-06), nearest to monster: [&apos;monster&apos;, &apos;giant&apos;, &apos;triangle&apos;, &apos;clown&apos;, &apos;robot&apos;, &apos;cow&apos;, &apos;storyline&apos;, &apos;demon&apos;, &apos;killer&apos;, &apos;mine&apos;]

epoch: 1, iter: 96100, loss: 30.80952262878418
epoch: 1, iter: 96200, loss: 30.514129638671875
epoch: 1, iter: 96300, loss: 30.41751480102539
epoch: 1, iter: 96400, loss: 30.401996612548828
epoch: 1, iter: 96500, loss: 30.39910125732422
epoch: 1, iter: 96600, loss: 29.805984497070312
epoch: 1, iter: 96700, loss: 29.829578399658203
epoch: 1, iter: 96800, loss: 30.401958465576172
epoch: 1, iter: 96900, loss: 30.372657775878906
epoch: 1, iter: 97000, loss: 30.669193267822266
epoch: 1, iter: 97100, loss: 30.45154571533203
epoch: 1, iter: 97200, loss: 30.4871883392334
epoch: 1, iter: 97300, loss: 30.424468994140625
epoch: 1, iter: 97400, loss: 30.36046600341797
epoch: 1, iter: 97500, loss: 30.400753021240234
epoch: 1, iter: 97600, loss: 30.647533416748047
epoch: 1, iter: 97700, loss: 30.4141788482666
epoch: 1, iter: 97800, loss: 30.66451644897461
epoch: 1, iter: 97900, loss: 30.682424545288086
epoch: 1, iter: 98000, loss: 30.974720001220703
epoch: 1, iteration: 98000, simlex-999: SpearmanrResult(correlation=0.1656781876945656, pvalue=2.5459901947414803e-07), men: SpearmanrResult(correlation=0.17076454997186605, pvalue=2.0886359821470924e-18), sim353: SpearmanrResult(correlation=0.2634260651785924, pvalue=1.9030239792892506e-06), nearest to monster: [&apos;monster&apos;, &apos;giant&apos;, &apos;triangle&apos;, &apos;robot&apos;, &apos;clown&apos;, &apos;cow&apos;, &apos;storyline&apos;, &apos;hammer&apos;, &apos;slogan&apos;, &apos;mine&apos;]

epoch: 1, iter: 98100, loss: 29.779621124267578
epoch: 1, iter: 98200, loss: 30.314851760864258
epoch: 1, iter: 98300, loss: 30.17748260498047
epoch: 1, iter: 98400, loss: 30.060449600219727
epoch: 1, iter: 98500, loss: 30.29900550842285
epoch: 1, iter: 98600, loss: 30.583925247192383
epoch: 1, iter: 98700, loss: 30.511886596679688
epoch: 1, iter: 98800, loss: 29.978679656982422
epoch: 1, iter: 98900, loss: 30.08024787902832
epoch: 1, iter: 99000, loss: 29.74579620361328
epoch: 1, iter: 99100, loss: 30.44879722595215
epoch: 1, iter: 99200, loss: 30.379261016845703
epoch: 1, iter: 99300, loss: 29.564411163330078
epoch: 1, iter: 99400, loss: 30.413551330566406
epoch: 1, iter: 99500, loss: 29.98810386657715
epoch: 1, iter: 99600, loss: 30.30841827392578
epoch: 1, iter: 99700, loss: 30.51578140258789
epoch: 1, iter: 99800, loss: 30.445234298706055
epoch: 1, iter: 99900, loss: 30.237821578979492
epoch: 1, iter: 100000, loss: 30.199050903320312
epoch: 1, iteration: 100000, simlex-999: SpearmanrResult(correlation=0.16715232584964468, pvalue=1.9843265854812782e-07), men: SpearmanrResult(correlation=0.17124287805386168, pvalue=1.6747013256150605e-18), sim353: SpearmanrResult(correlation=0.26460604437989393, pvalue=1.70487547849196e-06), nearest to monster: [&apos;monster&apos;, &apos;giant&apos;, &apos;triangle&apos;, &apos;robot&apos;, &apos;clown&apos;, &apos;storyline&apos;, &apos;cow&apos;, &apos;slogan&apos;, &apos;hammer&apos;, &apos;melody&apos;]

epoch: 1, iter: 100100, loss: 30.333574295043945
epoch: 1, iter: 100200, loss: 30.55845832824707
epoch: 1, iter: 100300, loss: 30.75680160522461
epoch: 1, iter: 100400, loss: 30.388769149780273
epoch: 1, iter: 100500, loss: 30.058666229248047
epoch: 1, iter: 100600, loss: 30.498638153076172
epoch: 1, iter: 100700, loss: 30.53042984008789
epoch: 1, iter: 100800, loss: 30.348159790039062
epoch: 1, iter: 100900, loss: 30.638376235961914
epoch: 1, iter: 101000, loss: 30.172426223754883
epoch: 1, iter: 101100, loss: 30.896692276000977
epoch: 1, iter: 101200, loss: 30.448551177978516
epoch: 1, iter: 101300, loss: 30.56205177307129
epoch: 1, iter: 101400, loss: 30.40060043334961
epoch: 1, iter: 101500, loss: 30.31627655029297
epoch: 1, iter: 101600, loss: 30.167484283447266
epoch: 1, iter: 101700, loss: 30.0115966796875
epoch: 1, iter: 101800, loss: 30.624086380004883
epoch: 1, iter: 101900, loss: 30.172834396362305
epoch: 1, iter: 102000, loss: 30.562152862548828
epoch: 1, iteration: 102000, simlex-999: SpearmanrResult(correlation=0.1670230978748453, pvalue=2.028337490988046e-07), men: SpearmanrResult(correlation=0.1713810110701936, pvalue=1.5710283985855815e-18), sim353: SpearmanrResult(correlation=0.26441292634173025, pvalue=1.7358951566969633e-06), nearest to monster: [&apos;monster&apos;, &apos;giant&apos;, &apos;triangle&apos;, &apos;robot&apos;, &apos;clown&apos;, &apos;storyline&apos;, &apos;cow&apos;, &apos;slogan&apos;, &apos;hammer&apos;, &apos;finger&apos;]

epoch: 1, iter: 102100, loss: 30.55624008178711
epoch: 1, iter: 102200, loss: 30.41084098815918
epoch: 1, iter: 102300, loss: 30.26647186279297
epoch: 1, iter: 102400, loss: 30.464323043823242
epoch: 1, iter: 102500, loss: 30.623504638671875
epoch: 1, iter: 102600, loss: 31.13153839111328
epoch: 1, iter: 102700, loss: 30.719829559326172
epoch: 1, iter: 102800, loss: 30.62751007080078
epoch: 1, iter: 102900, loss: 30.385581970214844
epoch: 1, iter: 103000, loss: 31.12250328063965
epoch: 1, iter: 103100, loss: 30.452640533447266
epoch: 1, iter: 103200, loss: 30.76857566833496
epoch: 1, iter: 103300, loss: 30.577342987060547
epoch: 1, iter: 103400, loss: 30.7468318939209
epoch: 1, iter: 103500, loss: 30.40059471130371
epoch: 1, iter: 103600, loss: 30.383655548095703
epoch: 1, iter: 103700, loss: 30.386371612548828
epoch: 1, iter: 103800, loss: 30.170000076293945
epoch: 1, iter: 103900, loss: 30.548282623291016
epoch: 1, iter: 104000, loss: 29.733184814453125
epoch: 1, iteration: 104000, simlex-999: SpearmanrResult(correlation=0.16893464777171635, pvalue=1.4637531128005816e-07), men: SpearmanrResult(correlation=0.17193637752975074, pvalue=1.2144154074333336e-18), sim353: SpearmanrResult(correlation=0.2644817771205538, pvalue=1.72477455415369e-06), nearest to monster: [&apos;monster&apos;, &apos;giant&apos;, &apos;robot&apos;, &apos;triangle&apos;, &apos;clown&apos;, &apos;storyline&apos;, &apos;hammer&apos;, &apos;bull&apos;, &apos;slogan&apos;, &apos;cow&apos;]

epoch: 1, iter: 104100, loss: 30.98358917236328
epoch: 1, iter: 104200, loss: 30.25103759765625
epoch: 1, iter: 104300, loss: 30.568321228027344
epoch: 1, iter: 104400, loss: 30.607681274414062
epoch: 1, iter: 104500, loss: 31.158170700073242
epoch: 1, iter: 104600, loss: 30.96269416809082
epoch: 1, iter: 104700, loss: 30.154834747314453
epoch: 1, iter: 104800, loss: 30.38031768798828
epoch: 1, iter: 104900, loss: 30.168392181396484
epoch: 1, iter: 105000, loss: 30.190406799316406
epoch: 1, iter: 105100, loss: 30.59976577758789
epoch: 1, iter: 105200, loss: 30.05843734741211
epoch: 1, iter: 105300, loss: 30.54991340637207
epoch: 1, iter: 105400, loss: 30.098140716552734
epoch: 1, iter: 105500, loss: 30.414899826049805
epoch: 1, iter: 105600, loss: 30.54265594482422
epoch: 1, iter: 105700, loss: 30.9292049407959
epoch: 1, iter: 105800, loss: 30.35509490966797
epoch: 1, iter: 105900, loss: 30.05394172668457
epoch: 1, iter: 106000, loss: 30.226543426513672
epoch: 1, iteration: 106000, simlex-999: SpearmanrResult(correlation=0.1684798203405728, pvalue=1.5824218505055718e-07), men: SpearmanrResult(correlation=0.1734020098336969, pvalue=6.130244612215065e-19), sim353: SpearmanrResult(correlation=0.2653148902030666, pvalue=1.595499439172943e-06), nearest to monster: [&apos;monster&apos;, &apos;giant&apos;, &apos;robot&apos;, &apos;clown&apos;, &apos;triangle&apos;, &apos;hammer&apos;, &apos;bull&apos;, &apos;storyline&apos;, &apos;slogan&apos;, &apos;cow&apos;]

epoch: 1, iter: 106100, loss: 30.15334701538086
epoch: 1, iter: 106200, loss: 30.41736602783203
epoch: 1, iter: 106300, loss: 30.16756820678711
epoch: 1, iter: 106400, loss: 30.476585388183594
epoch: 1, iter: 106500, loss: 31.16946792602539
epoch: 1, iter: 106600, loss: 30.209535598754883
epoch: 1, iter: 106700, loss: 30.48019790649414
epoch: 1, iter: 106800, loss: 29.56353187561035
epoch: 1, iter: 106900, loss: 30.35293960571289
epoch: 1, iter: 107000, loss: 30.54877471923828
epoch: 1, iter: 107100, loss: 29.95642852783203
epoch: 1, iter: 107200, loss: 29.863868713378906
epoch: 1, iter: 107300, loss: 30.54117202758789
epoch: 1, iter: 107400, loss: 30.564319610595703
epoch: 1, iter: 107500, loss: 30.289745330810547
epoch: 1, iter: 107600, loss: 30.34166717529297
epoch: 1, iter: 107700, loss: 30.228656768798828
epoch: 1, iter: 107800, loss: 30.19826889038086
epoch: 1, iter: 107900, loss: 29.98631477355957
epoch: 1, iter: 108000, loss: 30.534252166748047
epoch: 1, iteration: 108000, simlex-999: SpearmanrResult(correlation=0.16809445029314032, pvalue=1.6901904840218488e-07), men: SpearmanrResult(correlation=0.17424501907579965, pvalue=4.1259778445697578e-19), sim353: SpearmanrResult(correlation=0.2659931356800956, pvalue=1.4971527422559394e-06), nearest to monster: [&apos;monster&apos;, &apos;giant&apos;, &apos;robot&apos;, &apos;hammer&apos;, &apos;triangle&apos;, &apos;bull&apos;, &apos;clown&apos;, &apos;rod&apos;, &apos;storyline&apos;, &apos;slogan&apos;]

epoch: 1, iter: 108100, loss: 30.33737564086914
epoch: 1, iter: 108200, loss: 30.28573226928711
epoch: 1, iter: 108300, loss: 30.758136749267578
epoch: 1, iter: 108400, loss: 30.248483657836914
epoch: 1, iter: 108500, loss: 29.876144409179688
epoch: 1, iter: 108600, loss: 30.282499313354492
epoch: 1, iter: 108700, loss: 30.403133392333984
epoch: 1, iter: 108800, loss: 30.222488403320312
epoch: 1, iter: 108900, loss: 30.056053161621094
epoch: 1, iter: 109000, loss: 30.39307403564453
epoch: 1, iter: 109100, loss: 30.315738677978516
epoch: 1, iter: 109200, loss: 30.37272071838379
epoch: 1, iter: 109300, loss: 30.462688446044922
epoch: 1, iter: 109400, loss: 30.42410659790039
epoch: 1, iter: 109500, loss: 30.414215087890625
epoch: 1, iter: 109600, loss: 30.320392608642578
epoch: 1, iter: 109700, loss: 30.297039031982422
epoch: 1, iter: 109800, loss: 30.644512176513672
epoch: 1, iter: 109900, loss: 30.753337860107422
epoch: 1, iter: 110000, loss: 30.265583038330078
epoch: 1, iteration: 110000, simlex-999: SpearmanrResult(correlation=0.16907060515792793, pvalue=1.4299814068693673e-07), men: SpearmanrResult(correlation=0.17510555147417187, pvalue=2.7485794885676823e-19), sim353: SpearmanrResult(correlation=0.267975254442813, pvalue=1.241886586688811e-06), nearest to monster: [&apos;monster&apos;, &apos;giant&apos;, &apos;robot&apos;, &apos;bull&apos;, &apos;hammer&apos;, &apos;clown&apos;, &apos;rod&apos;, &apos;triangle&apos;, &apos;killer&apos;, &apos;reads&apos;]

epoch: 1, iter: 110100, loss: 30.272464752197266
epoch: 1, iter: 110200, loss: 30.38793182373047
epoch: 1, iter: 110300, loss: 30.590267181396484
epoch: 1, iter: 110400, loss: 30.97867202758789
epoch: 1, iter: 110500, loss: 30.195693969726562
epoch: 1, iter: 110600, loss: 30.050588607788086
epoch: 1, iter: 110700, loss: 30.010971069335938
epoch: 1, iter: 110800, loss: 30.200347900390625
epoch: 1, iter: 110900, loss: 30.716394424438477
epoch: 1, iter: 111000, loss: 30.02122688293457
epoch: 1, iter: 111100, loss: 30.24693489074707
epoch: 1, iter: 111200, loss: 30.085987091064453
epoch: 1, iter: 111300, loss: 30.499698638916016
epoch: 1, iter: 111400, loss: 30.532825469970703
epoch: 1, iter: 111500, loss: 29.860715866088867
epoch: 1, iter: 111600, loss: 30.18459701538086
epoch: 1, iter: 111700, loss: 30.063079833984375
epoch: 1, iter: 111800, loss: 30.4438533782959
epoch: 1, iter: 111900, loss: 29.979290008544922
epoch: 1, iter: 112000, loss: 29.959312438964844
epoch: 1, iteration: 112000, simlex-999: SpearmanrResult(correlation=0.17045668307885228, pvalue=1.1259417092002108e-07), men: SpearmanrResult(correlation=0.17566254497426181, pvalue=2.110750452163048e-19), sim353: SpearmanrResult(correlation=0.2669130119391746, pvalue=1.3729997266364257e-06), nearest to monster: [&apos;monster&apos;, &apos;giant&apos;, &apos;bull&apos;, &apos;robot&apos;, &apos;clown&apos;, &apos;hammer&apos;, &apos;triangle&apos;, &apos;killer&apos;, &apos;reads&apos;, &apos;slogan&apos;]

epoch: 1, iter: 112100, loss: 30.033226013183594
epoch: 1, iter: 112200, loss: 29.982528686523438
epoch: 1, iter: 112300, loss: 30.576221466064453
epoch: 1, iter: 112400, loss: 30.227249145507812
epoch: 1, iter: 112500, loss: 30.463727951049805
epoch: 1, iter: 112600, loss: 30.63485336303711
epoch: 1, iter: 112700, loss: 30.5284423828125
epoch: 1, iter: 112800, loss: 30.665870666503906
epoch: 1, iter: 112900, loss: 29.814481735229492
epoch: 1, iter: 113000, loss: 30.23995590209961
epoch: 1, iter: 113100, loss: 30.29913330078125
epoch: 1, iter: 113200, loss: 30.609512329101562
epoch: 1, iter: 113300, loss: 30.66790771484375
epoch: 1, iter: 113400, loss: 30.355817794799805
epoch: 1, iter: 113500, loss: 30.777101516723633
epoch: 1, iter: 113600, loss: 29.784292221069336
epoch: 1, iter: 113700, loss: 30.44632339477539
epoch: 1, iter: 113800, loss: 30.35899543762207
epoch: 1, iter: 113900, loss: 30.091773986816406
epoch: 1, iter: 114000, loss: 30.235483169555664
epoch: 1, iteration: 114000, simlex-999: SpearmanrResult(correlation=0.17074223750408546, pvalue=1.0715770710689782e-07), men: SpearmanrResult(correlation=0.176259639644225, pvalue=1.5888720988857355e-19), sim353: SpearmanrResult(correlation=0.2676783937677228, pvalue=1.2772687113870497e-06), nearest to monster: [&apos;monster&apos;, &apos;giant&apos;, &apos;bull&apos;, &apos;robot&apos;, &apos;clown&apos;, &apos;hammer&apos;, &apos;killer&apos;, &apos;triangle&apos;, &apos;slogan&apos;, &apos;storyline&apos;]

epoch: 1, iter: 114100, loss: 31.208988189697266
epoch: 1, iter: 114200, loss: 30.55046844482422
epoch: 1, iter: 114300, loss: 29.999637603759766
epoch: 1, iter: 114400, loss: 29.898786544799805
epoch: 1, iter: 114500, loss: 30.682052612304688
epoch: 1, iter: 114600, loss: 30.83867835998535
epoch: 1, iter: 114700, loss: 30.36109733581543
epoch: 1, iter: 114800, loss: 30.97061538696289
epoch: 1, iter: 114900, loss: 30.502185821533203
epoch: 1, iter: 115000, loss: 30.31426239013672
epoch: 1, iter: 115100, loss: 30.291278839111328
epoch: 1, iter: 115200, loss: 29.921966552734375
epoch: 1, iter: 115300, loss: 30.482467651367188
epoch: 1, iter: 115400, loss: 30.56399917602539
epoch: 1, iter: 115500, loss: 30.21322250366211
epoch: 1, iter: 115600, loss: 30.300979614257812
epoch: 1, iter: 115700, loss: 30.243253707885742
epoch: 1, iter: 115800, loss: 30.047298431396484
epoch: 1, iter: 115900, loss: 30.42316436767578
epoch: 1, iter: 116000, loss: 30.36935043334961
epoch: 1, iteration: 116000, simlex-999: SpearmanrResult(correlation=0.17049890390424202, pvalue=1.1177389455317111e-07), men: SpearmanrResult(correlation=0.1761607002941838, pvalue=1.6655510704355566e-19), sim353: SpearmanrResult(correlation=0.26764518119690817, pvalue=1.2812867355896495e-06), nearest to monster: [&apos;monster&apos;, &apos;giant&apos;, &apos;bull&apos;, &apos;clown&apos;, &apos;robot&apos;, &apos;hammer&apos;, &apos;killer&apos;, &apos;triangle&apos;, &apos;vampire&apos;, &apos;demon&apos;]

epoch: 1, iter: 116100, loss: 29.956588745117188
epoch: 1, iter: 116200, loss: 30.239765167236328
epoch: 1, iter: 116300, loss: 30.056724548339844
epoch: 1, iter: 116400, loss: 30.337177276611328
epoch: 1, iter: 116500, loss: 30.554336547851562
epoch: 1, iter: 116600, loss: 30.801679611206055
epoch: 1, iter: 116700, loss: 30.705276489257812
epoch: 1, iter: 116800, loss: 30.503780364990234
epoch: 1, iter: 116900, loss: 29.62342643737793
epoch: 1, iter: 117000, loss: 30.29004669189453
epoch: 1, iter: 117100, loss: 30.506996154785156
epoch: 1, iter: 117200, loss: 30.6331787109375
epoch: 1, iter: 117300, loss: 30.65314483642578
epoch: 1, iter: 117400, loss: 30.795137405395508
epoch: 1, iter: 117500, loss: 30.28030776977539
epoch: 1, iter: 117600, loss: 30.351322174072266
epoch: 1, iter: 117700, loss: 30.542030334472656
epoch: 1, iter: 117800, loss: 30.361120223999023
epoch: 1, iter: 117900, loss: 30.456024169921875
epoch: 1, iter: 118000, loss: 30.537174224853516
epoch: 1, iteration: 118000, simlex-999: SpearmanrResult(correlation=0.17280547641221922, pvalue=7.475906386705914e-08), men: SpearmanrResult(correlation=0.17732985405232565, pvalue=9.526064095701539e-20), sim353: SpearmanrResult(correlation=0.2698694905041053, pvalue=1.037261623872224e-06), nearest to monster: [&apos;monster&apos;, &apos;giant&apos;, &apos;robot&apos;, &apos;clown&apos;, &apos;hammer&apos;, &apos;bull&apos;, &apos;killer&apos;, &apos;vampire&apos;, &apos;triangle&apos;, &apos;demon&apos;]

epoch: 1, iter: 118100, loss: 29.840484619140625
epoch: 1, iter: 118200, loss: 30.325119018554688
epoch: 1, iter: 118300, loss: 30.51473045349121
epoch: 1, iter: 118400, loss: 30.261699676513672
epoch: 1, iter: 118500, loss: 30.180068969726562
epoch: 1, iter: 118600, loss: 30.017879486083984
epoch: 1, iter: 118700, loss: 30.56424903869629
epoch: 1, iter: 118800, loss: 30.457590103149414
epoch: 1, iter: 118900, loss: 30.63213539123535
epoch: 1, iter: 119000, loss: 30.692546844482422
epoch: 1, iter: 119100, loss: 30.539554595947266
epoch: 1, iter: 119200, loss: 30.656726837158203
epoch: 1, iter: 119300, loss: 30.380685806274414
epoch: 1, iter: 119400, loss: 29.897314071655273
epoch: 1, iter: 119500, loss: 29.90090560913086</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">model.load_state_dict(torch.load(<span class="string">"embedding-&#123;&#125;.th"</span>.format(EMBEDDING_SIZE)))</span><br></pre></td></tr></table></figure>

<h2 id="在-MEN-和-Simplex-999-数据集上做评估"><a href="#在-MEN-和-Simplex-999-数据集上做评估" class="headerlink" title="在 MEN 和 Simplex-999 数据集上做评估"></a>在 MEN 和 Simplex-999 数据集上做评估</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">embedding_weights = model.input_embeddings()</span><br><span class="line">print(<span class="string">"simlex-999"</span>, evaluate(<span class="string">"simlex-999.txt"</span>, embedding_weights))</span><br><span class="line">print(<span class="string">"men"</span>, evaluate(<span class="string">"men.txt"</span>, embedding_weights))</span><br><span class="line">print(<span class="string">"wordsim353"</span>, evaluate(<span class="string">"wordsim353.csv"</span>, embedding_weights))</span><br></pre></td></tr></table></figure>

<pre><code>simlex-999 SpearmanrResult(correlation=0.17251697429101504, pvalue=7.863946056740345e-08)
men SpearmanrResult(correlation=0.1778096817088841, pvalue=7.565661657312768e-20)
wordsim353 SpearmanrResult(correlation=0.27153702278146635, pvalue=8.842165885381714e-07)</code></pre><h2 id="寻找nearest-neighbors"><a href="#寻找nearest-neighbors" class="headerlink" title="寻找nearest neighbors"></a>寻找nearest neighbors</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> word <span class="keyword">in</span> [<span class="string">"good"</span>, <span class="string">"fresh"</span>, <span class="string">"monster"</span>, <span class="string">"green"</span>, <span class="string">"like"</span>, <span class="string">"america"</span>, <span class="string">"chicago"</span>, <span class="string">"work"</span>, <span class="string">"computer"</span>, <span class="string">"language"</span>]:</span><br><span class="line">    print(word, find_nearest(word))</span><br></pre></td></tr></table></figure>

<pre><code>good [&apos;good&apos;, &apos;bad&apos;, &apos;perfect&apos;, &apos;hard&apos;, &apos;questions&apos;, &apos;alone&apos;, &apos;money&apos;, &apos;false&apos;, &apos;truth&apos;, &apos;experience&apos;]
fresh [&apos;fresh&apos;, &apos;grain&apos;, &apos;waste&apos;, &apos;cooling&apos;, &apos;lighter&apos;, &apos;dense&apos;, &apos;mild&apos;, &apos;sized&apos;, &apos;warm&apos;, &apos;steel&apos;]
monster [&apos;monster&apos;, &apos;giant&apos;, &apos;robot&apos;, &apos;hammer&apos;, &apos;clown&apos;, &apos;bull&apos;, &apos;demon&apos;, &apos;triangle&apos;, &apos;storyline&apos;, &apos;slogan&apos;]
green [&apos;green&apos;, &apos;blue&apos;, &apos;yellow&apos;, &apos;white&apos;, &apos;cross&apos;, &apos;orange&apos;, &apos;black&apos;, &apos;red&apos;, &apos;mountain&apos;, &apos;gold&apos;]
like [&apos;like&apos;, &apos;unlike&apos;, &apos;etc&apos;, &apos;whereas&apos;, &apos;animals&apos;, &apos;soft&apos;, &apos;amongst&apos;, &apos;similarly&apos;, &apos;bear&apos;, &apos;drink&apos;]
america [&apos;america&apos;, &apos;africa&apos;, &apos;korea&apos;, &apos;india&apos;, &apos;australia&apos;, &apos;turkey&apos;, &apos;pakistan&apos;, &apos;mexico&apos;, &apos;argentina&apos;, &apos;carolina&apos;]
chicago [&apos;chicago&apos;, &apos;boston&apos;, &apos;illinois&apos;, &apos;texas&apos;, &apos;london&apos;, &apos;indiana&apos;, &apos;massachusetts&apos;, &apos;florida&apos;, &apos;berkeley&apos;, &apos;michigan&apos;]
work [&apos;work&apos;, &apos;writing&apos;, &apos;job&apos;, &apos;marx&apos;, &apos;solo&apos;, &apos;label&apos;, &apos;recording&apos;, &apos;nietzsche&apos;, &apos;appearance&apos;, &apos;stage&apos;]
computer [&apos;computer&apos;, &apos;digital&apos;, &apos;electronic&apos;, &apos;audio&apos;, &apos;video&apos;, &apos;graphics&apos;, &apos;hardware&apos;, &apos;software&apos;, &apos;computers&apos;, &apos;program&apos;]
language [&apos;language&apos;, &apos;languages&apos;, &apos;alphabet&apos;, &apos;arabic&apos;, &apos;grammar&apos;, &apos;pronunciation&apos;, &apos;dialect&apos;, &apos;programming&apos;, &apos;chinese&apos;, &apos;spelling&apos;]</code></pre><h2 id="单词之间的关系"><a href="#单词之间的关系" class="headerlink" title="单词之间的关系"></a>单词之间的关系</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">man_idx = word_to_idx[<span class="string">"man"</span>] </span><br><span class="line">king_idx = word_to_idx[<span class="string">"king"</span>] </span><br><span class="line">woman_idx = word_to_idx[<span class="string">"woman"</span>]</span><br><span class="line">embedding = embedding_weights[woman_idx] - embedding_weights[man_idx] + embedding_weights[king_idx]</span><br><span class="line">cos_dis = np.array([scipy.spatial.distance.cosine(e, embedding) <span class="keyword">for</span> e <span class="keyword">in</span> embedding_weights])</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> cos_dis.argsort()[:<span class="number">20</span>]:</span><br><span class="line">    print(idx_to_word[i])</span><br></pre></td></tr></table></figure>

<pre><code>king
henry
charles
pope
queen
iii
prince
elizabeth
alexander
constantine
edward
son
iv
louis
emperor
mary
james
joseph
frederick
francis</code></pre>
        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2020/02/29/hello-world/">
                Hello World
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2020-02-29</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            <p>Welcome to <a href="https://hexo.io/" target="_blank" rel="noopener">Hexo</a>! This is your very first post. Check <a href="https://hexo.io/docs/" target="_blank" rel="noopener">documentation</a> for more info. If you get any problems when using Hexo, you can find the answer in <a href="https://hexo.io/docs/troubleshooting.html" target="_blank" rel="noopener">troubleshooting</a> or you can ask me on <a href="https://github.com/hexojs/hexo/issues" target="_blank" rel="noopener">GitHub</a>.</p>
<h2 id="Quick-Start"><a href="#Quick-Start" class="headerlink" title="Quick Start"></a>Quick Start</h2><h3 id="Create-a-new-post"><a href="#Create-a-new-post" class="headerlink" title="Create a new post"></a>Create a new post</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo new <span class="string">"My New Post"</span></span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/writing.html" target="_blank" rel="noopener">Writing</a></p>
<h3 id="Run-server"><a href="#Run-server" class="headerlink" title="Run server"></a>Run server</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo server</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/server.html" target="_blank" rel="noopener">Server</a></p>
<h3 id="Generate-static-files"><a href="#Generate-static-files" class="headerlink" title="Generate static files"></a>Generate static files</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo generate</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/generating.html" target="_blank" rel="noopener">Generating</a></p>
<h3 id="Deploy-to-remote-sites"><a href="#Deploy-to-remote-sites" class="headerlink" title="Deploy to remote sites"></a>Deploy to remote sites</h3><figure class="highlight bash"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ hexo deploy</span><br></pre></td></tr></table></figure>

<p>More info: <a href="https://hexo.io/docs/one-command-deployment.html" target="_blank" rel="noopener">Deployment</a></p>

        </div>
    

</div>
            
                
<div class="post">

    <div class="post-header index">
        <h1 class="title">
            <a href="/2020/02/28/My-New-Post/">
                My New Post
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2020-02-28</span>
            
            
            
        </div>
    </div>

    
        <div class="content">
            
        </div>
    

</div>
            
        </section>
    </div>
</div>



    <div class="row">
        <div class="col-sm-12">
            <div class="wrap-pagination">
                <a class="" href="/">
                    <i class="fa fa-chevron-left" aria-hidden="true"></i>
                </a>
                <a class="disabled" href="/">
                    <i class="fa fa-chevron-right" aria-hidden="true"></i>
                </a>
            </div>
        </div>
    </div>




</div>

<!-- Footer -->
<div class="push"></div>

<footer class="footer-content">
    <div class="container">
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-6 col-lg-6 footer-about">
                <h2>About</h2>
                <p>
                    This theme was developed by <a href="https://github.com/klugjo" target="_blank" rel="noopener">Jonathan Klughertz</a>. The source code is available on Github. Create Websites. Make Magic.
                </p>
            </div>
            
    <div class="col-xs-6 col-sm-6 col-md-3 col-lg-3 recent-posts">
        <h2>Recent Posts</h2>
        <ul>
            
            <li>
                <a class="footer-post" href="/2020/03/08/%E5%AF%B9%E7%A7%B0%E7%9F%A9%E9%98%B5summary/">对称矩阵summary</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2020/03/07/%E4%BA%8C%E5%8F%89%E6%A0%91%E9%81%8D%E5%8E%86/">二叉树遍历</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2020/03/07/Gram%E7%9F%A9%E9%98%B5/">Gram矩阵</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2020/03/07/Jupyter%20Notebook%20%E5%9C%A8%20macOS%20%E7%B3%BB%E7%BB%9F%E4%B8%8A%E7%9A%84%E5%BF%AB%E6%8D%B7%E9%94%AE/">Jupyter Notebook 在 macOS </a>
            </li>
            
        </ul>
    </div>



            
        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <ul class="list-inline footer-social-icons">
                    
                    <li class="list-inline-item">
                        <a href="https://github.com/klugjo/hexo-theme-alpha-dust" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-github"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://twitter.com/?lang=en" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-twitter"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://www.facebook.com/" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-facebook"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://www.instagram.com/" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-instagram"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://dribbble.com/" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-dribbble"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://plus.google.com/" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-google-plus"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://www.behance.net/" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-behance"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://500px.com/" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-500px"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="mailto:test@example.com">
                            <span class="footer-icon-container">
                                <i class="fa fa-envelope-o"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="\#">
                            <span class="footer-icon-container">
                                <i class="fa fa-rss"></i>
                            </span>
                        </a>
                    </li>
                    
                </ul>
            </div>
        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <div class="footer-copyright">
                    @Untitled. All right reserved | Design & Hexo <a href="http://www.codeblocq.com/" target="_blank" rel="noopener">Jonathan Klughertz</a>
                </div>
            </div>
        </div>
    </div>
</footer>

<!-- After footer scripts -->

<!-- jQuery -->
<script src="//code.jquery.com/jquery-2.1.4.min.js"></script>

<!-- Tween Max -->
<script src="//cdnjs.cloudflare.com/ajax/libs/gsap/1.18.5/TweenMax.min.js"></script>

<!-- Gallery -->
<script src="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.js" type="text/javascript" charset="utf-8"></script>

<!-- Custom JavaScript -->

<script src="/js/main.js"></script>


<!-- Disqus Comments -->



</body>

</html>