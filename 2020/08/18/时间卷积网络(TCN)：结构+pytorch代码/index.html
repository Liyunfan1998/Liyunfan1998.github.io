<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--Description-->
    
        <meta name="description" content="时间卷积网络(TCN)：结构+pytorch代码TCN  TCN(Temporal Convolutional Network)是由Shaojie Bai et al.提出的，paper地址：https://arxiv.org/pdf/1803.01271.pdf
  想要了解TCN，最好先知道CN">
    

    <!--Author-->
    
        <meta name="author" content="Yunfan Li">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="时间卷积网络(TCN)：结构+pytorch代码"/>
    

    <!--Open Graph Description-->
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content="Hexo"/>

    <!--Type page-->
    
        <meta property="og:type" content="article" />
    

    <!--Page Cover-->
    

    
        <meta name="twitter:card" content="summary" />
    
    
    

    <!-- Title -->
    
    <title>时间卷积网络(TCN)：结构+pytorch代码 - Hexo</title>

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.2/css/bootstrap.min.css" integrity="sha384-y3tfxAZXuh4HwSYylfB+J125MxIs6mR5FOHamPBG064zB+AFeWH94NdvaCBm8qnd" crossorigin="anonymous">

    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="//oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="//oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Gallery -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.css" type="text/css" rel="stylesheet" />

    <!-- Custom CSS -->
    
<link rel="stylesheet" href="/css/style.css">


    <!-- Google Analytics -->
    


<meta name="generator" content="Hexo 4.2.0"></head>


<body>

<div class="bg-gradient"></div>
<div class="bg-pattern"></div>

<!-- Menu -->
<!--Menu Links and Overlay-->
<div class="menu-bg">
    <div class="menu-container">
        <ul>
            
            <li class="menu-item">
                <a href="/archives">
                    Home
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/archives">
                    Archives
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/about.html">
                    About
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/tags">
                    Tags
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/categories">
                    Categories
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/contact.html">
                    Contact
                </a>
            </li>
            
        </ul>
    </div>
</div>

<!--Hamburger Icon-->
<nav>
    <a href="#menu"></a>
</nav>

<div class="container">

    <!-- Main Content -->
    <div class="row">
    <div class="col-sm-12">

        <!--Title and Logo-->
        <header>
    <div class="logo">
        <a href="/"><i class="logo-icon fa fa-cube" aria-hidden="true"></i></a>
        
    </div>
</header>

        <section class="main">
            
<div class="post">

    <div class="post-header">
        <h1 class="title">
            <a href="/2020/08/18/%E6%97%B6%E9%97%B4%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C(TCN)%EF%BC%9A%E7%BB%93%E6%9E%84+pytorch%E4%BB%A3%E7%A0%81/">
                时间卷积网络(TCN)：结构+pytorch代码
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2020-08-18</span>
            
            
            
        </div>
    </div>

    <div class="content">

        <!-- Gallery -->
        

        <!-- Post Content -->
        <h1 id="时间卷积网络-TCN-：结构-pytorch代码"><a href="#时间卷积网络-TCN-：结构-pytorch代码" class="headerlink" title="时间卷积网络(TCN)：结构+pytorch代码"></a>时间卷积网络(TCN)：结构+pytorch代码</h1><h2 id="TCN"><a href="#TCN" class="headerlink" title="TCN"></a>TCN</h2><p>  TCN(Temporal Convolutional Network)是由Shaojie Bai et al.提出的，paper地址：<a href="https://arxiv.org/pdf/1803.01271.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1803.01271.pdf</a></p>
<p>  想要了解TCN，最好先知道<a href="https://blog.csdn.net/Leon_winter/article/details/87728755" target="_blank" rel="noopener">CNN</a>和<a href="https://blog.csdn.net/Leon_winter/article/details/89089458" target="_blank" rel="noopener">RNN</a>。</p>
<p>  以往一旦提起sequence，或者存在时间序列的数据，想到的神经网络模型就是RNN及其变种LSTM、GRU等。在上面论文提到，很多工作表明，在RNN这个框架中，很难再找到新的模型，其效果可以在很多任务中超越LSTM，但是跳出RNN这个框架，paper作者展示了利用CNN衍生出的TCN结构就很容易在很多任务中取得超过LSTM、GRU的效果。当然paper作者也表示，TCN并不指代一种模型，更像是一种类似RNN的框架，paper作者渴望抛砖引玉，让更多人来探索挖掘这个框架的能力。</p>
<h2 id="TCN结构"><a href="#TCN结构" class="headerlink" title="TCN结构"></a>TCN结构</h2><p>  TCN的设计十分巧妙，同ConvLSTM不同的是，ConvLSTM通过引入卷积操作，让LSTM网络可以处理图像信息，其卷积只对一个时间的输入图像进行操作，TCN则直接利用卷积强大的特性，跨时间步提取特征。</p>
<p>  TCN结构很像Wavenet，paper作者也表示确实借鉴了Wavenet的结构，TCN的结构在paper中表示如下，这是一个kernel size=3,dilations=[1,2,4]kernel~size = 3, dilations = [1, 2, 4]<em>kernel</em> <em>size</em>=3,<em>dilations</em>=[1,2,4]的TCN。</p>
<p><img src="https://img-blog.csdnimg.cn/20190829100025261.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0xlb25fd2ludGVy,size_16,color_FFFFFF,t_70" alt="img"></p>
<p>下图展示了更直接的TCN结构，kernel size=2,dilations=[1,2,4,8]kernel~size = 2, dilations = [1, 2, 4, 8]<em>kernel</em> <em>size</em>=2,<em>dilations</em>=[1,2,4,8]</p>
<p><img src="https://img-blog.csdnimg.cn/20190829093440798.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0xlb25fd2ludGVy,size_16,color_FFFFFF,t_70" alt="img"></p>
<p>kernel size等于2，即每一层的输入，是上一层的两个时刻的输出；dilations = [1, 2, 4, 8]，即每一层的输入的时间间隔有多大，dilation=4，即上一层每前推4个时间步的输出，作为这一层的输入，直到取够kernal size个输入。</p>
<p>  TCN要实现RNN的类似功能，需要解决两个问题，</p>
<ol>
<li>TCN如何像RNN那样，输入多长的时间步，输出时间步也是同样长度，或者说，每个时间的输入都有对应的输出；</li>
<li>如何保证历史数据不漏接(no leakage)。</li>
</ol>
<p>  为了解决上面的两个问题，paper作者分别引入了1-D FCN和因果卷积(Causal Convolutions)，可以说<br>TCN=1D FCN+Causal ConvolutionsTCN = 1D<del>FCN + Causal</del>Convolutions<em>TCN</em>=1<em>D</em> <em>FCN</em>+<em>Causal</em> <em>Convolutions</em></p>
<h3 id="1-D-FCN的结构"><a href="#1-D-FCN的结构" class="headerlink" title="1-D FCN的结构"></a>1-D FCN的结构</h3><p>  为了解决第一个问题，TCN利用了1-D FCN的结构，每一个隐层的输入输出的时间长度都相同，维持相同的时间步，具体来看，第一隐层不管kernel size和dilation为多少，输入若是n个时间步，输出也是n个时间步，同样第二隐层，第三隐层。。。的输入输出时间步长度都是n，这点和RNN就很像，不管在哪一层，每个时间步的输入都会有对应的输出。</p>
<p>  对于第一个时间步，没有任何历史的信息，TCN认为其历史数据全是0 (其实就是卷积操作的padding，这一点最好结合下面的代码理解)，同时paper作者通过实验发现，TCN保留长远历史信息的能力较LSTM更强。</p>
<h3 id="因果卷积-Causal-Convolutions"><a href="#因果卷积-Causal-Convolutions" class="headerlink" title="因果卷积(Causal Convolutions)"></a>因果卷积(Causal Convolutions)</h3><p>  为了解决第二个问题，TCN利用因果卷积(Causal Convolutions)，所谓因果，也就是对于输出t时刻的数据yty_{t}<em>yt</em>，其输入只可能是t以及t以前的时刻，即x0…xtx_{0}\dots x_{t}<em>x</em>0…<em>xt</em>，其结构如下：</p>
<p><img src="https://img-blog.csdnimg.cn/2019082909091041.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0xlb25fd2ludGVy,size_16,color_FFFFFF,t_70" alt="img"></p>
<p>不难发现，这样的卷积连接好像和最上面的TCN结构图不太一样，理论上利用因果卷积是可以搭建TCN，但是如果我们的输出和之前的1000个时间点都存在联系，要获取这种联系，因果卷积构成的TCN深度就是1000-1，如果和历史的10000个时间点有联系，那么深度就是10000-1…，那样TCN就太深了。</p>
<h3 id="膨胀因果卷积-Dilated-Causal-Convolutions"><a href="#膨胀因果卷积-Dilated-Causal-Convolutions" class="headerlink" title="膨胀因果卷积(Dilated Causal Convolutions)"></a>膨胀因果卷积(Dilated Causal Convolutions)</h3><p>  为了有效的应对长历史信息这一问题，paper作者利用了膨胀因果卷积(Dilated Causal Convolutions)，还是具有因果性，只不过引入了膨胀因子(dilation factor) dd<em>d</em>，对于kernel size=2,dilations=[1,2,4,8]kernel~size = 2, dilations = [1, 2, 4, 8]<em>kernel</em> <em>size</em>=2,<em>dilations</em>=[1,2,4,8]的TCN，其结构如下：</p>
<p><img src="https://img-blog.csdnimg.cn/20190829091941330.gif" alt="img"></p>
<p>一般膨胀系数是2的指数次方，即1，2，4，8，16，32…</p>
<h3 id="膨胀非因果卷积-Dilated-Non-Causal-Convolutions"><a href="#膨胀非因果卷积-Dilated-Non-Causal-Convolutions" class="headerlink" title="膨胀非因果卷积(Dilated Non-Causal Convolutions)"></a>膨胀非因果卷积(Dilated Non-Causal Convolutions)</h3><p>  LSTM是可以双边输入的，输入不仅利用历史信息，也利用了未来信息，TCN也能做到类似的实现，利用膨胀非因果卷积(Dilated Non-Causal Convolutions)，下图展示了kernel size=3,dilations=[1,2,4,8]kernel~size = 3, dilations = [1, 2, 4, 8]<em>kernel</em> <em>size</em>=3,<em>dilations</em>=[1,2,4,8]的膨胀非因果卷积构成的TCN：</p>
<p><img src="https://img-blog.csdnimg.cn/20190829092541148.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0xlb25fd2ludGVy,size_16,color_FFFFFF,t_70" alt="img"></p>
<h3 id="残差块结构"><a href="#残差块结构" class="headerlink" title="残差块结构"></a>残差块结构</h3><p>  同时，就算我们使用了膨胀因果卷积，有时模型可能仍然很深，较深的网络结构可能会引起梯度消失等问题，为了应对这种情况，paper作者利用了一种类似于ResNet中的残差块的结构，这样设计的TCN结构更加的具有泛化能力(generic)。</p>
<p><img src="https://img-blog.csdnimg.cn/20190829101302335.png?x-oss-process=image/watermark,type_ZmFuZ3poZW5naGVpdGk,shadow_10,text_aHR0cHM6Ly9ibG9nLmNzZG4ubmV0L0xlb25fd2ludGVy,size_16,color_FFFFFF,t_70" alt="img"></p>
<p>o=Activation(x+F(x))o=Activation(x+F(x))<em>o</em>=<em>Activation</em>(<em>x</em>+<em>F</em>(<em>x</em>))</p>
<p>可以看出来，残差结构替代了TCN层与层之间的简单连接，由于xx<em>x</em>和F(x)F(x)<em>F</em>(<em>x</em>)之间的通道数可能不一样，所以这里设计了一个1×1 Conv1\times1~Conv1×1 <em>Conv<em>来对x做一个简单的变换，使得变换后的xx</em>x<em>与F(x)F(x)</em>F</em>(<em>x</em>)可以相加。其实这里的图都有一定的欺骗性，每一层每个时刻只有一个网格并不代表这一时刻的通道数等于1。</p>
<h2 id="pytorch代码讲解"><a href="#pytorch代码讲解" class="headerlink" title="pytorch代码讲解"></a>pytorch代码讲解</h2><p>  paper给的代码是pytorch版本的，<a href="https://github.com/locuslab/TCN" target="_blank" rel="noopener">获取点这里</a>，其中TCN模型部分的代码如下，重难点部分给出了注释。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br><span class="line">75</span><br><span class="line">76</span><br><span class="line">77</span><br><span class="line">78</span><br><span class="line">79</span><br><span class="line">80</span><br><span class="line">81</span><br><span class="line">82</span><br><span class="line">83</span><br><span class="line">84</span><br><span class="line">85</span><br><span class="line">86</span><br><span class="line">87</span><br><span class="line">88</span><br><span class="line">89</span><br><span class="line">90</span><br><span class="line">91</span><br><span class="line">92</span><br><span class="line">93</span><br><span class="line">94</span><br><span class="line">95</span><br><span class="line">96</span><br><span class="line">97</span><br><span class="line">98</span><br><span class="line">99</span><br><span class="line">100</span><br><span class="line">101</span><br><span class="line">102</span><br><span class="line">103</span><br><span class="line">104</span><br><span class="line">105</span><br><span class="line">106</span><br></pre></td><td class="code"><pre><span class="line">import torch</span><br><span class="line">import torch.nn as nn</span><br><span class="line">from torch.nn.utils import weight_norm</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class Chomp1d(nn.Module):</span><br><span class="line">    def __init__(self, chomp_size):</span><br><span class="line">        super(Chomp1d, self).__init__()</span><br><span class="line">        self.chomp_size &#x3D; chomp_size</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        其实这就是一个裁剪的模块，裁剪多出来的padding</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        return x[:, :, :-self.chomp_size].contiguous()</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class TemporalBlock(nn.Module):</span><br><span class="line">    def __init__(self, n_inputs, n_outputs, kernel_size, stride, dilation, padding, dropout&#x3D;0.2):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        相当于一个Residual block</span><br><span class="line"></span><br><span class="line">        :param n_inputs: int, 输入通道数</span><br><span class="line">        :param n_outputs: int, 输出通道数</span><br><span class="line">        :param kernel_size: int, 卷积核尺寸</span><br><span class="line">        :param stride: int, 步长，一般为1</span><br><span class="line">        :param dilation: int, 膨胀系数</span><br><span class="line">        :param padding: int, 填充系数</span><br><span class="line">        :param dropout: float, dropout比率</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        super(TemporalBlock, self).__init__()</span><br><span class="line">        self.conv1 &#x3D; weight_norm(nn.Conv1d(n_inputs, n_outputs, kernel_size,</span><br><span class="line">                                           stride&#x3D;stride, padding&#x3D;padding, dilation&#x3D;dilation))</span><br><span class="line">        # 经过conv1，输出的size其实是(Batch, input_channel, seq_len + padding)</span><br><span class="line">        self.chomp1 &#x3D; Chomp1d(padding)  # 裁剪掉多出来的padding部分，维持输出时间步为seq_len</span><br><span class="line">        self.relu1 &#x3D; nn.ReLU()</span><br><span class="line">        self.dropout1 &#x3D; nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">        self.conv2 &#x3D; weight_norm(nn.Conv1d(n_outputs, n_outputs, kernel_size,</span><br><span class="line">                                           stride&#x3D;stride, padding&#x3D;padding, dilation&#x3D;dilation))</span><br><span class="line">        self.chomp2 &#x3D; Chomp1d(padding)  #  裁剪掉多出来的padding部分，维持输出时间步为seq_len</span><br><span class="line">        self.relu2 &#x3D; nn.ReLU()</span><br><span class="line">        self.dropout2 &#x3D; nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">        self.net &#x3D; nn.Sequential(self.conv1, self.chomp1, self.relu1, self.dropout1,</span><br><span class="line">                                 self.conv2, self.chomp2, self.relu2, self.dropout2)</span><br><span class="line">        self.downsample &#x3D; nn.Conv1d(n_inputs, n_outputs, 1) if n_inputs !&#x3D; n_outputs else None</span><br><span class="line">        self.relu &#x3D; nn.ReLU()</span><br><span class="line">        self.init_weights()</span><br><span class="line"></span><br><span class="line">    def init_weights(self):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        参数初始化</span><br><span class="line"></span><br><span class="line">        :return:</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        self.conv1.weight.data.normal_(0, 0.01)</span><br><span class="line">        self.conv2.weight.data.normal_(0, 0.01)</span><br><span class="line">        if self.downsample is not None:</span><br><span class="line">            self.downsample.weight.data.normal_(0, 0.01)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        :param x: size of (Batch, input_channel, seq_len)</span><br><span class="line">        :return:</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        out &#x3D; self.net(x)</span><br><span class="line">        res &#x3D; x if self.downsample is None else self.downsample(x)</span><br><span class="line">        return self.relu(out + res)</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">class TemporalConvNet(nn.Module):</span><br><span class="line">    def __init__(self, num_inputs, num_channels, kernel_size&#x3D;2, dropout&#x3D;0.2):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        TCN，目前paper给出的TCN结构很好的支持每个时刻为一个数的情况，即sequence结构，</span><br><span class="line">        对于每个时刻为一个向量这种一维结构，勉强可以把向量拆成若干该时刻的输入通道，</span><br><span class="line">        对于每个时刻为一个矩阵或更高维图像的情况，就不太好办。</span><br><span class="line"></span><br><span class="line">        :param num_inputs: int， 输入通道数</span><br><span class="line">        :param num_channels: list，每层的hidden_channel数，例如[25,25,25,25]表示有4个隐层，每层hidden_channel数为25</span><br><span class="line">        :param kernel_size: int, 卷积核尺寸</span><br><span class="line">        :param dropout: float, drop_out比率</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        super(TemporalConvNet, self).__init__()</span><br><span class="line">        layers &#x3D; []</span><br><span class="line">        num_levels &#x3D; len(num_channels)</span><br><span class="line">        for i in range(num_levels):</span><br><span class="line">            dilation_size &#x3D; 2  i   # 膨胀系数：1，2，4，8……</span><br><span class="line">            in_channels &#x3D; num_inputs if i &#x3D;&#x3D; 0 else num_channels[i-1]  # 确定每一层的输入通道数</span><br><span class="line">            out_channels &#x3D; num_channels[i]  # 确定每一层的输出通道数</span><br><span class="line">            layers +&#x3D; [TemporalBlock(in_channels, out_channels, kernel_size, stride&#x3D;1, dilation&#x3D;dilation_size,</span><br><span class="line">                                     padding&#x3D;(kernel_size-1) * dilation_size, dropout&#x3D;dropout)]</span><br><span class="line"></span><br><span class="line">        self.network &#x3D; nn.Sequential(*layers)</span><br><span class="line"></span><br><span class="line">    def forward(self, x):</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        输入x的结构不同于RNN，一般RNN的size为(Batch, seq_len, channels)或者(seq_len, Batch, channels)，</span><br><span class="line">        这里把seq_len放在channels后面，把所有时间步的数据拼起来，当做Conv1d的输入尺寸，实现卷积跨时间步的操作，</span><br><span class="line">        很巧妙的设计。</span><br><span class="line"></span><br><span class="line">        :param x: size of (Batch, input_channel, seq_len)</span><br><span class="line">        :return: size of (Batch, output_channel, seq_len)</span><br><span class="line">        &quot;&quot;&quot;</span><br><span class="line">        return self.network(x)</span><br><span class="line">123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657585960616263646566676869707172737475767778798081828384858687888990919293949596979899100101102103104105</span><br></pre></td></tr></table></figure>

<p>参考资料：<br>TCN: <a href="https://arxiv.org/pdf/1803.01271.pdf" target="_blank" rel="noopener">https://arxiv.org/pdf/1803.01271.pdf</a></p>
<p>因果卷积（causal）与扩展卷积（dilated）：<a href="https://blog.csdn.net/tonygsw/article/details/81280364" target="_blank" rel="noopener">https://blog.csdn.net/tonygsw/article/details/81280364</a></p>
<p>philipperemy/keras-tcn<br>：<a href="https://github.com/philipperemy/keras-tcn#why-temporal-convolutional-network" target="_blank" rel="noopener">https://github.com/philipperemy/keras-tcn#why-temporal-convolutional-network</a></p>

    </div>

    

    

    <!-- Comments -->
    

</div>
        </section>

    </div>
</div>


</div>

<!-- Footer -->
<div class="push"></div>

<footer class="footer-content">
    <div class="container">
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-6 col-lg-6 footer-about">
                <h2>About</h2>
                <p>
                    This theme was developed by <a href="https://github.com/klugjo" target="_blank" rel="noopener">Jonathan Klughertz</a>. The source code is available on Github. Create Websites. Make Magic.
                </p>
            </div>
            
    <div class="col-xs-6 col-sm-6 col-md-3 col-lg-3 recent-posts">
        <h2>Recent Posts</h2>
        <ul>
            
            <li>
                <a class="footer-post" href="/2020/08/18/%E6%97%B6%E9%97%B4%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C(TCN)%EF%BC%9A%E7%BB%93%E6%9E%84+pytorch%E4%BB%A3%E7%A0%81/">时间卷积网络(TCN)：结构+pytorch代码</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2020/07/19/%E3%80%90%E4%BA%BA%E4%BD%93%E5%A7%BF%E6%80%81%E3%80%91Stacked%20Hourglass%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/">【人体姿态】Stacked Hourglass算法</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2020/07/19/pytorch_resnet/">pytorch_resnet</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2020/07/11/VAE/">VAE</a>
            </li>
            
        </ul>
    </div>



            
        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <ul class="list-inline footer-social-icons">
                    
                    <li class="list-inline-item">
                        <a href="https://github.com/klugjo/hexo-theme-alpha-dust" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-github"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://twitter.com/?lang=en" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-twitter"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://www.facebook.com/" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-facebook"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://www.instagram.com/" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-instagram"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://dribbble.com/" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-dribbble"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://plus.google.com/" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-google-plus"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://www.behance.net/" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-behance"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://500px.com/" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-500px"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="mailto:test@example.com">
                            <span class="footer-icon-container">
                                <i class="fa fa-envelope-o"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="\#">
                            <span class="footer-icon-container">
                                <i class="fa fa-rss"></i>
                            </span>
                        </a>
                    </li>
                    
                </ul>
            </div>
        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <div class="footer-copyright">
                    @Untitled. All right reserved | Design & Hexo <a href="http://www.codeblocq.com/" target="_blank" rel="noopener">Jonathan Klughertz</a>
                </div>
            </div>
        </div>
    </div>
</footer>

<!-- After footer scripts -->

<!-- jQuery -->
<script src="//code.jquery.com/jquery-2.1.4.min.js"></script>

<!-- Tween Max -->
<script src="//cdnjs.cloudflare.com/ajax/libs/gsap/1.18.5/TweenMax.min.js"></script>

<!-- Gallery -->
<script src="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.js" type="text/javascript" charset="utf-8"></script>

<!-- Custom JavaScript -->

<script src="/js/main.js"></script>


<!-- Disqus Comments -->



</body>

</html>