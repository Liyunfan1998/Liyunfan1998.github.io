<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--Description-->
    
        <meta name="description" content="第四课 情感分析褚则伟 zeweichu@gmail.com
学习目标

学习和训练文本分类模型
学习torchtext的基本使用方法
BucketIterator


学习torch.nn的一些基本模型
Conv2d



本notebook参考了https://github.com/bentre">
    

    <!--Author-->
    
        <meta name="author" content="Yunfan Li">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="4.sentiment"/>
    

    <!--Open Graph Description-->
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content="Hexo"/>

    <!--Type page-->
    
        <meta property="og:type" content="article" />
    

    <!--Page Cover-->
    

    
        <meta name="twitter:card" content="summary" />
    
    
    

    <!-- Title -->
    
    <title>4.sentiment - Hexo</title>

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.2/css/bootstrap.min.css" integrity="sha384-y3tfxAZXuh4HwSYylfB+J125MxIs6mR5FOHamPBG064zB+AFeWH94NdvaCBm8qnd" crossorigin="anonymous">

    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="//oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="//oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Gallery -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.css" type="text/css" rel="stylesheet" />

    <!-- Custom CSS -->
    
<link rel="stylesheet" href="/css/style.css">


    <!-- Google Analytics -->
    


<meta name="generator" content="Hexo 4.2.0"></head>


<body>

<div class="bg-gradient"></div>
<div class="bg-pattern"></div>

<!-- Menu -->
<!--Menu Links and Overlay-->
<div class="menu-bg">
    <div class="menu-container">
        <ul>
            
            <li class="menu-item">
                <a href="/archives">
                    Home
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/archives">
                    Archives
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/about.html">
                    About
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/tags">
                    Tags
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/categories">
                    Categories
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/contact.html">
                    Contact
                </a>
            </li>
            
        </ul>
    </div>
</div>

<!--Hamburger Icon-->
<nav>
    <a href="#menu"></a>
</nav>

<div class="container">

    <!-- Main Content -->
    <div class="row">
    <div class="col-sm-12">

        <!--Title and Logo-->
        <header>
    <div class="logo">
        <a href="/"><i class="logo-icon fa fa-cube" aria-hidden="true"></i></a>
        
    </div>
</header>

        <section class="main">
            
<div class="post">

    <div class="post-header">
        <h1 class="title">
            <a href="/2020/02/29/4.sentiment/">
                4.sentiment
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2020-02-29</span>
            
            
            
        </div>
    </div>

    <div class="content">

        <!-- Gallery -->
        

        <!-- Post Content -->
        <h1 id="第四课-情感分析"><a href="#第四课-情感分析" class="headerlink" title="第四课 情感分析"></a>第四课 情感分析</h1><p>褚则伟 <a href="mailto:zeweichu@gmail.com">zeweichu@gmail.com</a></p>
<p>学习目标</p>
<ul>
<li>学习和训练文本分类模型</li>
<li>学习torchtext的基本使用方法<ul>
<li>BucketIterator</li>
</ul>
</li>
<li>学习torch.nn的一些基本模型<ul>
<li>Conv2d</li>
</ul>
</li>
</ul>
<p>本notebook参考了<a href="https://github.com/bentrevett/pytorch-sentiment-analysis" target="_blank" rel="noopener">https://github.com/bentrevett/pytorch-sentiment-analysis</a></p>
<p>在这份notebook中，我们会用PyTorch模型和TorchText再来做情感分析(检测一段文字的情感是正面的还是负面的)。我们会使用<a href="http://ai.stanford.edu/~amaas/data/sentiment/" target="_blank" rel="noopener">IMDb 数据集</a>，即电影评论。</p>
<p>模型从简单到复杂，我们会依次构建：</p>
<ul>
<li>Word Averaging模型</li>
<li>RNN/LSTM模型</li>
<li>CNN模型</li>
</ul>
<h2 id="准备数据"><a href="#准备数据" class="headerlink" title="准备数据"></a>准备数据</h2><ul>
<li>TorchText中的一个重要概念是<code>Field</code>。<code>Field</code>决定了你的数据会被怎样处理。在我们的情感分类任务中，我们所需要接触到的数据有文本字符串和两种情感，”pos”或者”neg”。</li>
<li><code>Field</code>的参数制定了数据会被怎样处理。</li>
<li>我们使用<code>TEXT</code> field来定义如何处理电影评论，使用<code>LABEL</code> field来处理两个情感类别。</li>
<li>我们的<code>TEXT</code> field带有<code>tokenize=&#39;spacy&#39;</code>，这表示我们会用<a href="https://spacy.io" target="_blank" rel="noopener">spaCy</a> tokenizer来tokenize英文句子。如果我们不特别声明<code>tokenize</code>这个参数，那么默认的分词方法是使用空格。</li>
<li>安装spaCy<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pip install -U spacy</span><br><span class="line">python -m spacy download en</span><br></pre></td></tr></table></figure></li>
<li><code>LABEL</code>由<code>LabelField</code>定义。这是一种特别的用来处理label的<code>Field</code>。我们后面会解释dtype。</li>
<li>更多关于<code>Fields</code>，参见<a href="https://github.com/pytorch/text/blob/master/torchtext/data/field.py" target="_blank" rel="noopener">https://github.com/pytorch/text/blob/master/torchtext/data/field.py</a></li>
<li>和之前一样，我们会设定random seeds使实验可以复现。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">from</span> torchtext <span class="keyword">import</span> data</span><br><span class="line"></span><br><span class="line">SEED = <span class="number">1234</span></span><br><span class="line"></span><br><span class="line">torch.manual_seed(SEED)</span><br><span class="line">torch.cuda.manual_seed(SEED)</span><br><span class="line">torch.backends.cudnn.deterministic = <span class="literal">True</span></span><br><span class="line"></span><br><span class="line">TEXT = data.Field(tokenize=<span class="string">'spacy'</span>)</span><br><span class="line">LABEL = data.LabelField(dtype=torch.float)</span><br></pre></td></tr></table></figure>

<ul>
<li>TorchText支持很多常见的自然语言处理数据集。</li>
<li>下面的代码会自动下载IMDb数据集，然后分成train/test两个<code>torchtext.datasets</code>类别。数据被前面的<code>Fields</code>处理。IMDb数据集一共有50000电影评论，每个评论都被标注为正面的或负面的。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">from</span> torchtext <span class="keyword">import</span> datasets</span><br><span class="line">train_data, test_data = datasets.IMDB.splits(TEXT, LABEL)</span><br></pre></td></tr></table></figure>

<p>查看每个数据split有多少条数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">f'Number of training examples: <span class="subst">&#123;len(train_data)&#125;</span>'</span>)</span><br><span class="line">print(<span class="string">f'Number of testing examples: <span class="subst">&#123;len(test_data)&#125;</span>'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Number of training examples: 25000
Number of testing examples: 25000</code></pre><p>查看一个example。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(vars(train_data.examples[<span class="number">0</span>]))</span><br></pre></td></tr></table></figure>

<pre><code>{&apos;text&apos;: [&apos;Brilliant&apos;, &apos;adaptation&apos;, &apos;of&apos;, &apos;the&apos;, &apos;novel&apos;, &apos;that&apos;, &apos;made&apos;, &apos;famous&apos;, &apos;the&apos;, &apos;relatives&apos;, &apos;of&apos;, &apos;Chilean&apos;, &apos;President&apos;, &apos;Salvador&apos;, &apos;Allende&apos;, &apos;killed&apos;, &apos;.&apos;, &apos;In&apos;, &apos;the&apos;, &apos;environment&apos;, &apos;of&apos;, &apos;a&apos;, &apos;large&apos;, &apos;estate&apos;, &apos;that&apos;, &apos;arises&apos;, &apos;from&apos;, &apos;the&apos;, &apos;ruins&apos;, &apos;,&apos;, &apos;becoming&apos;, &apos;a&apos;, &apos;force&apos;, &apos;to&apos;, &apos;abuse&apos;, &apos;and&apos;, &apos;exploitation&apos;, &apos;of&apos;, &apos;outrage&apos;, &apos;,&apos;, &apos;a&apos;, &apos;luxury&apos;, &apos;estate&apos;, &apos;for&apos;, &apos;the&apos;, &apos;benefit&apos;, &apos;of&apos;, &apos;the&apos;, &apos;upstart&apos;, &apos;Esteban&apos;, &apos;Trueba&apos;, &apos;and&apos;, &apos;his&apos;, &apos;undeserved&apos;, &apos;family&apos;, &apos;,&apos;, &apos;the&apos;, &apos;brilliant&apos;, &apos;Danish&apos;, &apos;director&apos;, &apos;Bille&apos;, &apos;August&apos;, &apos;recreates&apos;, &apos;,&apos;, &apos;in&apos;, &apos;micro&apos;, &apos;,&apos;, &apos;which&apos;, &apos;at&apos;, &apos;the&apos;, &apos;time&apos;, &apos;would&apos;, &apos;be&apos;, &apos;the&apos;, &apos;process&apos;, &apos;leading&apos;, &apos;to&apos;, &apos;the&apos;, &apos;greatest&apos;, &apos;infamy&apos;, &apos;of&apos;, &apos;his&apos;, &apos;story&apos;, &apos;to&apos;, &apos;the&apos;, &apos;hardened&apos;, &apos;Chilean&apos;, &apos;nation&apos;, &apos;,&apos;, &apos;and&apos;, &apos;whose&apos;, &apos;main&apos;, &apos;character&apos;, &apos;would&apos;, &apos;Augusto&apos;, &apos;Pinochet&apos;, &apos;(&apos;, &apos;Stephen&apos;, &apos;similarities&apos;, &apos;with&apos;, &apos;it&apos;, &apos;are&apos;, &apos;inevitable&apos;, &apos;:&apos;, &apos;recall&apos;, &apos;,&apos;, &apos;as&apos;, &apos;an&apos;, &apos;example&apos;, &apos;,&apos;, &apos;that&apos;, &apos;image&apos;, &apos;of&apos;, &apos;the&apos;, &apos;senator&apos;, &apos;with&apos;, &apos;dark&apos;, &apos;glasses&apos;, &apos;that&apos;, &apos;makes&apos;, &apos;him&apos;, &apos;the&apos;, &apos;wink&apos;, &apos;to&apos;, &apos;the&apos;, &apos;general&apos;, &apos;to&apos;, &apos;begin&apos;, &apos;making&apos;, &apos;the&apos;, &apos;palace).&lt;br&apos;, &apos;/&gt;&lt;br&apos;, &apos;/&gt;Bille&apos;, &apos;August&apos;, &apos;attends&apos;, &apos;an&apos;, &apos;exceptional&apos;, &apos;cast&apos;, &apos;in&apos;, &apos;the&apos;, &apos;Jeremy&apos;, &apos;protruding&apos;, &apos;Irons&apos;, &apos;,&apos;, &apos;whose&apos;, &apos;character&apos;, &apos;changes&apos;, &apos;from&apos;, &apos;arrogance&apos;, &apos;and&apos;, &apos;extreme&apos;, &apos;cruelty&apos;, &apos;,&apos;, &apos;the&apos;, &apos;hard&apos;, &apos;lesson&apos;, &apos;that&apos;, &apos;life&apos;, &apos;always&apos;, &apos;brings&apos;, &apos;us&apos;, &apos;to&apos;, &apos;almost&apos;, &apos;force&apos;, &apos;us&apos;, &apos;to&apos;, &apos;change&apos;, &apos;.&apos;, &apos;In&apos;, &apos;Esteban&apos;, &apos;fully&apos;, &apos;applies&apos;, &apos;the&apos;, &apos;law&apos;, &apos;of&apos;, &apos;resonance&apos;, &apos;,&apos;, &apos;with&apos;, &apos;great&apos;, &apos;wisdom&apos;, &apos;,&apos;, &apos;Solomon&apos;, &apos;describes&apos;, &apos;in&apos;, &apos;these&apos;, &apos;words:&quot;The&apos;, &apos;things&apos;, &apos;that&apos;, &apos;freckles&apos;, &apos;are&apos;, &apos;the&apos;, &apos;same&apos;, &apos;punishment&apos;, &apos;that&apos;, &apos;will&apos;, &apos;serve&apos;, &apos;you&apos;, &apos;.&apos;, &apos;&quot;&apos;, &apos;&lt;&apos;, &apos;br&apos;, &apos;/&gt;&lt;br&apos;, &apos;/&gt;Unforgettable&apos;, &apos;Glenn&apos;, &apos;Close&apos;, &apos;playing&apos;, &apos;splint&apos;, &apos;,&apos;, &apos;the&apos;, &apos;tainted&apos;, &apos;sister&apos;, &apos;of&apos;, &apos;Stephen&apos;, &apos;,&apos;, &apos;whose&apos;, &apos;sin&apos;, &apos;,&apos;, &apos;driven&apos;, &apos;by&apos;, &apos;loneliness&apos;, &apos;,&apos;, &apos;spiritual&apos;, &apos;and&apos;, &apos;platonic&apos;, &apos;love&apos;, &apos;was&apos;, &apos;the&apos;, &apos;wife&apos;, &apos;of&apos;, &apos;his&apos;, &apos;cruel&apos;, &apos;snowy&apos;, &apos;brother&apos;, &apos;.&apos;, &apos;Meryl&apos;, &apos;Streep&apos;, &apos;also&apos;, &apos;brilliant&apos;, &apos;,&apos;, &apos;a&apos;, &apos;woman&apos;, &apos;whose&apos;, &apos;name&apos;, &apos;came&apos;, &apos;to&apos;, &apos;him&apos;, &apos;like&apos;, &apos;a&apos;, &apos;glove&apos;, &apos;Clara&apos;, &apos;.&apos;, &apos;With&apos;, &apos;telekinetic&apos;, &apos;powers&apos;, &apos;,&apos;, &apos;cognitive&apos;, &apos;and&apos;, &apos;mediumistic&apos;, &apos;,&apos;, &apos;this&apos;, &apos;hardened&apos;, &apos;woman&apos;, &apos;,&apos;, &apos;loyal&apos;, &apos;to&apos;, &apos;his&apos;, &apos;blunt&apos;, &apos;,&apos;, &apos;conservative&apos;, &apos;husband&apos;, &apos;,&apos;, &apos;is&apos;, &apos;an&apos;, &apos;indicator&apos;, &apos;of&apos;, &apos;character&apos;, &apos;and&apos;, &apos;self&apos;, &apos;-&apos;, &apos;control&apos;, &apos;that&apos;, &apos;we&apos;, &apos;wish&apos;, &apos;for&apos;, &apos;ourselves&apos;, &apos;and&apos;, &apos;for&apos;, &apos;all&apos;, &apos;human&apos;, &apos;beings&apos;, &apos;.&apos;, &apos;&lt;&apos;, &apos;br&apos;, &apos;/&gt;&lt;br&apos;, &apos;/&gt;Every&apos;, &apos;character&apos;, &apos;is&apos;, &apos;a&apos;, &apos;portrait&apos;, &apos;of&apos;, &apos;virtuosity&apos;, &apos;(&apos;, &apos;as&apos;, &apos;Blanca&apos;, &apos;worthy&apos;, &apos;rebel&apos;, &apos;leader&apos;, &apos;Pedro&apos;, &apos;Segundo&apos;, &apos;unhappy&apos;, &apos;...&apos;, &apos;)&apos;, &apos;or&apos;, &apos;a&apos;, &apos;portrait&apos;, &apos;of&apos;, &apos;humiliation&apos;, &apos;,&apos;, &apos;like&apos;, &apos;Stephen&apos;, &apos;Jr.&apos;, &apos;,&apos;, &apos;the&apos;, &apos;bastard&apos;, &apos;child&apos;, &apos;of&apos;, &apos;Senator&apos;, &apos;,&apos;, &apos;who&apos;, &apos;serves&apos;, &apos;as&apos;, &apos;an&apos;, &apos;instrument&apos;, &apos;for&apos;, &apos;the&apos;, &apos;return&apos;, &apos;of&apos;, &apos;the&apos;, &apos;boomerang&apos;, &apos;.&apos;, &apos;&lt;&apos;, &apos;br&apos;, &apos;/&gt;&lt;br&apos;, &apos;/&gt;The&apos;, &apos;film&apos;, &apos;moves&apos;, &apos;the&apos;, &apos;bowels&apos;, &apos;,&apos;, &apos;we&apos;, &apos;recreated&apos;, &apos;some&apos;, &apos;facts&apos;, &apos;that&apos;, &apos;should&apos;, &apos;not&apos;, &apos;ever&apos;, &apos;be&apos;, &apos;repeated&apos;, &apos;,&apos;, &apos;but&apos;, &apos;that&apos;, &apos;absurdly&apos;, &apos;still&apos;, &apos;happen&apos;, &apos;(&apos;, &apos;Colombia&apos;, &apos;is&apos;, &apos;a&apos;, &apos;sad&apos;, &apos;example&apos;, &apos;)&apos;, &apos;and&apos;, &apos;another&apos;, &apos;reminder&apos;, &apos;that&apos;, &apos;,&apos;, &apos;against&apos;, &apos;all&apos;, &apos;,&apos;, &apos;life&apos;, &apos;is&apos;, &apos;wonderful&apos;, &apos;because&apos;, &apos;there&apos;, &apos;are&apos;, &apos;always&apos;, &apos;people&apos;, &apos;like&apos;, &apos;Isabel&apos;, &apos;Allende&apos;, &apos;and&apos;, &apos;immortalize&apos;, &apos;just&apos;, &apos;Bille&apos;, &apos;August&apos;, &apos;.&apos;], &apos;label&apos;: &apos;pos&apos;}</code></pre><ul>
<li>由于我们现在只有train/test这两个分类，所以我们需要创建一个新的validation set。我们可以使用<code>.split()</code>创建新的分类。</li>
<li>默认的数据分割是 70、30，如果我们声明<code>split_ratio</code>，可以改变split之间的比例，<code>split_ratio=0.8</code>表示80%的数据是训练集，20%是验证集。</li>
<li>我们还声明<code>random_state</code>这个参数，确保我们每次分割的数据集都是一样的。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line">train_data, valid_data = train_data.split(random_state=random.seed(SEED))</span><br></pre></td></tr></table></figure>

<p>检查一下现在每个部分有多少条数据。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">f'Number of training examples: <span class="subst">&#123;len(train_data)&#125;</span>'</span>)</span><br><span class="line">print(<span class="string">f'Number of validation examples: <span class="subst">&#123;len(valid_data)&#125;</span>'</span>)</span><br><span class="line">print(<span class="string">f'Number of testing examples: <span class="subst">&#123;len(test_data)&#125;</span>'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Number of training examples: 17500
Number of validation examples: 7500
Number of testing examples: 25000</code></pre><ul>
<li>下一步我们需要创建 <em>vocabulary</em> 。<em>vocabulary</em> 就是把每个单词一一映射到一个数字。<br><img src="assets/sentiment5.png" alt=""></li>
<li>我们使用最常见的25k个单词来构建我们的单词表，用<code>max_size</code>这个参数可以做到这一点。</li>
<li>所有其他的单词都用<code>&lt;unk&gt;</code>来表示。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># TEXT.build_vocab(train_data, max_size=25000)</span></span><br><span class="line"><span class="comment"># LABEL.build_vocab(train_data)</span></span><br><span class="line">TEXT.build_vocab(train_data, max_size=<span class="number">25000</span>, vectors=<span class="string">"glove.6B.100d"</span>, unk_init=torch.Tensor.normal_)</span><br><span class="line">LABEL.build_vocab(train_data)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">f"Unique tokens in TEXT vocabulary: <span class="subst">&#123;len(TEXT.vocab)&#125;</span>"</span>)</span><br><span class="line">print(<span class="string">f"Unique tokens in LABEL vocabulary: <span class="subst">&#123;len(LABEL.vocab)&#125;</span>"</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Unique tokens in TEXT vocabulary: 25002
Unique tokens in LABEL vocabulary: 2</code></pre><ul>
<li>当我们把句子传进模型的时候，我们是按照一个个 <em>batch</em> 穿进去的，也就是说，我们一次传入了好几个句子，而且每个batch中的句子必须是相同的长度。为了确保句子的长度相同，TorchText会把短的句子pad到和最长的句子等长。<br><img src="assets/sentiment6.png" alt=""></li>
<li>下面我们来看看训练数据集中最常见的单词。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(TEXT.vocab.freqs.most_common(<span class="number">20</span>))</span><br></pre></td></tr></table></figure>

<pre><code>[(&apos;the&apos;, 201455), (&apos;,&apos;, 192552), (&apos;.&apos;, 164402), (&apos;a&apos;, 108963), (&apos;and&apos;, 108649), (&apos;of&apos;, 100010), (&apos;to&apos;, 92873), (&apos;is&apos;, 76046), (&apos;in&apos;, 60904), (&apos;I&apos;, 54486), (&apos;it&apos;, 53405), (&apos;that&apos;, 49155), (&apos;&quot;&apos;, 43890), (&quot;&apos;s&quot;, 43151), (&apos;this&apos;, 42454), (&apos;-&apos;, 36769), (&apos;/&gt;&lt;br&apos;, 35511), (&apos;was&apos;, 34990), (&apos;as&apos;, 30324), (&apos;with&apos;, 29691)]</code></pre><p>我们可以直接用 <code>stoi</code>(<strong>s</strong>tring <strong>to</strong> <strong>i</strong>nt) 或者 <code>itos</code> (<strong>i</strong>nt <strong>to</strong>  <strong>s</strong>tring) 来查看我们的单词表。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(TEXT.vocab.itos[:<span class="number">10</span>])</span><br></pre></td></tr></table></figure>

<pre><code>[&apos;&lt;unk&gt;&apos;, &apos;&lt;pad&gt;&apos;, &apos;the&apos;, &apos;,&apos;, &apos;.&apos;, &apos;a&apos;, &apos;and&apos;, &apos;of&apos;, &apos;to&apos;, &apos;is&apos;]</code></pre><p>查看labels。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(LABEL.vocab.stoi)</span><br></pre></td></tr></table></figure>

<pre><code>defaultdict(&lt;function _default_unk_index at 0x7fbec39a79d8&gt;, {&apos;neg&apos;: 0, &apos;pos&apos;: 1})</code></pre><ul>
<li>最后一步数据的准备是创建iterators。每个itartion都会返回一个batch的examples。</li>
<li>我们会使用<code>BucketIterator</code>。<code>BucketIterator</code>会把长度差不多的句子放到同一个batch中，确保每个batch中不出现太多的padding。</li>
<li>严格来说，我们这份notebook中的模型代码都有一个问题，也就是我们把<code>&lt;pad&gt;</code>也当做了模型的输入进行训练。更好的做法是在模型中把由<code>&lt;pad&gt;</code>产生的输出给消除掉。在这节课中我们简单处理，直接把<code>&lt;pad&gt;</code>也用作模型输入了。由于<code>&lt;pad&gt;</code>数量不多，模型的效果也不差。</li>
<li>如果我们有GPU，还可以指定每个iteration返回的tensor都在GPU上。</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">BATCH_SIZE = <span class="number">64</span></span><br><span class="line"></span><br><span class="line">device = torch.device(<span class="string">'cuda'</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">'cpu'</span>)</span><br><span class="line"></span><br><span class="line">train_iterator, valid_iterator, test_iterator = data.BucketIterator.splits(</span><br><span class="line">    (train_data, valid_data, test_data), </span><br><span class="line">    batch_size=BATCH_SIZE,</span><br><span class="line">    device=device)</span><br></pre></td></tr></table></figure>

<h2 id="Word-Averaging模型"><a href="#Word-Averaging模型" class="headerlink" title="Word Averaging模型"></a>Word Averaging模型</h2><ul>
<li>我们首先介绍一个简单的Word Averaging模型。这个模型非常简单，我们把每个单词都通过<code>Embedding</code>层投射成word embedding vector，然后把一句话中的所有word vector做个平均，就是整个句子的vector表示了。接下来把这个sentence vector传入一个<code>Linear</code>层，做分类即可。</li>
</ul>
<p><img src="assets/sentiment8.png" alt=""></p>
<ul>
<li>我们使用<a href="https://pytorch.org/docs/stable/nn.html?highlight=avg_pool2d#torch.nn.functional.avg_pool2d" target="_blank" rel="noopener"><code>avg_pool2d</code></a>来做average pooling。我们的目标是把sentence length那个维度平均成1，然后保留embedding这个维度。</li>
</ul>
<p><img src="assets/sentiment9.png" alt=""></p>
<ul>
<li><code>avg_pool2d</code>的kernel size是 (<code>embedded.shape[1]</code>, 1)，所以句子长度的那个维度会被压扁。</li>
</ul>
<p><img src="assets/sentiment10.png" alt=""></p>
<p><img src="assets/sentiment11.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">WordAVGModel</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_dim, output_dim, pad_idx)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)</span><br><span class="line">        self.fc = nn.Linear(embedding_dim, output_dim)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text)</span>:</span></span><br><span class="line">        embedded = self.embedding(text) <span class="comment"># [sent len, batch size, emb dim]</span></span><br><span class="line">        embedded = embedded.permute(<span class="number">1</span>, <span class="number">0</span>, <span class="number">2</span>) <span class="comment"># [batch size, sent len, emb dim]</span></span><br><span class="line">        pooled = F.avg_pool2d(embedded, (embedded.shape[<span class="number">1</span>], <span class="number">1</span>)).squeeze(<span class="number">1</span>) <span class="comment"># [batch size, embedding_dim]</span></span><br><span class="line">        <span class="keyword">return</span> self.fc(pooled)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">INPUT_DIM = len(TEXT.vocab)</span><br><span class="line">EMBEDDING_DIM = <span class="number">100</span></span><br><span class="line">OUTPUT_DIM = <span class="number">1</span></span><br><span class="line">PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]</span><br><span class="line"></span><br><span class="line">model = WordAVGModel(INPUT_DIM, EMBEDDING_DIM, OUTPUT_DIM, PAD_IDX)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">count_parameters</span><span class="params">(model)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> sum(p.numel() <span class="keyword">for</span> p <span class="keyword">in</span> model.parameters() <span class="keyword">if</span> p.requires_grad)</span><br><span class="line"></span><br><span class="line">print(<span class="string">f'The model has <span class="subst">&#123;count_parameters(model):,&#125;</span> trainable parameters'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>The model has 2,500,301 trainable parameters</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">pretrained_embeddings = TEXT.vocab.vectors</span><br><span class="line">model.embedding.weight.data.copy_(pretrained_embeddings)</span><br></pre></td></tr></table></figure>




<pre><code>tensor([[-0.1117, -0.4966,  0.1631,  ...,  1.2647, -0.2753, -0.1325],
        [-0.8555, -0.7208,  1.3755,  ...,  0.0825, -1.1314,  0.3997],
        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],
        ...,
        [-0.7244, -0.0186,  0.0996,  ...,  0.0045, -1.0037,  0.6646],
        [-1.1243,  1.2040, -0.6489,  ..., -0.7526,  0.5711,  1.0081],
        [ 0.0860,  0.1367,  0.0321,  ..., -0.5542, -0.4557, -0.0382]])</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]</span><br><span class="line"></span><br><span class="line">model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)</span><br><span class="line">model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)</span><br></pre></td></tr></table></figure>

<h2 id="训练模型"><a href="#训练模型" class="headerlink" title="训练模型"></a>训练模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.optim <span class="keyword">as</span> optim</span><br><span class="line"></span><br><span class="line">optimizer = optim.Adam(model.parameters())</span><br><span class="line">criterion = nn.BCEWithLogitsLoss()</span><br><span class="line">model = model.to(device)</span><br><span class="line">criterion = criterion.to(device)</span><br></pre></td></tr></table></figure>

<p>计算预测的准确率</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">binary_accuracy</span><span class="params">(preds, y)</span>:</span></span><br><span class="line">    <span class="string">"""</span></span><br><span class="line"><span class="string">    Returns accuracy per batch, i.e. if you get 8/10 right, this returns 0.8, NOT 8</span></span><br><span class="line"><span class="string">    """</span></span><br><span class="line"></span><br><span class="line">    <span class="comment">#round predictions to the closest integer</span></span><br><span class="line">    rounded_preds = torch.round(torch.sigmoid(preds))</span><br><span class="line">    correct = (rounded_preds == y).float() <span class="comment">#convert into float for division </span></span><br><span class="line">    acc = correct.sum()/len(correct)</span><br><span class="line">    <span class="keyword">return</span> acc</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(model, iterator, optimizer, criterion)</span>:</span></span><br><span class="line">    </span><br><span class="line">    epoch_loss = <span class="number">0</span></span><br><span class="line">    epoch_acc = <span class="number">0</span></span><br><span class="line">    model.train()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">for</span> batch <span class="keyword">in</span> iterator:</span><br><span class="line">        optimizer.zero_grad()</span><br><span class="line">        predictions = model(batch.text).squeeze(<span class="number">1</span>)</span><br><span class="line">        loss = criterion(predictions, batch.label)</span><br><span class="line">        acc = binary_accuracy(predictions, batch.label)</span><br><span class="line">        loss.backward()</span><br><span class="line">        optimizer.step()</span><br><span class="line">        </span><br><span class="line">        epoch_loss += loss.item()</span><br><span class="line">        epoch_acc += acc.item()</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> epoch_loss / len(iterator), epoch_acc / len(iterator)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(model, iterator, criterion)</span>:</span></span><br><span class="line">    </span><br><span class="line">    epoch_loss = <span class="number">0</span></span><br><span class="line">    epoch_acc = <span class="number">0</span></span><br><span class="line">    model.eval()</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> batch <span class="keyword">in</span> iterator:</span><br><span class="line">            predictions = model(batch.text).squeeze(<span class="number">1</span>)</span><br><span class="line">            loss = criterion(predictions, batch.label)</span><br><span class="line">            acc = binary_accuracy(predictions, batch.label)</span><br><span class="line">            epoch_loss += loss.item()</span><br><span class="line">            epoch_acc += acc.item()</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> epoch_loss / len(iterator), epoch_acc / len(iterator)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> time</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">epoch_time</span><span class="params">(start_time, end_time)</span>:</span></span><br><span class="line">    elapsed_time = end_time - start_time</span><br><span class="line">    elapsed_mins = int(elapsed_time / <span class="number">60</span>)</span><br><span class="line">    elapsed_secs = int(elapsed_time - (elapsed_mins * <span class="number">60</span>))</span><br><span class="line">    <span class="keyword">return</span> elapsed_mins, elapsed_secs</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line">N_EPOCHS = <span class="number">10</span></span><br><span class="line"></span><br><span class="line">best_valid_loss = float(<span class="string">'inf'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(N_EPOCHS):</span><br><span class="line"></span><br><span class="line">    start_time = time.time()</span><br><span class="line">    </span><br><span class="line">    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)</span><br><span class="line">    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)</span><br><span class="line">    </span><br><span class="line">    end_time = time.time()</span><br><span class="line"></span><br><span class="line">    epoch_mins, epoch_secs = epoch_time(start_time, end_time)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> valid_loss &lt; best_valid_loss:</span><br><span class="line">        best_valid_loss = valid_loss</span><br><span class="line">        torch.save(model.state_dict(), <span class="string">'wordavg-model.pt'</span>)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">f'Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>:<span class="number">02</span>&#125;</span> | Epoch Time: <span class="subst">&#123;epoch_mins&#125;</span>m <span class="subst">&#123;epoch_secs&#125;</span>s'</span>)</span><br><span class="line">    print(<span class="string">f'\tTrain Loss: <span class="subst">&#123;train_loss:<span class="number">.3</span>f&#125;</span> | Train Acc: <span class="subst">&#123;train_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br><span class="line">    print(<span class="string">f'\t Val. Loss: <span class="subst">&#123;valid_loss:<span class="number">.3</span>f&#125;</span> |  Val. Acc: <span class="subst">&#123;valid_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Epoch: 01 | Epoch Time: 0m 2s
    Train Loss: 0.685 | Train Acc: 56.84%
     Val. Loss: 0.622 |  Val. Acc: 71.09%
Epoch: 02 | Epoch Time: 0m 2s
    Train Loss: 0.642 | Train Acc: 71.31%
     Val. Loss: 0.510 |  Val. Acc: 75.48%
Epoch: 03 | Epoch Time: 0m 2s
    Train Loss: 0.573 | Train Acc: 78.31%
     Val. Loss: 0.449 |  Val. Acc: 79.52%
Epoch: 04 | Epoch Time: 0m 2s
    Train Loss: 0.503 | Train Acc: 82.78%
     Val. Loss: 0.419 |  Val. Acc: 82.72%
Epoch: 05 | Epoch Time: 0m 2s
    Train Loss: 0.440 | Train Acc: 85.84%
     Val. Loss: 0.408 |  Val. Acc: 84.75%
Epoch: 06 | Epoch Time: 0m 2s
    Train Loss: 0.389 | Train Acc: 87.59%
     Val. Loss: 0.413 |  Val. Acc: 86.02%
Epoch: 07 | Epoch Time: 0m 2s
    Train Loss: 0.352 | Train Acc: 88.85%
     Val. Loss: 0.425 |  Val. Acc: 86.92%
Epoch: 08 | Epoch Time: 0m 2s
    Train Loss: 0.320 | Train Acc: 89.93%
     Val. Loss: 0.440 |  Val. Acc: 87.54%
Epoch: 09 | Epoch Time: 0m 2s
    Train Loss: 0.294 | Train Acc: 90.74%
     Val. Loss: 0.456 |  Val. Acc: 88.09%
Epoch: 10 | Epoch Time: 0m 2s
    Train Loss: 0.274 | Train Acc: 91.27%
     Val. Loss: 0.468 |  Val. Acc: 88.49%</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> spacy</span><br><span class="line">nlp = spacy.load(<span class="string">'en'</span>)</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">predict_sentiment</span><span class="params">(sentence)</span>:</span></span><br><span class="line">    tokenized = [tok.text <span class="keyword">for</span> tok <span class="keyword">in</span> nlp.tokenizer(sentence)]</span><br><span class="line">    indexed = [TEXT.vocab.stoi[t] <span class="keyword">for</span> t <span class="keyword">in</span> tokenized]</span><br><span class="line">    tensor = torch.LongTensor(indexed).to(device)</span><br><span class="line">    tensor = tensor.unsqueeze(<span class="number">1</span>)</span><br><span class="line">    prediction = torch.sigmoid(model(tensor))</span><br><span class="line">    <span class="keyword">return</span> prediction.item()</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict_sentiment(<span class="string">"This film is terrible"</span>)</span><br></pre></td></tr></table></figure>




<pre><code>5.568591932965664e-26</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">predict_sentiment(<span class="string">"This film is great"</span>)</span><br></pre></td></tr></table></figure>




<pre><code>1.0</code></pre><h2 id="RNN模型"><a href="#RNN模型" class="headerlink" title="RNN模型"></a>RNN模型</h2><ul>
<li>下面我们尝试把模型换成一个<strong>recurrent neural network</strong> (RNN)。RNN经常会被用来encode一个sequence<br>$$h_t = \text{RNN}(x_t, h_{t-1})$$</li>
<li>我们使用最后一个hidden state $h_T$来表示整个句子。</li>
<li>然后我们把$h_T$通过一个线性变换$f$，然后用来预测句子的情感。</li>
</ul>
<p><img src="assets/sentiment1.png" alt=""></p>
<p><img src="assets/sentiment7.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">RNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_dim, hidden_dim, output_dim, </span></span></span><br><span class="line"><span class="function"><span class="params">                 n_layers, bidirectional, dropout, pad_idx)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)</span><br><span class="line">        self.rnn = nn.LSTM(embedding_dim, hidden_dim, num_layers=n_layers, </span><br><span class="line">                           bidirectional=bidirectional, dropout=dropout)</span><br><span class="line">        self.fc = nn.Linear(hidden_dim*<span class="number">2</span>, output_dim)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text)</span>:</span></span><br><span class="line">        embedded = self.dropout(self.embedding(text)) <span class="comment">#[sent len, batch size, emb dim]</span></span><br><span class="line">        output, (hidden, cell) = self.rnn(embedded)</span><br><span class="line">        <span class="comment">#output = [sent len, batch size, hid dim * num directions]</span></span><br><span class="line">        <span class="comment">#hidden = [num layers * num directions, batch size, hid dim]</span></span><br><span class="line">        <span class="comment">#cell = [num layers * num directions, batch size, hid dim]</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment">#concat the final forward (hidden[-2,:,:]) and backward (hidden[-1,:,:]) hidden layers</span></span><br><span class="line">        <span class="comment">#and apply dropout</span></span><br><span class="line">        hidden = self.dropout(torch.cat((hidden[<span class="number">-2</span>,:,:], hidden[<span class="number">-1</span>,:,:]), dim=<span class="number">1</span>)) <span class="comment"># [batch size, hid dim * num directions]</span></span><br><span class="line">        <span class="keyword">return</span> self.fc(hidden.squeeze(<span class="number">0</span>))</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">INPUT_DIM = len(TEXT.vocab)</span><br><span class="line">EMBEDDING_DIM = <span class="number">100</span></span><br><span class="line">HIDDEN_DIM = <span class="number">256</span></span><br><span class="line">OUTPUT_DIM = <span class="number">1</span></span><br><span class="line">N_LAYERS = <span class="number">2</span></span><br><span class="line">BIDIRECTIONAL = <span class="literal">True</span></span><br><span class="line">DROPOUT = <span class="number">0.5</span></span><br><span class="line">PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]</span><br><span class="line"></span><br><span class="line">model = RNN(INPUT_DIM, EMBEDDING_DIM, HIDDEN_DIM, OUTPUT_DIM, </span><br><span class="line">            N_LAYERS, BIDIRECTIONAL, DROPOUT, PAD_IDX)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">print(<span class="string">f'The model has <span class="subst">&#123;count_parameters(model):,&#125;</span> trainable parameters'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>The model has 4,810,857 trainable parameters</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">model.embedding.weight.data.copy_(pretrained_embeddings)</span><br><span class="line">UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]</span><br><span class="line"></span><br><span class="line">model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)</span><br><span class="line">model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)</span><br><span class="line"></span><br><span class="line">print(model.embedding.weight.data)</span><br></pre></td></tr></table></figure>

<pre><code>tensor([[ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [ 0.0000,  0.0000,  0.0000,  ...,  0.0000,  0.0000,  0.0000],
        [-0.0382, -0.2449,  0.7281,  ..., -0.1459,  0.8278,  0.2706],
        ...,
        [-0.7244, -0.0186,  0.0996,  ...,  0.0045, -1.0037,  0.6646],
        [-1.1243,  1.2040, -0.6489,  ..., -0.7526,  0.5711,  1.0081],
        [ 0.0860,  0.1367,  0.0321,  ..., -0.5542, -0.4557, -0.0382]],
       device=&apos;cuda:0&apos;)</code></pre><h2 id="训练RNN模型"><a href="#训练RNN模型" class="headerlink" title="训练RNN模型"></a>训练RNN模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.Adam(model.parameters())</span><br><span class="line">model = model.to(device)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">N_EPOCHS = <span class="number">5</span></span><br><span class="line">best_valid_loss = float(<span class="string">'inf'</span>)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(N_EPOCHS):</span><br><span class="line">    start_time = time.time()</span><br><span class="line">    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)</span><br><span class="line">    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)</span><br><span class="line">    </span><br><span class="line">    end_time = time.time()</span><br><span class="line"></span><br><span class="line">    epoch_mins, epoch_secs = epoch_time(start_time, end_time)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> valid_loss &lt; best_valid_loss:</span><br><span class="line">        best_valid_loss = valid_loss</span><br><span class="line">        torch.save(model.state_dict(), <span class="string">'lstm-model.pt'</span>)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">f'Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>:<span class="number">02</span>&#125;</span> | Epoch Time: <span class="subst">&#123;epoch_mins&#125;</span>m <span class="subst">&#123;epoch_secs&#125;</span>s'</span>)</span><br><span class="line">    print(<span class="string">f'\tTrain Loss: <span class="subst">&#123;train_loss:<span class="number">.3</span>f&#125;</span> | Train Acc: <span class="subst">&#123;train_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br><span class="line">    print(<span class="string">f'\t Val. Loss: <span class="subst">&#123;valid_loss:<span class="number">.3</span>f&#125;</span> |  Val. Acc: <span class="subst">&#123;valid_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Epoch: 01 | Epoch Time: 1m 29s
    Train Loss: 0.676 | Train Acc: 57.69%
     Val. Loss: 0.694 |  Val. Acc: 53.40%
Epoch: 02 | Epoch Time: 1m 29s
    Train Loss: 0.641 | Train Acc: 63.77%
     Val. Loss: 0.744 |  Val. Acc: 49.22%
Epoch: 03 | Epoch Time: 1m 29s
    Train Loss: 0.618 | Train Acc: 65.77%
     Val. Loss: 0.534 |  Val. Acc: 73.72%
Epoch: 04 | Epoch Time: 1m 30s
    Train Loss: 0.634 | Train Acc: 63.79%
     Val. Loss: 0.619 |  Val. Acc: 66.85%
Epoch: 05 | Epoch Time: 1m 29s
    Train Loss: 0.448 | Train Acc: 79.19%
     Val. Loss: 0.340 |  Val. Acc: 86.63%</code></pre><p>You may have noticed the loss is not really decreasing and the accuracy is poor. This is due to several issues with the model which we’ll improve in the next notebook.</p>
<p>Finally, the metric we actually care about, the test loss and accuracy, which we get from our parameters that gave us the best validation loss.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.load_state_dict(torch.load(<span class="string">'lstm-model.pt'</span>))</span><br><span class="line">test_loss, test_acc = evaluate(model, test_iterator, criterion)</span><br><span class="line">print(<span class="string">f'Test Loss: <span class="subst">&#123;test_loss:<span class="number">.3</span>f&#125;</span> | Test Acc: <span class="subst">&#123;test_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br></pre></td></tr></table></figure>

<h2 id="CNN模型"><a href="#CNN模型" class="headerlink" title="CNN模型"></a>CNN模型</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">CNN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embedding_dim, n_filters, </span></span></span><br><span class="line"><span class="function"><span class="params">                 filter_sizes, output_dim, dropout, pad_idx)</span>:</span></span><br><span class="line">        super().__init__()</span><br><span class="line">        </span><br><span class="line">        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)</span><br><span class="line">        self.convs = nn.ModuleList([</span><br><span class="line">                                    nn.Conv2d(in_channels = <span class="number">1</span>, out_channels = n_filters, </span><br><span class="line">                                              kernel_size = (fs, embedding_dim)) </span><br><span class="line">                                    <span class="keyword">for</span> fs <span class="keyword">in</span> filter_sizes</span><br><span class="line">                                    ])</span><br><span class="line">        self.fc = nn.Linear(len(filter_sizes) * n_filters, output_dim)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, text)</span>:</span></span><br><span class="line">        text = text.permute(<span class="number">1</span>, <span class="number">0</span>) <span class="comment"># [batch size, sent len]</span></span><br><span class="line">        embedded = self.embedding(text) <span class="comment"># [batch size, sent len, emb dim]</span></span><br><span class="line">        embedded = embedded.unsqueeze(<span class="number">1</span>) <span class="comment"># [batch size, 1, sent len, emb dim]</span></span><br><span class="line">        conved = [F.relu(conv(embedded)).squeeze(<span class="number">3</span>) <span class="keyword">for</span> conv <span class="keyword">in</span> self.convs]</span><br><span class="line">            </span><br><span class="line">        <span class="comment">#conv_n = [batch size, n_filters, sent len - filter_sizes[n]]</span></span><br><span class="line">        </span><br><span class="line">        pooled = [F.max_pool1d(conv, conv.shape[<span class="number">2</span>]).squeeze(<span class="number">2</span>) <span class="keyword">for</span> conv <span class="keyword">in</span> conved]</span><br><span class="line">        </span><br><span class="line">        <span class="comment">#pooled_n = [batch size, n_filters]</span></span><br><span class="line">        </span><br><span class="line">        cat = self.dropout(torch.cat(pooled, dim=<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">        <span class="comment">#cat = [batch size, n_filters * len(filter_sizes)]</span></span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> self.fc(cat)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">INPUT_DIM = len(TEXT.vocab)</span><br><span class="line">EMBEDDING_DIM = <span class="number">100</span></span><br><span class="line">N_FILTERS = <span class="number">100</span></span><br><span class="line">FILTER_SIZES = [<span class="number">3</span>,<span class="number">4</span>,<span class="number">5</span>]</span><br><span class="line">OUTPUT_DIM = <span class="number">1</span></span><br><span class="line">DROPOUT = <span class="number">0.5</span></span><br><span class="line">PAD_IDX = TEXT.vocab.stoi[TEXT.pad_token]</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, FILTER_SIZES, OUTPUT_DIM, DROPOUT, PAD_IDX)</span><br><span class="line">model.embedding.weight.data.copy_(pretrained_embeddings)</span><br><span class="line">UNK_IDX = TEXT.vocab.stoi[TEXT.unk_token]</span><br><span class="line"></span><br><span class="line">model.embedding.weight.data[UNK_IDX] = torch.zeros(EMBEDDING_DIM)</span><br><span class="line">model.embedding.weight.data[PAD_IDX] = torch.zeros(EMBEDDING_DIM)</span><br><span class="line">model = model.to(device)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br></pre></td><td class="code"><pre><span class="line">optimizer = optim.Adam(model.parameters())</span><br><span class="line">criterion = nn.BCEWithLogitsLoss()</span><br><span class="line">criterion = criterion.to(device)</span><br><span class="line"></span><br><span class="line">N_EPOCHS = <span class="number">5</span></span><br><span class="line"></span><br><span class="line">best_valid_loss = float(<span class="string">'inf'</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(N_EPOCHS):</span><br><span class="line"></span><br><span class="line">    start_time = time.time()</span><br><span class="line">    </span><br><span class="line">    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)</span><br><span class="line">    valid_loss, valid_acc = evaluate(model, valid_iterator, criterion)</span><br><span class="line">    </span><br><span class="line">    end_time = time.time()</span><br><span class="line"></span><br><span class="line">    epoch_mins, epoch_secs = epoch_time(start_time, end_time)</span><br><span class="line">    </span><br><span class="line">    <span class="keyword">if</span> valid_loss &lt; best_valid_loss:</span><br><span class="line">        best_valid_loss = valid_loss</span><br><span class="line">        torch.save(model.state_dict(), <span class="string">'CNN-model.pt'</span>)</span><br><span class="line">    </span><br><span class="line">    print(<span class="string">f'Epoch: <span class="subst">&#123;epoch+<span class="number">1</span>:<span class="number">02</span>&#125;</span> | Epoch Time: <span class="subst">&#123;epoch_mins&#125;</span>m <span class="subst">&#123;epoch_secs&#125;</span>s'</span>)</span><br><span class="line">    print(<span class="string">f'\tTrain Loss: <span class="subst">&#123;train_loss:<span class="number">.3</span>f&#125;</span> | Train Acc: <span class="subst">&#123;train_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br><span class="line">    print(<span class="string">f'\t Val. Loss: <span class="subst">&#123;valid_loss:<span class="number">.3</span>f&#125;</span> |  Val. Acc: <span class="subst">&#123;valid_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Epoch: 01 | Epoch Time: 0m 11s
    Train Loss: 0.645 | Train Acc: 62.12%
     Val. Loss: 0.485 |  Val. Acc: 79.61%
Epoch: 02 | Epoch Time: 0m 11s
    Train Loss: 0.423 | Train Acc: 80.59%
     Val. Loss: 0.360 |  Val. Acc: 84.63%
Epoch: 03 | Epoch Time: 0m 11s
    Train Loss: 0.302 | Train Acc: 87.33%
     Val. Loss: 0.320 |  Val. Acc: 86.59%
Epoch: 04 | Epoch Time: 0m 11s
    Train Loss: 0.222 | Train Acc: 91.20%
     Val. Loss: 0.306 |  Val. Acc: 87.17%
Epoch: 05 | Epoch Time: 0m 11s
    Train Loss: 0.161 | Train Acc: 93.99%
     Val. Loss: 0.325 |  Val. Acc: 86.82%</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">model.load_state_dict(torch.load(<span class="string">'CNN-model.pt'</span>))</span><br><span class="line">test_loss, test_acc = evaluate(model, test_iterator, criterion)</span><br><span class="line">print(<span class="string">f'Test Loss: <span class="subst">&#123;test_loss:<span class="number">.3</span>f&#125;</span> | Test Acc: <span class="subst">&#123;test_acc*<span class="number">100</span>:<span class="number">.2</span>f&#125;</span>%'</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Test Loss: 0.336 | Test Acc: 85.66%</code></pre>
    </div>

    

    

    <!-- Comments -->
    

</div>
        </section>

    </div>
</div>


</div>

<!-- Footer -->
<div class="push"></div>

<footer class="footer-content">
    <div class="container">
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-6 col-lg-6 footer-about">
                <h2>About</h2>
                <p>
                    This theme was developed by <a href="https://github.com/klugjo" target="_blank" rel="noopener">Jonathan Klughertz</a>. The source code is available on Github. Create Websites. Make Magic.
                </p>
            </div>
            
    <div class="col-xs-6 col-sm-6 col-md-3 col-lg-3 recent-posts">
        <h2>Recent Posts</h2>
        <ul>
            
            <li>
                <a class="footer-post" href="/2020/08/18/%E6%97%B6%E9%97%B4%E5%8D%B7%E7%A7%AF%E7%BD%91%E7%BB%9C(TCN)%EF%BC%9A%E7%BB%93%E6%9E%84+pytorch%E4%BB%A3%E7%A0%81/">时间卷积网络(TCN)：结构+pytorch代码</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2020/07/19/%E3%80%90%E4%BA%BA%E4%BD%93%E5%A7%BF%E6%80%81%E3%80%91Stacked%20Hourglass%E7%AE%97%E6%B3%95%E8%AF%A6%E8%A7%A3/">【人体姿态】Stacked Hourglass算法</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2020/07/19/pytorch_resnet/">pytorch_resnet</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2020/07/11/VAE/">VAE</a>
            </li>
            
        </ul>
    </div>



            
        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <ul class="list-inline footer-social-icons">
                    
                    <li class="list-inline-item">
                        <a href="https://github.com/klugjo/hexo-theme-alpha-dust" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-github"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://twitter.com/?lang=en" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-twitter"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://www.facebook.com/" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-facebook"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://www.instagram.com/" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-instagram"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://dribbble.com/" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-dribbble"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://plus.google.com/" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-google-plus"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://www.behance.net/" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-behance"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://500px.com/" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-500px"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="mailto:test@example.com">
                            <span class="footer-icon-container">
                                <i class="fa fa-envelope-o"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="\#">
                            <span class="footer-icon-container">
                                <i class="fa fa-rss"></i>
                            </span>
                        </a>
                    </li>
                    
                </ul>
            </div>
        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <div class="footer-copyright">
                    @Untitled. All right reserved | Design & Hexo <a href="http://www.codeblocq.com/" target="_blank" rel="noopener">Jonathan Klughertz</a>
                </div>
            </div>
        </div>
    </div>
</footer>

<!-- After footer scripts -->

<!-- jQuery -->
<script src="//code.jquery.com/jquery-2.1.4.min.js"></script>

<!-- Tween Max -->
<script src="//cdnjs.cloudflare.com/ajax/libs/gsap/1.18.5/TweenMax.min.js"></script>

<!-- Gallery -->
<script src="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.js" type="text/javascript" charset="utf-8"></script>

<!-- Custom JavaScript -->

<script src="/js/main.js"></script>


<!-- Disqus Comments -->



</body>

</html>