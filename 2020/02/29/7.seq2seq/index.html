<!DOCTYPE html>
<html lang="en">

<!-- Head tag -->
<head>

    <meta charset="utf-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1">

    <!--Description-->
    
        <meta name="description" content="第六课 Seq2Seq, Attention褚则伟 zeweichu@gmail.com
在这份notebook当中，我们会(尽可能)复现Luong的attention模型
由于我们的数据集非常小，只有一万多个句子的训练数据，所以训练出来的模型效果并不好。如果大家想训练一个好一点的模型，可以参考下面">
    

    <!--Author-->
    
        <meta name="author" content="Yunfan Li">
    

    <!--Open Graph Title-->
    
        <meta property="og:title" content="7.seq2seq"/>
    

    <!--Open Graph Description-->
    

    <!--Open Graph Site Name-->
    <meta property="og:site_name" content="Hexo"/>

    <!--Type page-->
    
        <meta property="og:type" content="article" />
    

    <!--Page Cover-->
    

    
        <meta name="twitter:card" content="summary" />
    
    
    

    <!-- Title -->
    
    <title>7.seq2seq - Hexo</title>

    <!-- Bootstrap Core CSS -->
    <link rel="stylesheet" href="//maxcdn.bootstrapcdn.com/bootstrap/4.0.0-alpha.2/css/bootstrap.min.css" integrity="sha384-y3tfxAZXuh4HwSYylfB+J125MxIs6mR5FOHamPBG064zB+AFeWH94NdvaCBm8qnd" crossorigin="anonymous">

    <!-- Custom Fonts -->
    <link href="//maxcdn.bootstrapcdn.com/font-awesome/4.6.3/css/font-awesome.min.css" rel="stylesheet" type="text/css">

    <!-- HTML5 Shim and Respond.js IE8 support of HTML5 elements and media queries -->
    <!-- WARNING: Respond.js doesn't work if you view the page via file:// -->
    <!--[if lt IE 9]>
        <script src="//oss.maxcdn.com/libs/html5shiv/3.7.0/html5shiv.js"></script>
        <script src="//oss.maxcdn.com/libs/respond.js/1.4.2/respond.min.js"></script>
    <![endif]-->

    <!-- Gallery -->
    <link href="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.css" type="text/css" rel="stylesheet" />

    <!-- Custom CSS -->
    
<link rel="stylesheet" href="/css/style.css">


    <!-- Google Analytics -->
    


<meta name="generator" content="Hexo 4.2.0"></head>


<body>

<div class="bg-gradient"></div>
<div class="bg-pattern"></div>

<!-- Menu -->
<!--Menu Links and Overlay-->
<div class="menu-bg">
    <div class="menu-container">
        <ul>
            
            <li class="menu-item">
                <a href="/archives">
                    Home
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/archives">
                    Archives
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/about.html">
                    About
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/tags">
                    Tags
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/categories">
                    Categories
                </a>
            </li>
            
            <li class="menu-item">
                <a href="/contact.html">
                    Contact
                </a>
            </li>
            
        </ul>
    </div>
</div>

<!--Hamburger Icon-->
<nav>
    <a href="#menu"></a>
</nav>

<div class="container">

    <!-- Main Content -->
    <div class="row">
    <div class="col-sm-12">

        <!--Title and Logo-->
        <header>
    <div class="logo">
        <a href="/"><i class="logo-icon fa fa-cube" aria-hidden="true"></i></a>
        
    </div>
</header>

        <section class="main">
            
<div class="post">

    <div class="post-header">
        <h1 class="title">
            <a href="/2020/02/29/7.seq2seq/">
                7.seq2seq
            </a>
        </h1>
        <div class="post-info">
            
                <span class="date">2020-02-29</span>
            
            
            
        </div>
    </div>

    <div class="content">

        <!-- Gallery -->
        

        <!-- Post Content -->
        <h1 id="第六课-Seq2Seq-Attention"><a href="#第六课-Seq2Seq-Attention" class="headerlink" title="第六课 Seq2Seq, Attention"></a>第六课 Seq2Seq, Attention</h1><p>褚则伟 <a href="mailto:zeweichu@gmail.com">zeweichu@gmail.com</a></p>
<p>在这份notebook当中，我们会(尽可能)复现Luong的attention模型</p>
<p>由于我们的数据集非常小，只有一万多个句子的训练数据，所以训练出来的模型效果并不好。如果大家想训练一个好一点的模型，可以参考下面的资料。</p>
<h2 id="更多阅读"><a href="#更多阅读" class="headerlink" title="更多阅读"></a>更多阅读</h2><h4 id="课件"><a href="#课件" class="headerlink" title="课件"></a>课件</h4><ul>
<li><a href="http://cs224d.stanford.edu/lectures/CS224d-Lecture15.pdf" target="_blank" rel="noopener">cs224d</a></li>
</ul>
<h4 id="论文"><a href="#论文" class="headerlink" title="论文"></a>论文</h4><ul>
<li><a href="https://arxiv.org/abs/1406.1078" target="_blank" rel="noopener">Learning Phrase Representations using RNN Encoder-Decoder for Statistical Machine Translation</a></li>
<li><a href="https://arxiv.org/abs/1508.04025?context=cs" target="_blank" rel="noopener">Effective Approaches to Attention-based Neural Machine Translation</a></li>
<li><a href="https://arxiv.org/abs/1406.1078" target="_blank" rel="noopener">Neural Machine Translation by Jointly Learning to Align and Translate</a></li>
</ul>
<h4 id="PyTorch代码"><a href="#PyTorch代码" class="headerlink" title="PyTorch代码"></a>PyTorch代码</h4><ul>
<li><a href="https://github.com/spro/practical-pytorch/blob/master/seq2seq-translation/seq2seq-translation.ipynb" target="_blank" rel="noopener">seq2seq-tutorial</a></li>
<li><a href="https://github.com/bentrevett/pytorch-seq2seq" target="_blank" rel="noopener">Tutorial from Ben Trevett</a></li>
<li><a href="https://github.com/IBM/pytorch-seq2seq" target="_blank" rel="noopener">IBM seq2seq</a></li>
<li><a href="https://github.com/OpenNMT/OpenNMT-py" target="_blank" rel="noopener">OpenNMT-py</a></li>
</ul>
<h4 id="更多关于Machine-Translation"><a href="#更多关于Machine-Translation" class="headerlink" title="更多关于Machine Translation"></a>更多关于Machine Translation</h4><ul>
<li><a href="https://www.coursera.org/lecture/nlp-sequence-models/beam-search-4EtHZ" target="_blank" rel="noopener">Beam Search</a></li>
<li>Pointer network 文本摘要</li>
<li>Copy Mechanism 文本摘要</li>
<li>Converage Loss </li>
<li>ConvSeq2Seq</li>
<li>Transformer</li>
<li>Tensor2Tensor</li>
</ul>
<h4 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h4><ul>
<li>建议同学尝试对中文进行分词</li>
</ul>
<h4 id="NER"><a href="#NER" class="headerlink" title="NER"></a>NER</h4><ul>
<li><a href="https://github.com/allenai/allennlp/tree/master/allennlp" target="_blank" rel="noopener">https://github.com/allenai/allennlp/tree/master/allennlp</a></li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> os</span><br><span class="line"><span class="keyword">import</span> sys</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">from</span> collections <span class="keyword">import</span> Counter</span><br><span class="line"><span class="keyword">import</span> numpy <span class="keyword">as</span> np</span><br><span class="line"><span class="keyword">import</span> random</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> torch</span><br><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="keyword">import</span> nltk</span><br></pre></td></tr></table></figure>

<p>读入中英文数据</p>
<ul>
<li>英文我们使用nltk的word tokenizer来分词，并且使用小写字母</li>
<li>中文我们直接使用单个汉字作为基本单元</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">load_data</span><span class="params">(in_file)</span>:</span></span><br><span class="line">    cn = []</span><br><span class="line">    en = []</span><br><span class="line">    num_examples = <span class="number">0</span></span><br><span class="line">    <span class="keyword">with</span> open(in_file, <span class="string">'r'</span>) <span class="keyword">as</span> f:</span><br><span class="line">        <span class="keyword">for</span> line <span class="keyword">in</span> f:</span><br><span class="line">            line = line.strip().split(<span class="string">"\t"</span>)</span><br><span class="line">            </span><br><span class="line">            en.append([<span class="string">"BOS"</span>] + nltk.word_tokenize(line[<span class="number">0</span>].lower()) + [<span class="string">"EOS"</span>])</span><br><span class="line">            <span class="comment"># split chinese sentence into characters</span></span><br><span class="line">            cn.append([<span class="string">"BOS"</span>] + [c <span class="keyword">for</span> c <span class="keyword">in</span> line[<span class="number">1</span>]] + [<span class="string">"EOS"</span>])</span><br><span class="line">    <span class="keyword">return</span> en, cn</span><br><span class="line"></span><br><span class="line">train_file = <span class="string">"nmt/en-cn/train.txt"</span></span><br><span class="line">dev_file = <span class="string">"nmt/en-cn/dev.txt"</span></span><br><span class="line">train_en, train_cn = load_data(train_file)</span><br><span class="line">dev_en, dev_cn = load_data(dev_file)</span><br></pre></td></tr></table></figure>

<p>构建单词表</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br></pre></td><td class="code"><pre><span class="line">UNK_IDX = <span class="number">0</span></span><br><span class="line">PAD_IDX = <span class="number">1</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">build_dict</span><span class="params">(sentences, max_words=<span class="number">50000</span>)</span>:</span></span><br><span class="line">    word_count = Counter()</span><br><span class="line">    <span class="keyword">for</span> sentence <span class="keyword">in</span> sentences:</span><br><span class="line">        <span class="keyword">for</span> s <span class="keyword">in</span> sentence:</span><br><span class="line">            word_count[s] += <span class="number">1</span></span><br><span class="line">    ls = word_count.most_common(max_words)</span><br><span class="line">    total_words = len(ls) + <span class="number">2</span></span><br><span class="line">    word_dict = &#123;w[<span class="number">0</span>]: index+<span class="number">2</span> <span class="keyword">for</span> index, w <span class="keyword">in</span> enumerate(ls)&#125;</span><br><span class="line">    word_dict[<span class="string">"UNK"</span>] = UNK_IDX</span><br><span class="line">    word_dict[<span class="string">"PAD"</span>] = PAD_IDX</span><br><span class="line">    <span class="keyword">return</span> word_dict, total_words</span><br><span class="line"></span><br><span class="line">en_dict, en_total_words = build_dict(train_en)</span><br><span class="line">cn_dict, cn_total_words = build_dict(train_cn)</span><br><span class="line">inv_en_dict = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> en_dict.items()&#125;</span><br><span class="line">inv_cn_dict = &#123;v: k <span class="keyword">for</span> k, v <span class="keyword">in</span> cn_dict.items()&#125;</span><br></pre></td></tr></table></figure>

<p>把单词全部转变成数字</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">encode</span><span class="params">(en_sentences, cn_sentences, en_dict, cn_dict, sort_by_len=True)</span>:</span></span><br><span class="line">    <span class="string">'''</span></span><br><span class="line"><span class="string">        Encode the sequences. </span></span><br><span class="line"><span class="string">    '''</span></span><br><span class="line">    length = len(en_sentences)</span><br><span class="line">    out_en_sentences = [[en_dict.get(w, <span class="number">0</span>) <span class="keyword">for</span> w <span class="keyword">in</span> sent] <span class="keyword">for</span> sent <span class="keyword">in</span> en_sentences]</span><br><span class="line">    out_cn_sentences = [[cn_dict.get(w, <span class="number">0</span>) <span class="keyword">for</span> w <span class="keyword">in</span> sent] <span class="keyword">for</span> sent <span class="keyword">in</span> cn_sentences]</span><br><span class="line"></span><br><span class="line">    <span class="comment"># sort sentences by english lengths</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">len_argsort</span><span class="params">(seq)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> sorted(range(len(seq)), key=<span class="keyword">lambda</span> x: len(seq[x]))</span><br><span class="line">       </span><br><span class="line">    <span class="comment"># 把中文和英文按照同样的顺序排序</span></span><br><span class="line">    <span class="keyword">if</span> sort_by_len:</span><br><span class="line">        sorted_index = len_argsort(out_en_sentences)</span><br><span class="line">        out_en_sentences = [out_en_sentences[i] <span class="keyword">for</span> i <span class="keyword">in</span> sorted_index]</span><br><span class="line">        out_cn_sentences = [out_cn_sentences[i] <span class="keyword">for</span> i <span class="keyword">in</span> sorted_index]</span><br><span class="line">        </span><br><span class="line">    <span class="keyword">return</span> out_en_sentences, out_cn_sentences</span><br><span class="line"></span><br><span class="line">train_en, train_cn = encode(train_en, train_cn, en_dict, cn_dict)</span><br><span class="line">dev_en, dev_cn = encode(dev_en, dev_cn, en_dict, cn_dict)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># train_cn[:10]</span></span><br><span class="line">k = <span class="number">10000</span></span><br><span class="line">print(<span class="string">" "</span>.join([inv_cn_dict[i] <span class="keyword">for</span> i <span class="keyword">in</span> train_cn[k]]))</span><br><span class="line">print(<span class="string">" "</span>.join([inv_en_dict[i] <span class="keyword">for</span> i <span class="keyword">in</span> train_en[k]]))</span><br></pre></td></tr></table></figure>

<pre><code>BOS 他 来 这 里 的 目 的 是 什 么 ？ EOS
BOS for what purpose did he come here ? EOS</code></pre><p>把全部句子分成batch</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">get_minibatches</span><span class="params">(n, minibatch_size, shuffle=True)</span>:</span></span><br><span class="line">    idx_list = np.arange(<span class="number">0</span>, n, minibatch_size) <span class="comment"># [0, 1, ..., n-1]</span></span><br><span class="line">    <span class="keyword">if</span> shuffle:</span><br><span class="line">        np.random.shuffle(idx_list)</span><br><span class="line">    minibatches = []</span><br><span class="line">    <span class="keyword">for</span> idx <span class="keyword">in</span> idx_list:</span><br><span class="line">        minibatches.append(np.arange(idx, min(idx + minibatch_size, n)))</span><br><span class="line">    <span class="keyword">return</span> minibatches</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">prepare_data</span><span class="params">(seqs)</span>:</span></span><br><span class="line">    lengths = [len(seq) <span class="keyword">for</span> seq <span class="keyword">in</span> seqs]</span><br><span class="line">    n_samples = len(seqs)</span><br><span class="line">    max_len = np.max(lengths)</span><br><span class="line"></span><br><span class="line">    x = np.zeros((n_samples, max_len)).astype(<span class="string">'int32'</span>)</span><br><span class="line">    x_lengths = np.array(lengths).astype(<span class="string">"int32"</span>)</span><br><span class="line">    <span class="keyword">for</span> idx, seq <span class="keyword">in</span> enumerate(seqs):</span><br><span class="line">        x[idx, :lengths[idx]] = seq</span><br><span class="line">    <span class="keyword">return</span> x, x_lengths <span class="comment">#x_mask</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">gen_examples</span><span class="params">(en_sentences, cn_sentences, batch_size)</span>:</span></span><br><span class="line">    minibatches = get_minibatches(len(en_sentences), batch_size)</span><br><span class="line">    all_ex = []</span><br><span class="line">    <span class="keyword">for</span> minibatch <span class="keyword">in</span> minibatches:</span><br><span class="line">        mb_en_sentences = [en_sentences[t] <span class="keyword">for</span> t <span class="keyword">in</span> minibatch]</span><br><span class="line">        mb_cn_sentences = [cn_sentences[t] <span class="keyword">for</span> t <span class="keyword">in</span> minibatch]</span><br><span class="line">        mb_x, mb_x_len = prepare_data(mb_en_sentences)</span><br><span class="line">        mb_y, mb_y_len = prepare_data(mb_cn_sentences)</span><br><span class="line">        all_ex.append((mb_x, mb_x_len, mb_y, mb_y_len))</span><br><span class="line">    <span class="keyword">return</span> all_ex</span><br><span class="line"></span><br><span class="line">batch_size = <span class="number">64</span></span><br><span class="line">train_data = gen_examples(train_en, train_cn, batch_size)</span><br><span class="line">random.shuffle(train_data)</span><br><span class="line">dev_data = gen_examples(dev_en, dev_cn, batch_size)</span><br></pre></td></tr></table></figure>

<h3 id="没有Attention的版本"><a href="#没有Attention的版本" class="headerlink" title="没有Attention的版本"></a>没有Attention的版本</h3><p>下面是一个更简单的没有Attention的encoder decoder模型</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br><span class="line">63</span><br><span class="line">64</span><br><span class="line">65</span><br><span class="line">66</span><br><span class="line">67</span><br><span class="line">68</span><br><span class="line">69</span><br><span class="line">70</span><br><span class="line">71</span><br><span class="line">72</span><br><span class="line">73</span><br><span class="line">74</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PlainEncoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, hidden_size, dropout=<span class="number">0.2</span>)</span>:</span></span><br><span class="line">        super(PlainEncoder, self).__init__()</span><br><span class="line">        self.embed = nn.Embedding(vocab_size, hidden_size)</span><br><span class="line">        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=<span class="literal">True</span>)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, lengths)</span>:</span></span><br><span class="line">        sorted_len, sorted_idx = lengths.sort(<span class="number">0</span>, descending=<span class="literal">True</span>)</span><br><span class="line">        x_sorted = x[sorted_idx.long()]</span><br><span class="line">        embedded = self.dropout(self.embed(x_sorted))</span><br><span class="line">        </span><br><span class="line">        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_len.long().cpu().data.numpy(), batch_first=<span class="literal">True</span>)</span><br><span class="line">        packed_out, hid = self.rnn(packed_embedded)</span><br><span class="line">        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=<span class="literal">True</span>)</span><br><span class="line">        _, original_idx = sorted_idx.sort(<span class="number">0</span>, descending=<span class="literal">False</span>)</span><br><span class="line">        out = out[original_idx.long()].contiguous()</span><br><span class="line">        hid = hid[:, original_idx.long()].contiguous()</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> out, hid[[<span class="number">-1</span>]]</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PlainDecoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, hidden_size, dropout=<span class="number">0.2</span>)</span>:</span></span><br><span class="line">        super(PlainDecoder, self).__init__()</span><br><span class="line">        self.embed = nn.Embedding(vocab_size, hidden_size)</span><br><span class="line">        self.rnn = nn.GRU(hidden_size, hidden_size, batch_first=<span class="literal">True</span>)</span><br><span class="line">        self.out = nn.Linear(hidden_size, vocab_size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, y, y_lengths, hid)</span>:</span></span><br><span class="line">        sorted_len, sorted_idx = y_lengths.sort(<span class="number">0</span>, descending=<span class="literal">True</span>)</span><br><span class="line">        y_sorted = y[sorted_idx.long()]</span><br><span class="line">        hid = hid[:, sorted_idx.long()]</span><br><span class="line"></span><br><span class="line">        y_sorted = self.dropout(self.embed(y_sorted)) <span class="comment"># batch_size, output_length, embed_size</span></span><br><span class="line"></span><br><span class="line">        packed_seq = nn.utils.rnn.pack_padded_sequence(y_sorted, sorted_len.long().cpu().data.numpy(), batch_first=<span class="literal">True</span>)</span><br><span class="line">        out, hid = self.rnn(packed_seq, hid)</span><br><span class="line">        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=<span class="literal">True</span>)</span><br><span class="line">        _, original_idx = sorted_idx.sort(<span class="number">0</span>, descending=<span class="literal">False</span>)</span><br><span class="line">        output_seq = unpacked[original_idx.long()].contiguous()</span><br><span class="line"><span class="comment">#         print(output_seq.shape)</span></span><br><span class="line">        hid = hid[:, original_idx.long()].contiguous()</span><br><span class="line"></span><br><span class="line">        output = F.log_softmax(self.out(output_seq), <span class="number">-1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> output, hid</span><br><span class="line">    </span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PlainSeq2Seq</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, encoder, decoder)</span>:</span></span><br><span class="line">        super(PlainSeq2Seq, self).__init__()</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, x_lengths, y, y_lengths)</span>:</span></span><br><span class="line">        encoder_out, hid = self.encoder(x, x_lengths)</span><br><span class="line">        output, hid = self.decoder(y=y,</span><br><span class="line">                    y_lengths=y_lengths,</span><br><span class="line">                    hid=hid)</span><br><span class="line">        <span class="keyword">return</span> output, <span class="literal">None</span></span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">translate</span><span class="params">(self, x, x_lengths, y, max_length=<span class="number">10</span>)</span>:</span></span><br><span class="line">        encoder_out, hid = self.encoder(x, x_lengths)</span><br><span class="line">        preds = []</span><br><span class="line">        batch_size = x.shape[<span class="number">0</span>]</span><br><span class="line">        attns = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(max_length):</span><br><span class="line">            output, hid = self.decoder(y=y,</span><br><span class="line">                    y_lengths=torch.ones(batch_size).long().to(y.device),</span><br><span class="line">                    hid=hid)</span><br><span class="line">            y = output.max(<span class="number">2</span>)[<span class="number">1</span>].view(batch_size, <span class="number">1</span>)</span><br><span class="line">            preds.append(y)</span><br><span class="line">            </span><br><span class="line">        <span class="keyword">return</span> torch.cat(preds, <span class="number">1</span>), <span class="literal">None</span></span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># masked cross entropy loss</span></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">LanguageModelCriterion</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(LanguageModelCriterion, self).__init__()</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, input, target, mask)</span>:</span></span><br><span class="line">        <span class="comment"># input: (batch_size * seq_len) * vocab_size</span></span><br><span class="line">        input = input.contiguous().view(<span class="number">-1</span>, input.size(<span class="number">2</span>))</span><br><span class="line">        <span class="comment"># target: batch_size * 1</span></span><br><span class="line">        target = target.contiguous().view(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">        mask = mask.contiguous().view(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">        output = -input.gather(<span class="number">1</span>, target) * mask</span><br><span class="line">        output = torch.sum(output) / torch.sum(mask)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> output</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">device = torch.device(<span class="string">"cuda"</span> <span class="keyword">if</span> torch.cuda.is_available() <span class="keyword">else</span> <span class="string">"cpu"</span>)</span><br><span class="line">dropout = <span class="number">0.2</span></span><br><span class="line">hidden_size = <span class="number">100</span></span><br><span class="line">encoder = PlainEncoder(vocab_size=en_total_words,</span><br><span class="line">                      hidden_size=hidden_size,</span><br><span class="line">                      dropout=dropout)</span><br><span class="line">decoder = PlainDecoder(vocab_size=cn_total_words,</span><br><span class="line">                      hidden_size=hidden_size,</span><br><span class="line">                      dropout=dropout)</span><br><span class="line">model = PlainSeq2Seq(encoder, decoder)</span><br><span class="line">model = model.to(device)</span><br><span class="line">loss_fn = LanguageModelCriterion().to(device)</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters())</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">evaluate</span><span class="params">(model, data)</span>:</span></span><br><span class="line">    model.eval()</span><br><span class="line">    total_num_words = total_loss = <span class="number">0.</span></span><br><span class="line">    <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">        <span class="keyword">for</span> it, (mb_x, mb_x_len, mb_y, mb_y_len) <span class="keyword">in</span> enumerate(data):</span><br><span class="line">            mb_x = torch.from_numpy(mb_x).to(device).long()</span><br><span class="line">            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()</span><br><span class="line">            mb_input = torch.from_numpy(mb_y[:, :<span class="number">-1</span>]).to(device).long()</span><br><span class="line">            mb_output = torch.from_numpy(mb_y[:, <span class="number">1</span>:]).to(device).long()</span><br><span class="line">            mb_y_len = torch.from_numpy(mb_y_len<span class="number">-1</span>).to(device).long()</span><br><span class="line">            mb_y_len[mb_y_len&lt;=<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line"></span><br><span class="line">            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)</span><br><span class="line"></span><br><span class="line">            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[<span class="literal">None</span>, :] &lt; mb_y_len[:, <span class="literal">None</span>]</span><br><span class="line">            mb_out_mask = mb_out_mask.float()</span><br><span class="line"></span><br><span class="line">            loss = loss_fn(mb_pred, mb_output, mb_out_mask)</span><br><span class="line"></span><br><span class="line">            num_words = torch.sum(mb_y_len).item()</span><br><span class="line">            total_loss += loss.item() * num_words</span><br><span class="line">            total_num_words += num_words</span><br><span class="line">    print(<span class="string">"Evaluation loss"</span>, total_loss/total_num_words)</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">train</span><span class="params">(model, data, num_epochs=<span class="number">20</span>)</span>:</span></span><br><span class="line">    <span class="keyword">for</span> epoch <span class="keyword">in</span> range(num_epochs):</span><br><span class="line">        model.train()</span><br><span class="line">        total_num_words = total_loss = <span class="number">0.</span></span><br><span class="line">        <span class="keyword">for</span> it, (mb_x, mb_x_len, mb_y, mb_y_len) <span class="keyword">in</span> enumerate(data):</span><br><span class="line">            mb_x = torch.from_numpy(mb_x).to(device).long()</span><br><span class="line">            mb_x_len = torch.from_numpy(mb_x_len).to(device).long()</span><br><span class="line">            mb_input = torch.from_numpy(mb_y[:, :<span class="number">-1</span>]).to(device).long()</span><br><span class="line">            mb_output = torch.from_numpy(mb_y[:, <span class="number">1</span>:]).to(device).long()</span><br><span class="line">            mb_y_len = torch.from_numpy(mb_y_len<span class="number">-1</span>).to(device).long()</span><br><span class="line">            mb_y_len[mb_y_len&lt;=<span class="number">0</span>] = <span class="number">1</span></span><br><span class="line">            </span><br><span class="line">            mb_pred, attn = model(mb_x, mb_x_len, mb_input, mb_y_len)</span><br><span class="line">            </span><br><span class="line">            mb_out_mask = torch.arange(mb_y_len.max().item(), device=device)[<span class="literal">None</span>, :] &lt; mb_y_len[:, <span class="literal">None</span>]</span><br><span class="line">            mb_out_mask = mb_out_mask.float()</span><br><span class="line">            </span><br><span class="line">            loss = loss_fn(mb_pred, mb_output, mb_out_mask)</span><br><span class="line">            </span><br><span class="line">            num_words = torch.sum(mb_y_len).item()</span><br><span class="line">            total_loss += loss.item() * num_words</span><br><span class="line">            total_num_words += num_words</span><br><span class="line">            </span><br><span class="line">            <span class="comment"># 更新模型</span></span><br><span class="line">            optimizer.zero_grad()</span><br><span class="line">            loss.backward()</span><br><span class="line">            torch.nn.utils.clip_grad_norm_(model.parameters(), <span class="number">5.</span>)</span><br><span class="line">            optimizer.step()</span><br><span class="line">            </span><br><span class="line">            <span class="keyword">if</span> it % <span class="number">100</span> == <span class="number">0</span>:</span><br><span class="line">                print(<span class="string">"Epoch"</span>, epoch, <span class="string">"iteration"</span>, it, <span class="string">"loss"</span>, loss.item())</span><br><span class="line"></span><br><span class="line">                </span><br><span class="line">        print(<span class="string">"Epoch"</span>, epoch, <span class="string">"Training loss"</span>, total_loss/total_num_words)</span><br><span class="line">        <span class="keyword">if</span> epoch % <span class="number">5</span> == <span class="number">0</span>:</span><br><span class="line">            evaluate(model, dev_data)</span><br><span class="line">train(model, train_data, num_epochs=<span class="number">20</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Epoch 0 iteration 0 loss 8.050323486328125
Epoch 0 iteration 100 loss 5.278979301452637
Epoch 0 iteration 200 loss 4.444733619689941
Epoch 0 Training loss 5.433318799975385
Evaluation loss 4.822829000278033
Epoch 1 iteration 0 loss 4.692166805267334
Epoch 1 iteration 100 loss 4.708909511566162
Epoch 1 iteration 200 loss 3.8643922805786133
Epoch 1 Training loss 4.5993410716009135
Epoch 2 iteration 0 loss 4.17959451675415
Epoch 2 iteration 100 loss 4.352121829986572
Epoch 2 iteration 200 loss 3.5356297492980957
Epoch 2 Training loss 4.198561833806036
Epoch 3 iteration 0 loss 3.8728413581848145
Epoch 3 iteration 100 loss 4.134408950805664
Epoch 3 iteration 200 loss 3.303772211074829
Epoch 3 Training loss 3.9386860033522813
Epoch 4 iteration 0 loss 3.64646053314209
Epoch 4 iteration 100 loss 3.947233200073242
Epoch 4 iteration 200 loss 3.1333234310150146
Epoch 4 Training loss 3.745685762442693
Epoch 5 iteration 0 loss 3.481276035308838
Epoch 5 iteration 100 loss 3.827454090118408
Epoch 5 iteration 200 loss 2.9994454383850098
Epoch 5 Training loss 3.5913285724858954
Evaluation loss 3.6815984345855037
Epoch 6 iteration 0 loss 3.3354697227478027
Epoch 6 iteration 100 loss 3.6918392181396484
Epoch 6 iteration 200 loss 2.8618223667144775
Epoch 6 Training loss 3.465248799091302
Epoch 7 iteration 0 loss 3.2224643230438232
Epoch 7 iteration 100 loss 3.5980327129364014
Epoch 7 iteration 200 loss 2.783277988433838
Epoch 7 Training loss 3.357013859409834
Epoch 8 iteration 0 loss 3.141510248184204
Epoch 8 iteration 100 loss 3.5131657123565674
Epoch 8 iteration 200 loss 2.715005397796631
Epoch 8 Training loss 3.2614931554428166
Epoch 9 iteration 0 loss 3.0618908405303955
Epoch 9 iteration 100 loss 3.4437694549560547
Epoch 9 iteration 200 loss 2.5995192527770996
Epoch 9 Training loss 3.1806184197973404
Epoch 10 iteration 0 loss 2.9288880825042725
Epoch 10 iteration 100 loss 3.350996971130371
Epoch 10 iteration 200 loss 2.5103659629821777
Epoch 10 Training loss 3.101915731518774
Evaluation loss 3.393061912401112
Epoch 11 iteration 0 loss 2.874830722808838
Epoch 11 iteration 100 loss 3.3034920692443848
Epoch 11 iteration 200 loss 2.4885127544403076
Epoch 11 Training loss 3.0369929761565384
Epoch 12 iteration 0 loss 2.8056483268737793
Epoch 12 iteration 100 loss 3.2505335807800293
Epoch 12 iteration 200 loss 2.4071717262268066
Epoch 12 Training loss 2.973809002606383
Epoch 13 iteration 0 loss 2.7353591918945312
Epoch 13 iteration 100 loss 3.178480863571167
Epoch 13 iteration 200 loss 2.3422422409057617
Epoch 13 Training loss 2.9169208222083847
Epoch 14 iteration 0 loss 2.6794426441192627
Epoch 14 iteration 100 loss 3.129685401916504
Epoch 14 iteration 200 loss 2.3255887031555176
Epoch 14 Training loss 2.86419656519231
Epoch 15 iteration 0 loss 2.6482393741607666
Epoch 15 iteration 100 loss 3.0710315704345703
Epoch 15 iteration 200 loss 2.2372782230377197
Epoch 15 Training loss 2.8170104509222287
Evaluation loss 3.2708830728055336
Epoch 16 iteration 0 loss 2.567857503890991
Epoch 16 iteration 100 loss 3.0710268020629883
Epoch 16 iteration 200 loss 2.238800525665283
Epoch 16 Training loss 2.771683479683666
Epoch 17 iteration 0 loss 2.5122745037078857
Epoch 17 iteration 100 loss 3.002455472946167
Epoch 17 iteration 200 loss 2.1964993476867676
Epoch 17 Training loss 2.733348611161267
Epoch 18 iteration 0 loss 2.49585223197937
Epoch 18 iteration 100 loss 2.971094846725464
Epoch 18 iteration 200 loss 2.1383423805236816
Epoch 18 Training loss 2.6926882812821322
Epoch 19 iteration 0 loss 2.436241388320923
Epoch 19 iteration 100 loss 2.942230224609375
Epoch 19 iteration 200 loss 2.0685524940490723
Epoch 19 Training loss 2.6545419067862515</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">translate_dev</span><span class="params">(i)</span>:</span></span><br><span class="line">    en_sent = <span class="string">" "</span>.join([inv_en_dict[w] <span class="keyword">for</span> w <span class="keyword">in</span> dev_en[i]])</span><br><span class="line">    print(en_sent)</span><br><span class="line">    cn_sent = <span class="string">" "</span>.join([inv_cn_dict[w] <span class="keyword">for</span> w <span class="keyword">in</span> dev_cn[i]])</span><br><span class="line">    print(<span class="string">""</span>.join(cn_sent))</span><br><span class="line"></span><br><span class="line">    mb_x = torch.from_numpy(np.array(dev_en[i]).reshape(<span class="number">1</span>, <span class="number">-1</span>)).long().to(device)</span><br><span class="line">    mb_x_len = torch.from_numpy(np.array([len(dev_en[i])])).long().to(device)</span><br><span class="line">    bos = torch.Tensor([[cn_dict[<span class="string">"BOS"</span>]]]).long().to(device)</span><br><span class="line"></span><br><span class="line">    translation, attn = model.translate(mb_x, mb_x_len, bos)</span><br><span class="line">    translation = [inv_cn_dict[i] <span class="keyword">for</span> i <span class="keyword">in</span> translation.data.cpu().numpy().reshape(<span class="number">-1</span>)]</span><br><span class="line">    trans = []</span><br><span class="line">    <span class="keyword">for</span> word <span class="keyword">in</span> translation:</span><br><span class="line">        <span class="keyword">if</span> word != <span class="string">"EOS"</span>:</span><br><span class="line">            trans.append(word)</span><br><span class="line">        <span class="keyword">else</span>:</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    print(<span class="string">""</span>.join(trans))</span><br><span class="line"></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>,<span class="number">120</span>):</span><br><span class="line">    translate_dev(i)</span><br><span class="line">    print()</span><br></pre></td></tr></table></figure>

<pre><code>BOS you have nice skin . EOS
BOS 你 的 皮 膚 真 好 。 EOS
你必須吃。

BOS you &apos;re UNK correct . EOS
BOS 你 部 分 正 确 。 EOS
你是一个好的。

BOS everyone admired his courage . EOS
BOS 每 個 人 都 佩 服 他 的 勇 氣 。 EOS
每个人都在学习。

BOS what time is it ? EOS
BOS 几 点 了 ？ EOS
它什么是谁？

BOS i &apos;m free tonight . EOS
BOS 我 今 晚 有 空 。 EOS
我很快就會。

BOS here is your book . EOS
BOS 這 是 你 的 書 。 EOS
這是你的。

BOS they are at lunch . EOS
BOS 他 们 在 吃 午 饭 。 EOS
他们有个大学。

BOS this chair is UNK . EOS
BOS 這 把 椅 子 很 UNK 。 EOS
這個房間是一個人的。

BOS it &apos;s pretty heavy . EOS
BOS 它 真 重 。 EOS
它是一個好的。

BOS many attended his funeral . EOS
BOS 很 多 人 都 参 加 了 他 的 葬 礼 。 EOS
許多的人都喜歡茶。

BOS training will be provided . EOS
BOS 会 有 训 练 。 EOS
要下雨。

BOS someone is watching you . EOS
BOS 有 人 在 看 著 你 。 EOS
有人是你的。

BOS i slapped his face . EOS
BOS 我 摑 了 他 的 臉 。 EOS
我認為他的手臂。

BOS i like UNK music . EOS
BOS 我 喜 歡 流 行 音 樂 。 EOS
我喜歡打棒球。

BOS tom had no children . EOS
BOS T o m 沒 有 孩 子 。 EOS
汤姆没有人。

BOS please lock the door . EOS
BOS 請 把 門 鎖 上 。 EOS
請把你的車。

BOS tom has calmed down . EOS
BOS 汤 姆 冷 静 下 来 了 。 EOS
汤姆在花園裡。

BOS please speak more loudly . EOS
BOS 請 說 大 聲 一 點 兒 。 EOS
請稍好喝咖啡。

BOS keep next sunday free . EOS
BOS 把 下 周 日 空 出 来 。 EOS
繼續工作很多。

BOS i made a mistake . EOS
BOS 我 犯 了 一 個 錯 。 EOS
我是一個小孩。</code></pre><p>数据全部处理完成，现在我们开始构建seq2seq模型</p>
<h4 id="Encoder"><a href="#Encoder" class="headerlink" title="Encoder"></a>Encoder</h4><ul>
<li>Encoder模型的任务是把输入文字传入embedding层和GRU层，转换成一些hidden states作为后续的context vectors</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Encoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=<span class="number">0.2</span>)</span>:</span></span><br><span class="line">        super(Encoder, self).__init__()</span><br><span class="line">        self.embed = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.rnn = nn.GRU(embed_size, enc_hidden_size, batch_first=<span class="literal">True</span>, bidirectional=<span class="literal">True</span>)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line">        self.fc = nn.Linear(enc_hidden_size * <span class="number">2</span>, dec_hidden_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, lengths)</span>:</span></span><br><span class="line">        sorted_len, sorted_idx = lengths.sort(<span class="number">0</span>, descending=<span class="literal">True</span>)</span><br><span class="line">        x_sorted = x[sorted_idx.long()]</span><br><span class="line">        embedded = self.dropout(self.embed(x_sorted))</span><br><span class="line">        </span><br><span class="line">        packed_embedded = nn.utils.rnn.pack_padded_sequence(embedded, sorted_len.long().cpu().data.numpy(), batch_first=<span class="literal">True</span>)</span><br><span class="line">        packed_out, hid = self.rnn(packed_embedded)</span><br><span class="line">        out, _ = nn.utils.rnn.pad_packed_sequence(packed_out, batch_first=<span class="literal">True</span>)</span><br><span class="line">        _, original_idx = sorted_idx.sort(<span class="number">0</span>, descending=<span class="literal">False</span>)</span><br><span class="line">        out = out[original_idx.long()].contiguous()</span><br><span class="line">        hid = hid[:, original_idx.long()].contiguous()</span><br><span class="line">        </span><br><span class="line">        hid = torch.cat([hid[<span class="number">-2</span>], hid[<span class="number">-1</span>]], dim=<span class="number">1</span>)</span><br><span class="line">        hid = torch.tanh(self.fc(hid)).unsqueeze(<span class="number">0</span>)</span><br><span class="line"></span><br><span class="line">        <span class="keyword">return</span> out, hid</span><br></pre></td></tr></table></figure>

<h4 id="Luong-Attention"><a href="#Luong-Attention" class="headerlink" title="Luong Attention"></a>Luong Attention</h4><ul>
<li>根据context vectors和当前的输出hidden states，计算输出</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Attention</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, enc_hidden_size, dec_hidden_size)</span>:</span></span><br><span class="line">        super(Attention, self).__init__()</span><br><span class="line"></span><br><span class="line">        self.enc_hidden_size = enc_hidden_size</span><br><span class="line">        self.dec_hidden_size = dec_hidden_size</span><br><span class="line"></span><br><span class="line">        self.linear_in = nn.Linear(enc_hidden_size*<span class="number">2</span>, dec_hidden_size, bias=<span class="literal">False</span>)</span><br><span class="line">        self.linear_out = nn.Linear(enc_hidden_size*<span class="number">2</span> + dec_hidden_size, dec_hidden_size)</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, output, context, mask)</span>:</span></span><br><span class="line">        <span class="comment"># output: batch_size, output_len, dec_hidden_size</span></span><br><span class="line">        <span class="comment"># context: batch_size, context_len, 2*enc_hidden_size</span></span><br><span class="line">    </span><br><span class="line">        batch_size = output.size(<span class="number">0</span>)</span><br><span class="line">        output_len = output.size(<span class="number">1</span>)</span><br><span class="line">        input_len = context.size(<span class="number">1</span>)</span><br><span class="line">        </span><br><span class="line">        context_in = self.linear_in(context.view(batch_size*input_len, <span class="number">-1</span>)).view(                </span><br><span class="line">            batch_size, input_len, <span class="number">-1</span>) <span class="comment"># batch_size, context_len, dec_hidden_size</span></span><br><span class="line">        </span><br><span class="line">        <span class="comment"># context_in.transpose(1,2): batch_size, dec_hidden_size, context_len </span></span><br><span class="line">        <span class="comment"># output: batch_size, output_len, dec_hidden_size</span></span><br><span class="line">        attn = torch.bmm(output, context_in.transpose(<span class="number">1</span>,<span class="number">2</span>)) </span><br><span class="line">        <span class="comment"># batch_size, output_len, context_len</span></span><br><span class="line"></span><br><span class="line">        attn.data.masked_fill(mask, <span class="number">-1e6</span>)</span><br><span class="line"></span><br><span class="line">        attn = F.softmax(attn, dim=<span class="number">2</span>) </span><br><span class="line">        <span class="comment"># batch_size, output_len, context_len</span></span><br><span class="line"></span><br><span class="line">        context = torch.bmm(attn, context) </span><br><span class="line">        <span class="comment"># batch_size, output_len, enc_hidden_size</span></span><br><span class="line">        </span><br><span class="line">        output = torch.cat((context, output), dim=<span class="number">2</span>) <span class="comment"># batch_size, output_len, hidden_size*2</span></span><br><span class="line"></span><br><span class="line">        output = output.view(batch_size*output_len, <span class="number">-1</span>)</span><br><span class="line">        output = torch.tanh(self.linear_out(output))</span><br><span class="line">        output = output.view(batch_size, output_len, <span class="number">-1</span>)</span><br><span class="line">        <span class="keyword">return</span> output, attn</span><br></pre></td></tr></table></figure>

<h4 id="Decoder"><a href="#Decoder" class="headerlink" title="Decoder"></a>Decoder</h4><ul>
<li>decoder会根据已经翻译的句子内容，和context vectors，来决定下一个输出的单词</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Decoder</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, vocab_size, embed_size, enc_hidden_size, dec_hidden_size, dropout=<span class="number">0.2</span>)</span>:</span></span><br><span class="line">        super(Decoder, self).__init__()</span><br><span class="line">        self.embed = nn.Embedding(vocab_size, embed_size)</span><br><span class="line">        self.attention = Attention(enc_hidden_size, dec_hidden_size)</span><br><span class="line">        self.rnn = nn.GRU(embed_size, hidden_size, batch_first=<span class="literal">True</span>)</span><br><span class="line">        self.out = nn.Linear(dec_hidden_size, vocab_size)</span><br><span class="line">        self.dropout = nn.Dropout(dropout)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">create_mask</span><span class="params">(self, x_len, y_len)</span>:</span></span><br><span class="line">        <span class="comment"># a mask of shape x_len * y_len</span></span><br><span class="line">        device = x_len.device</span><br><span class="line">        max_x_len = x_len.max()</span><br><span class="line">        max_y_len = y_len.max()</span><br><span class="line">        x_mask = torch.arange(max_x_len, device=x_len.device)[<span class="literal">None</span>, :] &lt; x_len[:, <span class="literal">None</span>]</span><br><span class="line">        y_mask = torch.arange(max_y_len, device=x_len.device)[<span class="literal">None</span>, :] &lt; y_len[:, <span class="literal">None</span>]</span><br><span class="line">        mask = (<span class="number">1</span> - x_mask[:, :, <span class="literal">None</span>] * y_mask[:, <span class="literal">None</span>, :]).byte()</span><br><span class="line">        <span class="keyword">return</span> mask</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, ctx, ctx_lengths, y, y_lengths, hid)</span>:</span></span><br><span class="line">        sorted_len, sorted_idx = y_lengths.sort(<span class="number">0</span>, descending=<span class="literal">True</span>)</span><br><span class="line">        y_sorted = y[sorted_idx.long()]</span><br><span class="line">        hid = hid[:, sorted_idx.long()]</span><br><span class="line">        </span><br><span class="line">        y_sorted = self.dropout(self.embed(y_sorted)) <span class="comment"># batch_size, output_length, embed_size</span></span><br><span class="line"></span><br><span class="line">        packed_seq = nn.utils.rnn.pack_padded_sequence(y_sorted, sorted_len.long().cpu().data.numpy(), batch_first=<span class="literal">True</span>)</span><br><span class="line">        out, hid = self.rnn(packed_seq, hid)</span><br><span class="line">        unpacked, _ = nn.utils.rnn.pad_packed_sequence(out, batch_first=<span class="literal">True</span>)</span><br><span class="line">        _, original_idx = sorted_idx.sort(<span class="number">0</span>, descending=<span class="literal">False</span>)</span><br><span class="line">        output_seq = unpacked[original_idx.long()].contiguous()</span><br><span class="line">        hid = hid[:, original_idx.long()].contiguous()</span><br><span class="line"></span><br><span class="line">        mask = self.create_mask(y_lengths, ctx_lengths)</span><br><span class="line"></span><br><span class="line">        output, attn = self.attention(output_seq, ctx, mask)</span><br><span class="line">        output = F.log_softmax(self.out(output), <span class="number">-1</span>)</span><br><span class="line">        </span><br><span class="line">        <span class="keyword">return</span> output, hid, attn</span><br></pre></td></tr></table></figure>

<h4 id="Seq2Seq"><a href="#Seq2Seq" class="headerlink" title="Seq2Seq"></a>Seq2Seq</h4><ul>
<li>最后我们构建Seq2Seq模型把encoder, attention, decoder串到一起</li>
</ul>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Seq2Seq</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, encoder, decoder)</span>:</span></span><br><span class="line">        super(Seq2Seq, self).__init__()</span><br><span class="line">        self.encoder = encoder</span><br><span class="line">        self.decoder = decoder</span><br><span class="line">        </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x, x_lengths, y, y_lengths)</span>:</span></span><br><span class="line">        encoder_out, hid = self.encoder(x, x_lengths)</span><br><span class="line">        output, hid, attn = self.decoder(ctx=encoder_out, </span><br><span class="line">                    ctx_lengths=x_lengths,</span><br><span class="line">                    y=y,</span><br><span class="line">                    y_lengths=y_lengths,</span><br><span class="line">                    hid=hid)</span><br><span class="line">        <span class="keyword">return</span> output, attn</span><br><span class="line">    </span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">translate</span><span class="params">(self, x, x_lengths, y, max_length=<span class="number">100</span>)</span>:</span></span><br><span class="line">        encoder_out, hid = self.encoder(x, x_lengths)</span><br><span class="line">        preds = []</span><br><span class="line">        batch_size = x.shape[<span class="number">0</span>]</span><br><span class="line">        attns = []</span><br><span class="line">        <span class="keyword">for</span> i <span class="keyword">in</span> range(max_length):</span><br><span class="line">            output, hid, attn = self.decoder(ctx=encoder_out, </span><br><span class="line">                    ctx_lengths=x_lengths,</span><br><span class="line">                    y=y,</span><br><span class="line">                    y_lengths=torch.ones(batch_size).long().to(y.device),</span><br><span class="line">                    hid=hid)</span><br><span class="line">            y = output.max(<span class="number">2</span>)[<span class="number">1</span>].view(batch_size, <span class="number">1</span>)</span><br><span class="line">            preds.append(y)</span><br><span class="line">            attns.append(attn)</span><br><span class="line">        <span class="keyword">return</span> torch.cat(preds, <span class="number">1</span>), torch.cat(attns, <span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>训练</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">dropout = <span class="number">0.2</span></span><br><span class="line">embed_size = hidden_size = <span class="number">100</span></span><br><span class="line">encoder = Encoder(vocab_size=en_total_words,</span><br><span class="line">                       embed_size=embed_size,</span><br><span class="line">                      enc_hidden_size=hidden_size,</span><br><span class="line">                       dec_hidden_size=hidden_size,</span><br><span class="line">                      dropout=dropout)</span><br><span class="line">decoder = Decoder(vocab_size=cn_total_words,</span><br><span class="line">                      embed_size=embed_size,</span><br><span class="line">                      enc_hidden_size=hidden_size,</span><br><span class="line">                       dec_hidden_size=hidden_size,</span><br><span class="line">                      dropout=dropout)</span><br><span class="line">model = Seq2Seq(encoder, decoder)</span><br><span class="line">model = model.to(device)</span><br><span class="line">loss_fn = LanguageModelCriterion().to(device)</span><br><span class="line">optimizer = torch.optim.Adam(model.parameters())</span><br></pre></td></tr></table></figure>


<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">train(model, train_data, num_epochs=<span class="number">30</span>)</span><br></pre></td></tr></table></figure>

<pre><code>Epoch 0 iteration 0 loss 8.078022003173828
Epoch 0 iteration 100 loss 5.414377689361572
Epoch 0 iteration 200 loss 4.643333435058594
Epoch 0 Training loss 5.485134587536152
Evaluation loss 5.067514630874862
Epoch 1 iteration 0 loss 4.940210342407227
Epoch 1 iteration 100 loss 4.9903435707092285
Epoch 1 iteration 200 loss 4.186498641967773
Epoch 1 Training loss 4.877356682952294
Epoch 2 iteration 0 loss 4.509239196777344
Epoch 2 iteration 100 loss 4.570853233337402
Epoch 2 iteration 200 loss 3.7934508323669434
Epoch 2 Training loss 4.453642889638262
Epoch 3 iteration 0 loss 4.11014986038208
Epoch 3 iteration 100 loss 4.230580806732178
Epoch 3 iteration 200 loss 3.4451844692230225
Epoch 3 Training loss 4.105205834096106
Epoch 4 iteration 0 loss 3.788179397583008
Epoch 4 iteration 100 loss 3.984476089477539
Epoch 4 iteration 200 loss 3.205059289932251
Epoch 4 Training loss 3.8313639103406314
Epoch 5 iteration 0 loss 3.572876214981079
Epoch 5 iteration 100 loss 3.7907521724700928
Epoch 5 iteration 200 loss 3.0604655742645264
Epoch 5 Training loss 3.61275750220716
Evaluation loss 3.6225900108158475
Epoch 6 iteration 0 loss 3.331376552581787
Epoch 6 iteration 100 loss 3.607234239578247
Epoch 6 iteration 200 loss 2.8438034057617188
Epoch 6 Training loss 3.4240881394610914
Epoch 7 iteration 0 loss 3.1553823947906494
Epoch 7 iteration 100 loss 3.4283368587493896
Epoch 7 iteration 200 loss 2.679870367050171
Epoch 7 Training loss 3.2619650765874195
Epoch 8 iteration 0 loss 3.0175576210021973
Epoch 8 iteration 100 loss 3.313087224960327
Epoch 8 iteration 200 loss 2.573970079421997
Epoch 8 Training loss 3.119750910546451
Epoch 9 iteration 0 loss 2.8687644004821777
Epoch 9 iteration 100 loss 3.2016961574554443
Epoch 9 iteration 200 loss 2.4501001834869385
Epoch 9 Training loss 2.9937007481445184
Epoch 10 iteration 0 loss 2.7964212894439697
Epoch 10 iteration 100 loss 3.094231128692627
Epoch 10 iteration 200 loss 2.2865397930145264
Epoch 10 Training loss 2.879919764606877
Evaluation loss 3.164760209368642
Epoch 11 iteration 0 loss 2.6683473587036133
Epoch 11 iteration 100 loss 3.008727788925171
Epoch 11 iteration 200 loss 2.1880834102630615
Epoch 11 Training loss 2.7794466071573467
Epoch 12 iteration 0 loss 2.5640454292297363
Epoch 12 iteration 100 loss 2.896376132965088
Epoch 12 iteration 200 loss 2.1036128997802734
Epoch 12 Training loss 2.684113484535982
Epoch 13 iteration 0 loss 2.520007371902466
Epoch 13 iteration 100 loss 2.8189423084259033
Epoch 13 iteration 200 loss 2.0698890686035156
Epoch 13 Training loss 2.5990255668547055
Epoch 14 iteration 0 loss 2.42832612991333
Epoch 14 iteration 100 loss 2.7819204330444336
Epoch 14 iteration 200 loss 1.923954725265503
Epoch 14 Training loss 2.5176252404633574
Epoch 15 iteration 0 loss 2.360988140106201
Epoch 15 iteration 100 loss 2.6843974590301514
Epoch 15 iteration 200 loss 1.912152886390686
Epoch 15 Training loss 2.4463321701504275
Evaluation loss 2.9698491313827047
Epoch 16 iteration 0 loss 2.2877912521362305
Epoch 16 iteration 100 loss 2.6055469512939453
Epoch 16 iteration 200 loss 1.8231658935546875
Epoch 16 Training loss 2.3756549535366713
Epoch 17 iteration 0 loss 2.191697597503662
Epoch 17 iteration 100 loss 2.5865063667297363
Epoch 17 iteration 200 loss 1.7817124128341675
Epoch 17 Training loss 2.313343924902058
Epoch 18 iteration 0 loss 2.1245803833007812
Epoch 18 iteration 100 loss 2.525496482849121
Epoch 18 iteration 200 loss 1.672200322151184
Epoch 18 Training loss 2.2498218108556114
Epoch 19 iteration 0 loss 2.06477427482605
Epoch 19 iteration 100 loss 2.443316698074341
Epoch 19 iteration 200 loss 1.6326298713684082
Epoch 19 Training loss 2.19988960411091
Epoch 20 iteration 0 loss 2.0234487056732178
Epoch 20 iteration 100 loss 2.416968822479248
Epoch 20 iteration 200 loss 1.583616852760315
Epoch 20 Training loss 2.1513965044733827
Evaluation loss 2.8699020465835643
Epoch 21 iteration 0 loss 2.008730411529541
Epoch 21 iteration 100 loss 2.3642444610595703
Epoch 21 iteration 200 loss 1.5385680198669434
Epoch 21 Training loss 2.098746986360735
Epoch 22 iteration 0 loss 1.910429835319519
Epoch 22 iteration 100 loss 2.339489459991455
Epoch 22 iteration 200 loss 1.4784246683120728
Epoch 22 Training loss 2.051404798098097
Epoch 23 iteration 0 loss 1.8959044218063354
Epoch 23 iteration 100 loss 2.2653536796569824
Epoch 23 iteration 200 loss 1.4792706966400146
Epoch 23 Training loss 2.00636701965731
Epoch 24 iteration 0 loss 1.8477107286453247
Epoch 24 iteration 100 loss 2.1904118061065674
Epoch 24 iteration 200 loss 1.3925689458847046
Epoch 24 Training loss 1.965628425139225
Epoch 25 iteration 0 loss 1.7790645360946655
Epoch 25 iteration 100 loss 2.182420492172241
Epoch 25 iteration 200 loss 1.3576843738555908
Epoch 25 Training loss 1.9238889035465652
Evaluation loss 2.826008448512912
Epoch 26 iteration 0 loss 1.73543381690979
Epoch 26 iteration 100 loss 2.1740329265594482
Epoch 26 iteration 200 loss 1.328704595565796
Epoch 26 Training loss 1.889945533318946
Epoch 27 iteration 0 loss 1.7498269081115723
Epoch 27 iteration 100 loss 2.1384894847869873
Epoch 27 iteration 200 loss 1.277467966079712
Epoch 27 Training loss 1.852515173441663
Epoch 28 iteration 0 loss 1.6980342864990234
Epoch 28 iteration 100 loss 2.1195883750915527
Epoch 28 iteration 200 loss 1.2595137357711792
Epoch 28 Training loss 1.8210893462516964
Epoch 29 iteration 0 loss 1.6773594617843628
Epoch 29 iteration 100 loss 2.0760860443115234
Epoch 29 iteration 200 loss 1.2345834970474243
Epoch 29 Training loss 1.7873437400435428</code></pre><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">100</span>,<span class="number">120</span>):</span><br><span class="line">    translate_dev(i)</span><br><span class="line">    print()</span><br></pre></td></tr></table></figure>

<pre><code>BOS you have nice skin . EOS
BOS 你 的 皮 膚 真 好 。 EOS
你好害怕。

BOS you &apos;re UNK correct . EOS
BOS 你 部 分 正 确 。 EOS
你是全子的声音。

BOS everyone admired his courage . EOS
BOS 每 個 人 都 佩 服 他 的 勇 氣 。 EOS
他的袋子是他的勇氣。

BOS what time is it ? EOS
BOS 几 点 了 ？ EOS
多少时间是什么？

BOS i &apos;m free tonight . EOS
BOS 我 今 晚 有 空 。 EOS
我今晚有空。

BOS here is your book . EOS
BOS 這 是 你 的 書 。 EOS
这儿是你的书。

BOS they are at lunch . EOS
BOS 他 们 在 吃 午 饭 。 EOS
他们在午餐。

BOS this chair is UNK . EOS
BOS 這 把 椅 子 很 UNK 。 EOS
這些花一下是正在的。

BOS it &apos;s pretty heavy . EOS
BOS 它 真 重 。 EOS
它很美的脚。

BOS many attended his funeral . EOS
BOS 很 多 人 都 参 加 了 他 的 葬 礼 。 EOS
多多衛年轻地了他。

BOS training will be provided . EOS
BOS 会 有 训 练 。 EOS
别将被付錢。

BOS someone is watching you . EOS
BOS 有 人 在 看 著 你 。 EOS
有人看你。

BOS i slapped his face . EOS
BOS 我 摑 了 他 的 臉 。 EOS
我把他的臉抱歉。

BOS i like UNK music . EOS
BOS 我 喜 歡 流 行 音 樂 。 EOS
我喜歡音樂。

BOS tom had no children . EOS
BOS T o m 沒 有 孩 子 。 EOS
汤姆没有照顧孩子。

BOS please lock the door . EOS
BOS 請 把 門 鎖 上 。 EOS
请把門開門。

BOS tom has calmed down . EOS
BOS 汤 姆 冷 静 下 来 了 。 EOS
汤姆在做了。

BOS please speak more loudly . EOS
BOS 請 說 大 聲 一 點 兒 。 EOS
請說更多。

BOS keep next sunday free . EOS
BOS 把 下 周 日 空 出 来 。 EOS
繼續下週一下一步。

BOS i made a mistake . EOS
BOS 我 犯 了 一 個 錯 。 EOS
我做了一件事。</code></pre>
    </div>

    

    

    <!-- Comments -->
    

</div>
        </section>

    </div>
</div>


</div>

<!-- Footer -->
<div class="push"></div>

<footer class="footer-content">
    <div class="container">
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-6 col-lg-6 footer-about">
                <h2>About</h2>
                <p>
                    This theme was developed by <a href="https://github.com/klugjo" target="_blank" rel="noopener">Jonathan Klughertz</a>. The source code is available on Github. Create Websites. Make Magic.
                </p>
            </div>
            
    <div class="col-xs-6 col-sm-6 col-md-3 col-lg-3 recent-posts">
        <h2>Recent Posts</h2>
        <ul>
            
            <li>
                <a class="footer-post" href="/2020/04/17/%E9%9B%B6%E6%AC%A1%E5%AD%A6%E4%B9%A0%EF%BC%88Zero-Shot%20Learning%EF%BC%89%E5%85%A5%E9%97%A8/">零次学习（Zero-Shot Learning）入</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2020/04/13/LaTeX%E5%85%AC%E5%BC%8F%E6%89%8B%E5%86%8C(%E5%85%A8%E7%BD%91%E6%9C%80%E5%85%A8)/">LaTeX公式手册(全网最全)</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2020/04/08/R%E8%AF%AD%E8%A8%80%E5%9F%BA%E7%A1%80%E7%BB%98%E5%9B%BE/">R语言基础绘图</a>
            </li>
            
            <li>
                <a class="footer-post" href="/2020/03/30/%E8%B4%B9%E9%9B%AA%E4%BF%A1%E6%81%AF%E7%9F%A9%E9%98%B5/">费雪信息矩阵</a>
            </li>
            
        </ul>
    </div>



            
        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <ul class="list-inline footer-social-icons">
                    
                    <li class="list-inline-item">
                        <a href="https://github.com/klugjo/hexo-theme-alpha-dust" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-github"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://twitter.com/?lang=en" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-twitter"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://www.facebook.com/" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-facebook"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://www.instagram.com/" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-instagram"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://dribbble.com/" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-dribbble"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://plus.google.com/" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-google-plus"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://www.behance.net/" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-behance"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="https://500px.com/" target="_blank" rel="noopener">
                            <span class="footer-icon-container">
                                <i class="fa fa-500px"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="mailto:test@example.com">
                            <span class="footer-icon-container">
                                <i class="fa fa-envelope-o"></i>
                            </span>
                        </a>
                    </li>
                    
                    
                    <li class="list-inline-item">
                        <a href="\#">
                            <span class="footer-icon-container">
                                <i class="fa fa-rss"></i>
                            </span>
                        </a>
                    </li>
                    
                </ul>
            </div>
        </div>
        <div class="row">
            <div class="col-xs-12 col-sm-12 col-md-12 col-lg-12">
                <div class="footer-copyright">
                    @Untitled. All right reserved | Design & Hexo <a href="http://www.codeblocq.com/" target="_blank" rel="noopener">Jonathan Klughertz</a>
                </div>
            </div>
        </div>
    </div>
</footer>

<!-- After footer scripts -->

<!-- jQuery -->
<script src="//code.jquery.com/jquery-2.1.4.min.js"></script>

<!-- Tween Max -->
<script src="//cdnjs.cloudflare.com/ajax/libs/gsap/1.18.5/TweenMax.min.js"></script>

<!-- Gallery -->
<script src="//cdnjs.cloudflare.com/ajax/libs/featherlight/1.3.5/featherlight.min.js" type="text/javascript" charset="utf-8"></script>

<!-- Custom JavaScript -->

<script src="/js/main.js"></script>


<!-- Disqus Comments -->



</body>

</html>